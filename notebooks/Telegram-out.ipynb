{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.094175,
     "end_time": "2021-01-18T07:34:05.576363",
     "exception": false,
     "start_time": "2021-01-18T07:34:05.482188",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Telegram Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.090783,
     "end_time": "2021-01-18T07:34:05.755566",
     "exception": false,
     "start_time": "2021-01-18T07:34:05.664783",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Prepare environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-18T07:34:05.946817Z",
     "iopub.status.busy": "2021-01-18T07:34:05.944371Z",
     "iopub.status.idle": "2021-01-18T07:34:05.950379Z",
     "shell.execute_reply": "2021-01-18T07:34:05.949440Z"
    },
    "papermill": {
     "duration": 0.107581,
     "end_time": "2021-01-18T07:34:05.950623",
     "exception": false,
     "start_time": "2021-01-18T07:34:05.843042",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Set flag to true, if you work in visual studio code (connected to custom docker)\n",
    "Set flag to false, if you work in browser (jupyter notebook ui from custom docker)\n",
    "\"\"\"\n",
    "C_LOCAL                 = True\n",
    "\n",
    "\"\"\"\n",
    "Set flag to true, if you want process no long term running tasks\n",
    "\"\"\"\n",
    "C_SHORT_RUN             = False\n",
    "\n",
    "\"\"\"\n",
    "Resolve new urls?\n",
    "Set flag to false, if you dont want to resolve new urls\n",
    "\"\"\"\n",
    "C_RESOLVE_NEW_URLS      = True\n",
    "\n",
    "\"\"\"\n",
    "Load DataSets\n",
    "Ava:    [\"dataSet0\", \"dataSet1\", \"dataSet1a\", \"dataSet2\"]\n",
    "Htdocs: [\"dataSet0\", \"dataSet1a\", \"dataSet2\"]\n",
    "Req:    [\"dataSet0]\n",
    "\"\"\"\n",
    "C_LOAD_DATASET          = [\"dataSet0\", \"dataSet1a\", \"dataSet2\"]\n",
    "\n",
    "\"\"\"\n",
    "Time Plot Freq\n",
    "e.g. 1M = 1 Month\n",
    "e.g. 1W = 1 Week\n",
    "e.g. 1D = 1 Day\n",
    "\"\"\"\n",
    "C_TIME_PLOT_FREQ        = \"1D\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-18T07:34:06.143874Z",
     "iopub.status.busy": "2021-01-18T07:34:06.142804Z",
     "iopub.status.idle": "2021-01-18T07:34:09.739069Z",
     "shell.execute_reply": "2021-01-18T07:34:09.739521Z"
    },
    "papermill": {
     "duration": 3.697327,
     "end_time": "2021-01-18T07:34:09.739683",
     "exception": false,
     "start_time": "2021-01-18T07:34:06.042356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import default libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Import url libs\n",
    "from urllib.parse import urlparse\n",
    "from collections import Counter\n",
    "import requests\n",
    "\n",
    "# File Handler Lib\n",
    "from pathlib import Path\n",
    "\n",
    "# Set graph widget (used by jupyter notebook)\n",
    "#%matplotlib notebook   #interactive graphs\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-18T07:34:09.922901Z",
     "iopub.status.busy": "2021-01-18T07:34:09.922396Z",
     "iopub.status.idle": "2021-01-18T07:34:10.432850Z",
     "shell.execute_reply": "2021-01-18T07:34:10.431660Z"
    },
    "papermill": {
     "duration": 0.603816,
     "end_time": "2021-01-18T07:34:10.433095",
     "exception": false,
     "start_time": "2021-01-18T07:34:09.829279",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install and import Graph Lib\n",
    "import networkx as nx\n",
    "#! pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-18T07:34:10.619479Z",
     "iopub.status.busy": "2021-01-18T07:34:10.618837Z",
     "iopub.status.idle": "2021-01-18T07:34:10.651604Z",
     "shell.execute_reply": "2021-01-18T07:34:10.650617Z"
    },
    "papermill": {
     "duration": 0.125917,
     "end_time": "2021-01-18T07:34:10.651809",
     "exception": false,
     "start_time": "2021-01-18T07:34:10.525892",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install and import  JSON Lib\n",
    "#! pip install demjson\n",
    "import demjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-18T07:34:10.840324Z",
     "iopub.status.busy": "2021-01-18T07:34:10.839817Z",
     "iopub.status.idle": "2021-01-18T07:34:12.616031Z",
     "shell.execute_reply": "2021-01-18T07:34:12.616926Z"
    },
    "papermill": {
     "duration": 1.87108,
     "end_time": "2021-01-18T07:34:12.617195",
     "exception": false,
     "start_time": "2021-01-18T07:34:10.746115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install and import Natural Language Toolkit\n",
    "#! pip install nltk\n",
    "import nltk\n",
    "\n",
    "# Ngrams\n",
    "import re\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Stopwords\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-18T07:34:12.835589Z",
     "iopub.status.busy": "2021-01-18T07:34:12.834997Z",
     "iopub.status.idle": "2021-01-18T07:34:13.909276Z",
     "shell.execute_reply": "2021-01-18T07:34:13.908085Z"
    },
    "papermill": {
     "duration": 1.170887,
     "end_time": "2021-01-18T07:34:13.909520",
     "exception": false,
     "start_time": "2021-01-18T07:34:12.738633",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Gensim and pyLDAvis\n",
    "#! pip install gensim\n",
    "#! pip install pyLDAvis\n",
    "\n",
    "import gensim\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "\n",
    "import os\n",
    "\n",
    "# TODO Set to ignore?\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.simplefilter(\"once\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-18T07:34:14.098840Z",
     "iopub.status.busy": "2021-01-18T07:34:14.098227Z",
     "iopub.status.idle": "2021-01-18T07:34:18.651986Z",
     "shell.execute_reply": "2021-01-18T07:34:18.650799Z"
    },
    "papermill": {
     "duration": 4.646483,
     "end_time": "2021-01-18T07:34:18.652238",
     "exception": false,
     "start_time": "2021-01-18T07:34:14.005755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: demoji in /opt/conda/lib/python3.8/site-packages (0.4.0)\r\n",
      "Requirement already satisfied: requests<3.0.0 in /opt/conda/lib/python3.8/site-packages (from demoji) (2.25.1)\r\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.8/site-packages (from demoji) (0.4.4)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0->demoji) (1.26.2)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0->demoji) (2.10)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0->demoji) (2020.12.5)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0->demoji) (4.0.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading emoji data ..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... OK"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (Got response in 0.48 seconds)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing emoji data to /home/jovyan/.demoji/codes.json ..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... OK"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Install and import demoji\n",
    "# Dont exists on conda\n",
    "import sys\n",
    "!{sys.executable} -m pip install demoji\n",
    "\n",
    "import demoji\n",
    "demoji.download_codes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-18T07:34:18.853011Z",
     "iopub.status.busy": "2021-01-18T07:34:18.852490Z",
     "iopub.status.idle": "2021-01-18T07:34:18.919607Z",
     "shell.execute_reply": "2021-01-18T07:34:18.919095Z"
    },
    "papermill": {
     "duration": 0.168104,
     "end_time": "2021-01-18T07:34:18.919740",
     "exception": false,
     "start_time": "2021-01-18T07:34:18.751636",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/wordcloud/wordcloud.py:35: ResourceWarning: unclosed file <_io.TextIOWrapper name='/opt/conda/lib/python3.8/site-packages/wordcloud/stopwords' mode='r' encoding='UTF-8'>\n",
      "  STOPWORDS = set(map(str.strip, open(os.path.join(FILE, 'stopwords')).readlines()))\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "# Install and import WordCloud\n",
    "#! pip install wordcloud\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-18T07:34:19.121029Z",
     "iopub.status.busy": "2021-01-18T07:34:19.120405Z",
     "iopub.status.idle": "2021-01-18T07:34:19.161394Z",
     "shell.execute_reply": "2021-01-18T07:34:19.161904Z"
    },
    "papermill": {
     "duration": 0.142437,
     "end_time": "2021-01-18T07:34:19.162076",
     "exception": false,
     "start_time": "2021-01-18T07:34:19.019639",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip install lxml\n",
    "from lxml.html import fromstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-18T07:34:19.363704Z",
     "iopub.status.busy": "2021-01-18T07:34:19.363199Z",
     "iopub.status.idle": "2021-01-18T07:34:19.366496Z",
     "shell.execute_reply": "2021-01-18T07:34:19.365966Z"
    },
    "papermill": {
     "duration": 0.103667,
     "end_time": "2021-01-18T07:34:19.366608",
     "exception": false,
     "start_time": "2021-01-18T07:34:19.262941",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Show all columns (pandas hides columns by default)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-18T07:34:19.573228Z",
     "iopub.status.busy": "2021-01-18T07:34:19.572510Z",
     "iopub.status.idle": "2021-01-18T07:34:19.574428Z",
     "shell.execute_reply": "2021-01-18T07:34:19.573877Z"
    },
    "papermill": {
     "duration": 0.105037,
     "end_time": "2021-01-18T07:34:19.574565",
     "exception": false,
     "start_time": "2021-01-18T07:34:19.469528",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set plot style\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-18T07:34:19.777263Z",
     "iopub.status.busy": "2021-01-18T07:34:19.776507Z",
     "iopub.status.idle": "2021-01-18T07:34:24.131129Z",
     "shell.execute_reply": "2021-01-18T07:34:24.130475Z"
    },
    "papermill": {
     "duration": 4.455231,
     "end_time": "2021-01-18T07:34:24.131272",
     "exception": false,
     "start_time": "2021-01-18T07:34:19.676041",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Workdir -\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 12696\r\n",
      "drwxrwxr-x 6 jovyan  1000     4096 Jan 18 07:34 .\r\n",
      "drwxrwxr-x 6 jovyan  1000     4096 Jan 18 07:33 ..\r\n",
      "-rw-rw-r-- 1 jovyan  1000       46 Jan 13 07:37 additionalStopwords.txt\r\n",
      "drwxrwxr-x 2 jovyan  1000     4096 Jan 18 07:33 cache\r\n",
      "-rwxrwxr-x 1 jovyan  1000      273 Jan 16 12:14 clean-notebook.sh\r\n",
      "drwxrwxr-x 7 jovyan  1000     4096 Jan 13 09:31 data\r\n",
      "-rw-rw-r-- 1 jovyan  1000    29281 Jan 18 07:33 HuggingFace.ipynb\r\n",
      "-rw-rw-r-- 1 jovyan  1000    12522 Jan 14 07:28 inputFiles.csv\r\n",
      "drwxr-xr-x 2 jovyan users     4096 Jan  3 11:12 .ipynb_checkpoints\r\n",
      "drwxrwxr-x 4 jovyan  1000     4096 Jan 18 07:33 output\r\n",
      "-rwxrwxr-x 1 jovyan  1000      382 Jan 16 13:18 run-notebook.sh\r\n",
      "-rwxrwxr-x 1 jovyan  1000 12743409 Jan 18 07:33 Telegram.ipynb\r\n",
      "-rw-r--r-- 1 jovyan users   165424 Jan 18 07:34 Telegram-out.ipynb\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Outputdir -\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16\r\n",
      "drwxrwxr-x 4 jovyan 1000 4096 Jan 18 07:33 .\r\n",
      "drwxrwxr-x 6 jovyan 1000 4096 Jan 18 07:34 ..\r\n",
      "drwxrwxr-x 2 jovyan 1000 4096 Jan 18 07:33 autoWordCloud\r\n",
      "-rw-rw-r-- 1 jovyan 1000    0 Jan  3 10:43 .gitkeep\r\n",
      "drwxrwxr-x 2 jovyan 1000 4096 Jan 18 07:33 pyLDAvis\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Cachedir -\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4376\r\n",
      "drwxrwxr-x 2 jovyan  1000    4096 Jan 18 07:33 .\r\n",
      "drwxrwxr-x 6 jovyan  1000    4096 Jan 18 07:34 ..\r\n",
      "-rw-r--r-- 1 jovyan users    1170 Jan 17 18:39 auto-wordcloud-attila-hildmann.csv\r\n",
      "-rw-r--r-- 1 jovyan users    1936 Jan 17 18:39 auto-wordcloud-eva-herman.csv\r\n",
      "-rw-rw-r-- 1 jovyan  1000    3780 Jan 17 18:38 auto-wordcloud-oliver-janich.csv\r\n",
      "-rw-rw-r-- 1 jovyan  1000    1386 Jan 17 18:40 auto-wordcloud-xavier-naidoo.csv\r\n",
      "-rw-rw-r-- 1 jovyan  1000       0 Jan  4 14:37 .gitkeep\r\n",
      "-rw-rw-r-- 1 jovyan  1000  145139 Jan 17 03:21 resolved-urls.csv\r\n",
      "-rw-rw-r-- 1 jovyan  1000 4306175 Jan 18 07:33 resolved-youtube.csv\r\n"
     ]
    }
   ],
   "source": [
    "# Set env vars\n",
    "if(C_LOCAL == True):\n",
    "    dir_var = \"./work/notebooks/\"\n",
    "else:\n",
    "    dir_var = \"./\"\n",
    "\n",
    "dir_var_output = dir_var + \"output/\"\n",
    "\n",
    "dir_var_cache= dir_var + \"cache/\"\n",
    "\n",
    "# Debug output\n",
    "! echo \"- Workdir -\"\n",
    "! ls -al $dir_var\n",
    "\n",
    "! echo\n",
    "! echo \"- Outputdir -\"\n",
    "! ls -al $dir_var_output\n",
    "\n",
    "! echo\n",
    "! echo \"- Cachedir -\"\n",
    "! ls -al $dir_var_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.098851,
     "end_time": "2021-01-18T07:34:24.332123",
     "exception": false,
     "start_time": "2021-01-18T07:34:24.233272",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Global functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-18T07:34:24.537745Z",
     "iopub.status.busy": "2021-01-18T07:34:24.537171Z",
     "iopub.status.idle": "2021-01-18T07:34:24.540395Z",
     "shell.execute_reply": "2021-01-18T07:34:24.539830Z"
    },
    "papermill": {
     "duration": 0.110175,
     "end_time": "2021-01-18T07:34:24.540516",
     "exception": false,
     "start_time": "2021-01-18T07:34:24.430341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dictGloStopwatches = dict()\n",
    "\n",
    "# Start timer (for reporting)\n",
    "def gloStartStopwatch(key):\n",
    "    print(\"[Stopwatch started >>\" + str(key) + \"<<]\")\n",
    "    dictGloStopwatches[key] = time.time()\n",
    "\n",
    "# Stop timer (for reporting)\n",
    "def gloStopStopwatch(key):\n",
    "    endTime     = time.time()\n",
    "    startTime   = dictGloStopwatches[key]\n",
    "    print(\"[Stopwatch stopped >>\" + str(key) + \"<< (\" + '{:5.3f}s'.format(endTime-startTime) + \")]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-18T07:34:24.746857Z",
     "iopub.status.busy": "2021-01-18T07:34:24.746283Z",
     "iopub.status.idle": "2021-01-18T07:34:24.748771Z",
     "shell.execute_reply": "2021-01-18T07:34:24.748250Z"
    },
    "papermill": {
     "duration": 0.107727,
     "end_time": "2021-01-18T07:34:24.748885",
     "exception": false,
     "start_time": "2021-01-18T07:34:24.641158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Check if text is json formatted\n",
    "\n",
    "param   text        InputText\n",
    "param   singleMode  Boolean (set to true, if text is part of a message)\n",
    "\"\"\"\n",
    "def gloCheckIsTextJsonFormatted(text, singleMode):\n",
    "    textString = str(text)\n",
    "    if      (singleMode == False and textString.startswith(\"[\") == True and textString.endswith(\"]\") == True):\n",
    "        return True\n",
    "    elif    (singleMode == True and textString.startswith(\"{\") == True and textString.endswith(\"}\") == True):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-18T07:34:24.953801Z",
     "iopub.status.busy": "2021-01-18T07:34:24.953244Z",
     "iopub.status.idle": "2021-01-18T07:34:24.955870Z",
     "shell.execute_reply": "2021-01-18T07:34:24.955352Z"
    },
    "papermill": {
     "duration": 0.10544,
     "end_time": "2021-01-18T07:34:24.955981",
     "exception": false,
     "start_time": "2021-01-18T07:34:24.850541",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gloReplaceGermanChars(inputText):\n",
    "\n",
    "    inputText = inputText.replace(\"ö\", \"oe\")\n",
    "    inputText = inputText.replace(\"ü\", \"ue\")\n",
    "    inputText = inputText.replace(\"ä\", \"ae\")\n",
    "\n",
    "    inputText = inputText.replace(\"Ö\", \"Oe\")\n",
    "    inputText = inputText.replace(\"Ü\", \"Ue\")\n",
    "    inputText = inputText.replace(\"Ä\", \"Ae\")\n",
    "\n",
    "    inputText = inputText.replace(\"ß\", \"ss\")\n",
    "    \n",
    "    return inputText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-18T07:34:25.161668Z",
     "iopub.status.busy": "2021-01-18T07:34:25.161159Z",
     "iopub.status.idle": "2021-01-18T07:34:25.165001Z",
     "shell.execute_reply": "2021-01-18T07:34:25.164078Z"
    },
    "papermill": {
     "duration": 0.109516,
     "end_time": "2021-01-18T07:34:25.165192",
     "exception": false,
     "start_time": "2021-01-18T07:34:25.055676",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rm unsafe chars\n",
    "def gloConvertToSafeString(text):\n",
    "    text = demoji.replace(text, \"\")\n",
    "    text = gloReplaceGermanChars(text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    #text = text.encode('ascii', 'ignore')\n",
    "    #text = text.decode('ascii')\n",
    "    return text\n",
    "\n",
    "# Generate unique chat name\n",
    "def gloConvertToSafeChatName(chatName):\n",
    "    chatName = gloConvertToSafeString(chatName)\n",
    "    return chatName[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-18T07:34:25.373416Z",
     "iopub.status.busy": "2021-01-18T07:34:25.372896Z",
     "iopub.status.idle": "2021-01-18T07:34:25.375277Z",
     "shell.execute_reply": "2021-01-18T07:34:25.374794Z"
    },
    "papermill": {
     "duration": 0.108046,
     "end_time": "2021-01-18T07:34:25.375394",
     "exception": false,
     "start_time": "2021-01-18T07:34:25.267348",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dict File Cache\n",
    "dictFileCache = {}\n",
    "\n",
    "# Write dict to file (CSV)\n",
    "def gloWriteDictToFile(filename, targetDict):\n",
    "    dictFileCache = {} #Clear cache\n",
    "    d = pd.DataFrame.from_dict(targetDict, orient=\"index\")\n",
    "    d.to_csv(dir_var_cache + filename, header=False)\n",
    "\n",
    "# Read dict from file (CSV)\n",
    "def gloReadDictFromFile(filename):\n",
    "    # Cache?\n",
    "    if(filename in dictFileCache):\n",
    "        return dictFileCache[filename]\n",
    "\n",
    "    d = pd.read_csv(dir_var_cache + filename, header=None, index_col=0, squeeze=True)\n",
    "    retDict = d.to_dict()\n",
    "\n",
    "    dictFileCache[filename] = retDict #Add to cache\n",
    "\n",
    "    return retDict\n",
    "\n",
    "# Init csv file if not exists\n",
    "def gloInitFileDict(filename):\n",
    "    f = Path(dir_var_cache + filename)\n",
    "    if(f.exists() == False):\n",
    "        print(\"Init cache file >>\" + filename + \"<<\")\n",
    "        f.touch()\n",
    "        gloWriteDictToFile(filename, {\"initKey\": \"initValue\"})\n",
    "    else:\n",
    "        print(\"Cache already exists >>\" + filename + \"<<\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-18T07:34:25.586568Z",
     "iopub.status.busy": "2021-01-18T07:34:25.585790Z",
     "iopub.status.idle": "2021-01-18T07:34:25.589491Z",
     "shell.execute_reply": "2021-01-18T07:34:25.588419Z"
    },
    "papermill": {
     "duration": 0.111226,
     "end_time": "2021-01-18T07:34:25.589681",
     "exception": false,
     "start_time": "2021-01-18T07:34:25.478455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if is already cached\n",
    "def gloCheckIsAlreadyCached(filename, targetKey):\n",
    "    targetDict = gloReadDictFromFile(filename)\n",
    "    if(targetKey in targetDict.keys()):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Add key to cache\n",
    "def gloAddToCache(filename, targetKey, targetValue):\n",
    "    targetDict = gloReadDictFromFile(filename)\n",
    "    targetDict[targetKey] = targetValue\n",
    "    gloWriteDictToFile(filename, targetDict)\n",
    "\n",
    "# Get key from cache\n",
    "def gloGetCached(filename, targetKey):\n",
    "    targetDict = gloReadDictFromFile(filename)\n",
    "    return targetDict[targetKey]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.099451,
     "end_time": "2021-01-18T07:34:25.788991",
     "exception": false,
     "start_time": "2021-01-18T07:34:25.689540",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Init Cache Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-18T07:34:25.990069Z",
     "iopub.status.busy": "2021-01-18T07:34:25.989517Z",
     "iopub.status.idle": "2021-01-18T07:34:25.993436Z",
     "shell.execute_reply": "2021-01-18T07:34:25.992773Z"
    },
    "papermill": {
     "duration": 0.106935,
     "end_time": "2021-01-18T07:34:25.993577",
     "exception": false,
     "start_time": "2021-01-18T07:34:25.886642",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache already exists >>resolved-urls.csv<<"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gloInitFileDict(\"resolved-urls.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-18T07:34:26.197667Z",
     "iopub.status.busy": "2021-01-18T07:34:26.197151Z",
     "iopub.status.idle": "2021-01-18T07:34:26.201243Z",
     "shell.execute_reply": "2021-01-18T07:34:26.200815Z"
    },
    "papermill": {
     "duration": 0.107786,
     "end_time": "2021-01-18T07:34:26.201355",
     "exception": false,
     "start_time": "2021-01-18T07:34:26.093569",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache already exists >>resolved-youtube.csv<<"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gloInitFileDict(\"resolved-youtube.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.098843,
     "end_time": "2021-01-18T07:34:26.413384",
     "exception": false,
     "start_time": "2021-01-18T07:34:26.314541",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Process input jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-18T07:34:26.615685Z",
     "iopub.status.busy": "2021-01-18T07:34:26.615121Z",
     "iopub.status.idle": "2021-01-18T07:34:26.620786Z",
     "shell.execute_reply": "2021-01-18T07:34:26.619616Z"
    },
    "papermill": {
     "duration": 0.109349,
     "end_time": "2021-01-18T07:34:26.621023",
     "exception": false,
     "start_time": "2021-01-18T07:34:26.511674",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stopwatch started >>Global notebook<<]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gloStartStopwatch(\"Global notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-18T07:34:26.832220Z",
     "iopub.status.busy": "2021-01-18T07:34:26.831617Z",
     "iopub.status.idle": "2021-01-18T07:34:26.875630Z",
     "shell.execute_reply": "2021-01-18T07:34:26.874464Z"
    },
    "papermill": {
     "duration": 0.15025,
     "end_time": "2021-01-18T07:34:26.875874",
     "exception": false,
     "start_time": "2021-01-18T07:34:26.725624",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read jobs from file\n",
    "dfInputFiles = pd.read_csv(dir_var + \"inputFiles.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-18T07:34:27.109160Z",
     "iopub.status.busy": "2021-01-18T07:34:27.107998Z",
     "iopub.status.idle": "2021-01-18T07:34:27.143312Z",
     "shell.execute_reply": "2021-01-18T07:34:27.142210Z"
    },
    "papermill": {
     "duration": 0.162228,
     "end_time": "2021-01-18T07:34:27.143525",
     "exception": false,
     "start_time": "2021-01-18T07:34:26.981297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfFilter = pd.DataFrame()\n",
    "\n",
    "for dS in C_LOAD_DATASET:\n",
    "    dfFilter = dfFilter.append(dfInputFiles[dfInputFiles.inputDesc == dS])\n",
    "\n",
    "dfInputFiles = dfFilter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.100025,
     "end_time": "2021-01-18T07:34:27.348791",
     "exception": false,
     "start_time": "2021-01-18T07:34:27.248766",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Overview input jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-18T07:34:27.583845Z",
     "iopub.status.busy": "2021-01-18T07:34:27.582814Z",
     "iopub.status.idle": "2021-01-18T07:34:27.602290Z",
     "shell.execute_reply": "2021-01-18T07:34:27.601858Z"
    },
    "papermill": {
     "duration": 0.154217,
     "end_time": "2021-01-18T07:34:27.602418",
     "exception": false,
     "start_time": "2021-01-18T07:34:27.448201",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputName</th>\n",
       "      <th>inputPath</th>\n",
       "      <th>inputType</th>\n",
       "      <th>inputId</th>\n",
       "      <th>inputDesc</th>\n",
       "      <th>inputDownloadType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>ATTILA HILDMANN</td>\n",
       "      <td>DS-05-01-2021/ChatExport_2021-01-05-hildmann</td>\n",
       "      <td>public_channel</td>\n",
       "      <td>10034163583</td>\n",
       "      <td>dataSet0</td>\n",
       "      <td>onlyText</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Oliver Janich oeffentlich</td>\n",
       "      <td>DS-05-01-2021/ChatExport_2021-01-05-janich</td>\n",
       "      <td>public_channel</td>\n",
       "      <td>9808932799</td>\n",
       "      <td>dataSet0</td>\n",
       "      <td>onlyText</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Eva Herman Offiziell</td>\n",
       "      <td>DS-05-01-2021/ChatExport_2021-01-05-evaherman</td>\n",
       "      <td>public_channel</td>\n",
       "      <td>9915108907</td>\n",
       "      <td>dataSet0</td>\n",
       "      <td>onlyText</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Xavier Naidoo (inoffiziell)</td>\n",
       "      <td>DS-05-01-2021/ChatExport_2021-01-05-xavier</td>\n",
       "      <td>public_channel</td>\n",
       "      <td>9874390332</td>\n",
       "      <td>dataSet0</td>\n",
       "      <td>onlyText</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>FREIHEITS-CHAT</td>\n",
       "      <td>DS-05-01-2021a/ChatExport_2021-01-05-freiheits...</td>\n",
       "      <td>public_supergroup</td>\n",
       "      <td>9717909816</td>\n",
       "      <td>dataSet1a</td>\n",
       "      <td>onlyText</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>FREIHEITSCHAT - BLITZ</td>\n",
       "      <td>DS-05-01-2021a/ChatExport_2021-01-05-freiheits...</td>\n",
       "      <td>public_supergroup</td>\n",
       "      <td>9943834900</td>\n",
       "      <td>dataSet1a</td>\n",
       "      <td>onlyText</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Livestreaming fuer Deutschland</td>\n",
       "      <td>DS-05-01-2021a/ChatExport_2021-01-05-liveFuerD...</td>\n",
       "      <td>public_supergroup</td>\n",
       "      <td>9943535763</td>\n",
       "      <td>dataSet1a</td>\n",
       "      <td>onlyText</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Querdenken 089 Muenchen - Diskussion u. Austausch</td>\n",
       "      <td>DS-13-01-2021/ChatExport_2021-01-13-querdenken089</td>\n",
       "      <td>public_supergroup</td>\n",
       "      <td>10025647074</td>\n",
       "      <td>dataSet2</td>\n",
       "      <td>onlyText</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>Querdenken 591 Emsland - Info Kanal</td>\n",
       "      <td>DS-13-01-2021/ChatExport_2021-01-13-querdenken...</td>\n",
       "      <td>public_channel</td>\n",
       "      <td>9864889937</td>\n",
       "      <td>dataSet2</td>\n",
       "      <td>onlyText</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Querdenken 773 Hegau - Diskussion u. Austausch</td>\n",
       "      <td>DS-13-01-2021/ChatExport_2021-01-13-querdenken773</td>\n",
       "      <td>public_supergroup</td>\n",
       "      <td>9818433650</td>\n",
       "      <td>dataSet2</td>\n",
       "      <td>onlyText</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>Querdenken 773 Hegau - Info Kanal</td>\n",
       "      <td>DS-13-01-2021/ChatExport_2021-01-13-querdenken...</td>\n",
       "      <td>public_channel</td>\n",
       "      <td>9885532851</td>\n",
       "      <td>dataSet2</td>\n",
       "      <td>onlyText</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>Querdenken 711 Stuttgart - Diskussion u. Austa...</td>\n",
       "      <td>DS-13-01-2021/ChatExport_2021-01-13-querdenken711</td>\n",
       "      <td>public_supergroup</td>\n",
       "      <td>9812812343</td>\n",
       "      <td>dataSet2</td>\n",
       "      <td>onlyText</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Querdenken 711 Stuttgart - Info Kanal</td>\n",
       "      <td>DS-13-01-2021/ChatExport_2021-01-13-querdenken...</td>\n",
       "      <td>public_channel</td>\n",
       "      <td>9818761759</td>\n",
       "      <td>dataSet2</td>\n",
       "      <td>onlyText</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>Querdenken 69 Frankfurt - Diskussion u. Austausch</td>\n",
       "      <td>DS-13-01-2021/ChatExport_2021-01-13-querdenken69</td>\n",
       "      <td>public_supergroup</td>\n",
       "      <td>9906959218</td>\n",
       "      <td>dataSet2</td>\n",
       "      <td>onlyText</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>Querdenken 69 Frankfurt - Info Kanal</td>\n",
       "      <td>DS-13-01-2021/ChatExport_2021-01-13-querdenken...</td>\n",
       "      <td>public_channel</td>\n",
       "      <td>9991668824</td>\n",
       "      <td>dataSet2</td>\n",
       "      <td>onlyText</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             inputName  \\\n",
       "47                                     ATTILA HILDMANN   \n",
       "48                           Oliver Janich oeffentlich   \n",
       "49                                Eva Herman Offiziell   \n",
       "50                         Xavier Naidoo (inoffiziell)   \n",
       "58                                      FREIHEITS-CHAT   \n",
       "60                               FREIHEITSCHAT - BLITZ   \n",
       "98                      Livestreaming fuer Deutschland   \n",
       "100  Querdenken 089 Muenchen - Diskussion u. Austausch   \n",
       "101                Querdenken 591 Emsland - Info Kanal   \n",
       "102     Querdenken 773 Hegau - Diskussion u. Austausch   \n",
       "103                  Querdenken 773 Hegau - Info Kanal   \n",
       "104  Querdenken 711 Stuttgart - Diskussion u. Austa...   \n",
       "105              Querdenken 711 Stuttgart - Info Kanal   \n",
       "106  Querdenken 69 Frankfurt - Diskussion u. Austausch   \n",
       "107               Querdenken 69 Frankfurt - Info Kanal   \n",
       "\n",
       "                                             inputPath          inputType  \\\n",
       "47        DS-05-01-2021/ChatExport_2021-01-05-hildmann     public_channel   \n",
       "48          DS-05-01-2021/ChatExport_2021-01-05-janich     public_channel   \n",
       "49       DS-05-01-2021/ChatExport_2021-01-05-evaherman     public_channel   \n",
       "50          DS-05-01-2021/ChatExport_2021-01-05-xavier     public_channel   \n",
       "58   DS-05-01-2021a/ChatExport_2021-01-05-freiheits...  public_supergroup   \n",
       "60   DS-05-01-2021a/ChatExport_2021-01-05-freiheits...  public_supergroup   \n",
       "98   DS-05-01-2021a/ChatExport_2021-01-05-liveFuerD...  public_supergroup   \n",
       "100  DS-13-01-2021/ChatExport_2021-01-13-querdenken089  public_supergroup   \n",
       "101  DS-13-01-2021/ChatExport_2021-01-13-querdenken...     public_channel   \n",
       "102  DS-13-01-2021/ChatExport_2021-01-13-querdenken773  public_supergroup   \n",
       "103  DS-13-01-2021/ChatExport_2021-01-13-querdenken...     public_channel   \n",
       "104  DS-13-01-2021/ChatExport_2021-01-13-querdenken711  public_supergroup   \n",
       "105  DS-13-01-2021/ChatExport_2021-01-13-querdenken...     public_channel   \n",
       "106   DS-13-01-2021/ChatExport_2021-01-13-querdenken69  public_supergroup   \n",
       "107  DS-13-01-2021/ChatExport_2021-01-13-querdenken...     public_channel   \n",
       "\n",
       "         inputId  inputDesc inputDownloadType  \n",
       "47   10034163583   dataSet0          onlyText  \n",
       "48    9808932799   dataSet0          onlyText  \n",
       "49    9915108907   dataSet0          onlyText  \n",
       "50    9874390332   dataSet0          onlyText  \n",
       "58    9717909816  dataSet1a          onlyText  \n",
       "60    9943834900  dataSet1a          onlyText  \n",
       "98    9943535763  dataSet1a          onlyText  \n",
       "100  10025647074   dataSet2          onlyText  \n",
       "101   9864889937   dataSet2          onlyText  \n",
       "102   9818433650   dataSet2          onlyText  \n",
       "103   9885532851   dataSet2          onlyText  \n",
       "104   9812812343   dataSet2          onlyText  \n",
       "105   9818761759   dataSet2          onlyText  \n",
       "106   9906959218   dataSet2          onlyText  \n",
       "107   9991668824   dataSet2          onlyText  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfInputFiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.101079,
     "end_time": "2021-01-18T07:34:27.805660",
     "exception": false,
     "start_time": "2021-01-18T07:34:27.704581",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Transform data into DataFrmaes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-18T07:34:28.014022Z",
     "iopub.status.busy": "2021-01-18T07:34:28.013505Z",
     "iopub.status.idle": "2021-01-18T07:34:28.020571Z",
     "shell.execute_reply": "2021-01-18T07:34:28.019845Z"
    },
    "papermill": {
     "duration": 0.115411,
     "end_time": "2021-01-18T07:34:28.020745",
     "exception": false,
     "start_time": "2021-01-18T07:34:27.905334",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# Convert to DataFrame Meta (Chat Meta)\n",
    "def convertToDataFrameMeta(filePath):\n",
    "    dF = pd.read_json(dir_var + \"data/\" + filePath + \"/result.json\", encoding='utf-8')\n",
    "    return dF\n",
    "\n",
    "# Convert to DataFrame Messages (Chat Messages)\n",
    "def convertToDataFrameMessages(filePath):\n",
    "    dF = pd.json_normalize(dictMeta[filePath].messages)\n",
    "    return dF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-18T07:34:28.235369Z",
     "iopub.status.busy": "2021-01-18T07:34:28.234814Z",
     "iopub.status.idle": "2021-01-18T07:34:28.240905Z",
     "shell.execute_reply": "2021-01-18T07:34:28.240377Z"
    },
    "papermill": {
     "duration": 0.11554,
     "end_time": "2021-01-18T07:34:28.241042",
     "exception": false,
     "start_time": "2021-01-18T07:34:28.125502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: DeprecationWarning: invalid escape sequence \\.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:6: DeprecationWarning: invalid escape sequence \\w\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/6718633/python-regular-expression-again-match-url\n",
    "def getUrlRegex():\n",
    "    return \"((?:https?://)?(?:(?:www\\.)?(?:[\\da-z\\.-]+)\\.(?:[a-z]{2,6})|(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)|(?:(?:[0-9a-fA-F]{1,4}:){7,7}[0-9a-fA-F]{1,4}|(?:[0-9a-fA-F]{1,4}:){1,7}:|(?:[0-9a-fA-F]{1,4}:){1,6}:[0-9a-fA-F]{1,4}|(?:[0-9a-fA-F]{1,4}:){1,5}(?::[0-9a-fA-F]{1,4}){1,2}|(?:[0-9a-fA-F]{1,4}:){1,4}(?::[0-9a-fA-F]{1,4}){1,3}|(?:[0-9a-fA-F]{1,4}:){1,3}(?::[0-9a-fA-F]{1,4}){1,4}|(?:[0-9a-fA-F]{1,4}:){1,2}(?::[0-9a-fA-F]{1,4}){1,5}|[0-9a-fA-F]{1,4}:(?:(?::[0-9a-fA-F]{1,4}){1,6})|:(?:(?::[0-9a-fA-F]{1,4}){1,7}|:)|fe80:(?::[0-9a-fA-F]{0,4}){0,4}%[0-9a-zA-Z]{1,}|::(?:ffff(?::0{1,4}){0,1}:){0,1}(?:(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])|(?:[0-9a-fA-F]{1,4}:){1,4}:(?:(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])))(?::[0-9]{1,4}|[1-5][0-9]{4}|6[0-4][0-9]{3}|65[0-4][0-9]{2}|655[0-2][0-9]|6553[0-5])?(?:/[\\w\\.-]*)*/?)\"\n",
    "\n",
    "def gefGetHashtagRegex():\n",
    "    return \"#(\\w+)\"\n",
    "\n",
    "def hashTagExtractHashTags(inputText):\n",
    "\n",
    "    inputText = str(inputText)\n",
    "\n",
    "    inputText = re.sub('\\n', ' ', inputText) # Replace \\n\n",
    "    inputText = demoji.replace(inputText, \" \") # Rm emoji\n",
    "    inputText = gloReplaceGermanChars(inputText) # Replace german chars\n",
    "\n",
    "    return re.findall(gefGetHashtagRegex(), inputText)\n",
    "\n",
    "def urlExtractUrls(inputText):\n",
    "    return re.findall(getUrlRegex(), str(inputText))\n",
    "\n",
    "def urlRemoveUrls(inputText):\n",
    "    return re.sub(getUrlRegex(), \" \", str(inputText))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-18T07:34:28.456642Z",
     "iopub.status.busy": "2021-01-18T07:34:28.456046Z",
     "iopub.status.idle": "2021-01-18T07:34:28.459346Z",
     "shell.execute_reply": "2021-01-18T07:34:28.458767Z"
    },
    "papermill": {
     "duration": 0.114077,
     "end_time": "2021-01-18T07:34:28.459468",
     "exception": false,
     "start_time": "2021-01-18T07:34:28.345391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get params from extractedTextData\n",
    "See cell below (key)\n",
    "\"\"\"\n",
    "def getExtractedTextDataParam(key, extractedTextData):\n",
    "\n",
    "    a,b,c,d,e,f,g = extractedTextData\n",
    "\n",
    "    if(key == 0):\n",
    "\n",
    "        return urlRemoveUrls(a)\n",
    "\n",
    "    elif(key == 1):\n",
    "\n",
    "        before = b\n",
    "        extracted = urlExtractUrls(a)\n",
    "\n",
    "        after = before\n",
    "        after.extend(extracted)\n",
    "\n",
    "        \"\"\"\n",
    "        if(str(extracted) != \"[]\"):\n",
    "            # TODO: Fix return bug\n",
    "            print(\"Debug >>\" + str(before) + \"/\" + str(extracted) + \">>\" + str(after) + \"<<\")\n",
    "        \"\"\"\n",
    "\n",
    "        return after\n",
    "\n",
    "    elif(key == 2):\n",
    "\n",
    "        # TODO: Refactor dont take it from extractedTextData\n",
    "        return hashTagExtractHashTags(a)\n",
    "\n",
    "    else:\n",
    "        switcher = {\n",
    "            3: d,\n",
    "            4: e,\n",
    "            5: f,\n",
    "            6: g\n",
    "        }\n",
    "        return switcher.get(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-18T07:34:28.684648Z",
     "iopub.status.busy": "2021-01-18T07:34:28.679381Z",
     "iopub.status.idle": "2021-01-18T07:34:28.694201Z",
     "shell.execute_reply": "2021-01-18T07:34:28.693048Z"
    },
    "papermill": {
     "duration": 0.131046,
     "end_time": "2021-01-18T07:34:28.694473",
     "exception": false,
     "start_time": "2021-01-18T07:34:28.563427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract text data (see cell above key)\n",
    "See cell above (key)\n",
    "\n",
    "param   procIsJsonFormatted Boolean (is text json formatted?)\n",
    "param   text                String  (text from message) \n",
    "\n",
    "return\n",
    "a   procText            Plain Text\n",
    "b   processedURLs       Array of URLs in Text\n",
    "c   processedHashtags   Array of Hashtags in Text #TODO: RM\n",
    "d   processedBolds      Array of Bold Items in Text\n",
    "e   processedItalics    Array of Italic Items in Text\n",
    "f   processedUnderlines Array of Underlined Items in Text\n",
    "g   processedEmails     Array of E-Mails in Text\n",
    "\"\"\"\n",
    "def extractTextData(procIsJsonFormatted, text):\n",
    "    \n",
    "    # 3 returns in this function...\n",
    "    \n",
    "    processedURLs       = list()\n",
    "    processedHashtags   = list() # TODO: RM\n",
    "    processedBolds      = list()\n",
    "    processedItalics    = list()\n",
    "    processedUnderlines = list()\n",
    "    processedEmails     = list()\n",
    "    \n",
    "    if(procIsJsonFormatted != True):\n",
    "        #Is not JSON formatted (return normal text)\n",
    "        return (text, processedURLs, processedHashtags, processedBolds, processedItalics, processedUnderlines, processedEmails)\n",
    "    else:\n",
    "        #Is is JSON formatted (try to parse)\n",
    "        try:\n",
    "            returnList = []\n",
    "            jsonList = demjson.decode(str(text), encoding='utf8')\n",
    "\n",
    "            # Do for each item in list\n",
    "            for lItem in jsonList:\n",
    "\n",
    "                messageString = str(lItem)\n",
    "\n",
    "                isJsonSubString = gloCheckIsTextJsonFormatted(messageString, singleMode = True)\n",
    "\n",
    "                if(isJsonSubString):\n",
    "                    # Is Json Sub String\n",
    "                    subJsonString = demjson.decode(str(messageString), encoding='utf8')\n",
    "                    subJsonType = subJsonString[\"type\"]\n",
    "\n",
    "                    if(subJsonType == \"bold\"):\n",
    "                        #text included\n",
    "                        processedBolds.append(subJsonString[\"text\"])\n",
    "                        returnList.append(subJsonString[\"text\"])\n",
    "                        \n",
    "                    elif(subJsonType == \"italic\"):\n",
    "                        #text included\n",
    "                        processedItalics.append(subJsonString[\"text\"])\n",
    "                        returnList.append(subJsonString[\"text\"])\n",
    "                        \n",
    "                    elif(subJsonType == \"underline\"):\n",
    "                        #text included\n",
    "                        processedUnderlines.append(subJsonString[\"text\"])\n",
    "                        returnList.append(subJsonString[\"text\"])\n",
    "                    \n",
    "                    elif(subJsonType == \"email\"):\n",
    "                        #text included\n",
    "                        processedEmails.append(subJsonString[\"text\"])\n",
    "                        \n",
    "                    elif(subJsonType == \"text_link\"):\n",
    "                        #text and href included\n",
    "                        processedURLs.append(subJsonString[\"href\"])\n",
    "                        #returnList.append(subJsonString[\"text\"])\n",
    "                        \n",
    "                    elif(subJsonType == \"link\"):\n",
    "                        #text included\n",
    "                        processedURLs.append(subJsonString[\"text\"])\n",
    "                        \n",
    "                    elif(subJsonType == \"hashtag\"):\n",
    "                        #text included\n",
    "                        #processedHashtags.append(subJsonString[\"text\"]) # TODO: Refactor: Dont add hashtags here!\n",
    "                        returnList.append(subJsonString[\"text\"])\n",
    "                        \n",
    "                    elif(subJsonType == \"mention\"):\n",
    "                        #text included\n",
    "                        returnList.append(subJsonString[\"text\"])\n",
    "                        \n",
    "                    elif(subJsonType == \"mention_name\"):\n",
    "                        #text and user_id included\n",
    "                        returnList.append(subJsonString[\"text\"])\n",
    "                        \n",
    "                    elif(subJsonType == \"bot_command\"):\n",
    "                        #text included\n",
    "                        returnList = returnList \n",
    "                        \n",
    "                    elif(subJsonType == \"code\"):\n",
    "                        #text included\n",
    "                        returnList = returnList\n",
    "                        \n",
    "                    elif(subJsonType == \"phone\"):\n",
    "                        #text included\n",
    "                        returnList = returnList\n",
    "                        \n",
    "                    elif(subJsonType == \"strikethrough\"):\n",
    "                        #text included\n",
    "                        returnList.append(subJsonString[\"text\"])\n",
    "                        \n",
    "                    elif(subJsonType == \"pre\"):\n",
    "                        #text and language included\n",
    "                        returnList.append(subJsonString[\"text\"])\n",
    "                        \n",
    "                    elif(subJsonType == \"bank_card\"):\n",
    "                        #text included\n",
    "                        returnList = returnList\n",
    "                        \n",
    "                    else:\n",
    "                        print(\"- Error: Unkown json type >>\" + str(subJsonType) + \"<< (ignore) >>\" + str(text) + \"<<\")\n",
    "\n",
    "                else:\n",
    "                    # Is no json formatted sub string (append text)\n",
    "                    returnList.append(messageString)\n",
    "\n",
    "            return (''.join(returnList), processedURLs, processedHashtags, processedBolds, processedItalics, processedUnderlines, processedEmails)\n",
    "        \n",
    "        except:\n",
    "            # Parser error (set inputText to returnText)\n",
    "            print(\"- Warn: Json parser error (set inputText to returnText) >>\" + str(text) + \"<<\")\n",
    "            return (text, processedURLs, processedHashtags, processedBolds, processedItalics, processedUnderlines, processedEmails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-18T07:34:28.906917Z",
     "iopub.status.busy": "2021-01-18T07:34:28.906327Z",
     "iopub.status.idle": "2021-01-18T07:34:28.909285Z",
     "shell.execute_reply": "2021-01-18T07:34:28.908859Z"
    },
    "papermill": {
     "duration": 0.107903,
     "end_time": "2021-01-18T07:34:28.909395",
     "exception": false,
     "start_time": "2021-01-18T07:34:28.801492",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evalIsValidText(procTDTextLength):\n",
    "    if(procTDTextLength > 0):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-18T07:34:29.119169Z",
     "iopub.status.busy": "2021-01-18T07:34:29.118668Z",
     "iopub.status.idle": "2021-01-18T07:34:29.121633Z",
     "shell.execute_reply": "2021-01-18T07:34:29.121122Z"
    },
    "papermill": {
     "duration": 0.110152,
     "end_time": "2021-01-18T07:34:29.121740",
     "exception": false,
     "start_time": "2021-01-18T07:34:29.011588",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evalContainsSomething(att):\n",
    "    if(str(att) == \"nan\"):\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-18T07:34:29.335767Z",
     "iopub.status.busy": "2021-01-18T07:34:29.335264Z",
     "iopub.status.idle": "2021-01-18T07:34:29.338242Z",
     "shell.execute_reply": "2021-01-18T07:34:29.337726Z"
    },
    "papermill": {
     "duration": 0.112007,
     "end_time": "2021-01-18T07:34:29.338348",
     "exception": false,
     "start_time": "2021-01-18T07:34:29.226341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evalNonEmptyList(att):\n",
    "    if(str(att) == \"[]\"):\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2021-01-18T07:34:29.443641",
     "status": "running"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "gloStartStopwatch(\"Extract Text Data\")\n",
    "\n",
    "# Add Key = filePath / Value = DataFrame (Chat Meta)\n",
    "dictMeta = {}\n",
    "for fP in dfInputFiles.inputPath:\n",
    "    dictMeta[fP] = convertToDataFrameMeta(fP)\n",
    "\n",
    "# Add Key = filePath / Value = DataFrame (Chat Message)\n",
    "dictMessages = {}\n",
    "for fP in dfInputFiles.inputPath:\n",
    "\n",
    "    gloStartStopwatch(\"TD-Extract \" + fP)\n",
    "    dfMessages                          = convertToDataFrameMessages(fP)\n",
    "\n",
    "    # Get chat attributes and check if message is json formatted\n",
    "    dfMessages[\"procChatFilePath\"]      = fP\n",
    "    dfMessages[\"procChatType\"]          = dictMeta[fP].type.iloc[0]\n",
    "    dfMessages[\"procIsJsonFormatted\"]   = dfMessages[\"text\"].apply(gloCheckIsTextJsonFormatted, singleMode = False)\n",
    "    \n",
    "    # Extract Text Data\n",
    "    dfMessages[\"tmpExtractedTD\"]        = dfMessages.apply(lambda x: extractTextData(x.procIsJsonFormatted, x.text), axis=1)\n",
    "\n",
    "    # Extract Text Data (params)\n",
    "    dfMessages[\"procTDText\"]            = dfMessages.apply(lambda x: getExtractedTextDataParam(0, x.tmpExtractedTD), axis=1)\n",
    "    dfMessages[\"procTDURLs\"]            = dfMessages.apply(lambda x: getExtractedTextDataParam(1, x.tmpExtractedTD), axis=1)\n",
    "    dfMessages[\"procTDHashtags\"]        = dfMessages.apply(lambda x: getExtractedTextDataParam(2, x.tmpExtractedTD), axis=1)\n",
    "    dfMessages[\"procTDBolds\"]           = dfMessages.apply(lambda x: getExtractedTextDataParam(3, x.tmpExtractedTD), axis=1)\n",
    "    dfMessages[\"procTDItalics\"]         = dfMessages.apply(lambda x: getExtractedTextDataParam(4, x.tmpExtractedTD), axis=1)\n",
    "    dfMessages[\"procTDUnderlines\"]      = dfMessages.apply(lambda x: getExtractedTextDataParam(5, x.tmpExtractedTD), axis=1)\n",
    "    dfMessages[\"procTDEmails\"]          = dfMessages.apply(lambda x: getExtractedTextDataParam(6, x.tmpExtractedTD), axis=1)\n",
    "\n",
    "    # Process text again\n",
    "    dfMessages['procTDCleanText']           = dfMessages['procTDText'].map(lambda x: re.sub('\\n', ' ', x)) # Replace \\n\n",
    "    dfMessages['procTDEmojis']              = dfMessages['procTDCleanText'].map(lambda x: demoji.findall_list(x, desc = False)) # Filter out emoji\n",
    "    dfMessages['procTDEmojisDesc']          = dfMessages['procTDCleanText'].map(lambda x: demoji.findall_list(x, desc = True)) # Filter out emoji with desc\n",
    "    dfMessages['procTDCleanText']           = dfMessages['procTDCleanText'].map(lambda x: demoji.replace(x, \" \")) # Rm emoji\n",
    "    dfMessages['procTDCleanText']           = dfMessages['procTDCleanText'].map(lambda x: gloReplaceGermanChars(x)) # Replace german chars\n",
    "    dfMessages['procTDSafeText']            = dfMessages['procTDCleanText'].map(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', ' ', x)) # Filter out . ! ? ... (get only safe chars)\n",
    "    dfMessages['procTDSafeLowercaseText']   = dfMessages['procTDSafeText'].map(lambda x: x.lower()) # To lower\n",
    "\n",
    "    # Calc text size\n",
    "    dfMessages[\"procTDTextLength\"]      = dfMessages[\"procTDCleanText\"].str.len()\n",
    "\n",
    "    # Add columns (if not exists)\n",
    "    if \"photo\" not in dfMessages:\n",
    "        print(\"- Debug: Add column >>photo<<\")\n",
    "        dfMessages[\"photo\"] = np.nan\n",
    "\n",
    "    if \"file\" not in dfMessages:\n",
    "        print(\"- Debug: Add column >>file<<\")\n",
    "        dfMessages[\"file\"] = np.nan\n",
    "\n",
    "    if \"edited\" not in dfMessages:\n",
    "        print(\"- Debug: Add column >>edited<<\")\n",
    "        dfMessages[\"edited\"] = np.nan\n",
    "\n",
    "    if \"forwarded_from\" not in dfMessages:\n",
    "        print(\"- Debug: Add column >>forwarded_from<<\")\n",
    "        dfMessages[\"forwarded_from\"] = np.nan\n",
    "\n",
    "    # Evaluate attributes\n",
    "    dfMessages[\"procEvalIsValidText\"]   = dfMessages.procTDTextLength.apply(evalIsValidText)\n",
    "\n",
    "    dfMessages[\"procEvalContainsPhoto\"] = dfMessages.photo.apply(evalContainsSomething)\n",
    "    dfMessages[\"procEvalContainsFile\"]  = dfMessages.file.apply(evalContainsSomething) \n",
    "    dfMessages[\"procEvalIsEdited\"]      = dfMessages.edited.apply(evalContainsSomething)\n",
    "    dfMessages[\"procEvalIsForwarded\"]   = dfMessages.forwarded_from.apply(evalContainsSomething)\n",
    "    \n",
    "    dfMessages[\"procEvalContainsUrl\"]              = dfMessages.procTDURLs.apply(evalNonEmptyList)\n",
    "    dfMessages[\"procEvalContainsHashtag\"]          = dfMessages.procTDHashtags.apply(evalNonEmptyList)\n",
    "    dfMessages[\"procEvalContainsBoldItem\"]         = dfMessages.procTDBolds.apply(evalNonEmptyList)\n",
    "    dfMessages[\"procEvalContainsItalicItem\"]       = dfMessages.procTDItalics.apply(evalNonEmptyList)\n",
    "    dfMessages[\"procEvalContainsUnderlineItem\"]    = dfMessages.procTDUnderlines.apply(evalNonEmptyList)\n",
    "    dfMessages[\"procEvalContainsEmailItem\"]        = dfMessages.procTDEmails.apply(evalNonEmptyList)\n",
    "    dfMessages['procEvalContainsEmojiItem']        = dfMessages.procTDEmojis.apply(evalNonEmptyList)\n",
    "\n",
    "    # Add to dict    \n",
    "    dictMessages[fP] = dfMessages\n",
    "    gloStopStopwatch(\"TD-Extract \" + fP)\n",
    "\n",
    "# All Messages to DataFrame\n",
    "gloStartStopwatch(\"Generate global DataFrame\")\n",
    "dfAllDataMessages = pd.DataFrame()\n",
    "for fP in dfInputFiles.inputPath:\n",
    "    dfMessages        = dictMessages[fP].copy()\n",
    "    dfAllDataMessages = dfAllDataMessages.append(dfMessages)\n",
    "gloStopStopwatch(\"Generate global DataFrame\")\n",
    "\n",
    "gloStopStopwatch(\"Extract Text Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Meta-Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "#df = dfAllDataMessages[[\"date\", \"text\", \"procTDCleanText\", \"procTDHashtags\", \"procEvalContainsHashtag\", \"procIsJsonFormatted\"]].copy()\n",
    "\n",
    "#df = df[df.procIsJsonFormatted == True]\n",
    "#df = df[df.procEvalContainsHashtag == True]\n",
    "#df = df[df.procTDCleanText.str.contains(\"#\")]\n",
    "\n",
    "#df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Type of channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfInputFiles.inputType.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Define queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def queryChatId(filePath):\n",
    "    dfMeta = dictMeta[filePath].copy()\n",
    "    return str(dfMeta[\"id\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def queryChatName(filePath):\n",
    "    dfMeta      = dictMeta[filePath].copy()\n",
    "    chatName    = str(dfMeta[\"name\"].iloc[0])\n",
    "    chatName    = gloConvertToSafeChatName(chatName)\n",
    "    return chatName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def queryChatType(filePath):\n",
    "    dfMeta = dictMeta[filePath].copy()\n",
    "    return str(dfMeta[\"type\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def queryNumberOfMessages(filePath):\n",
    "    dfMessages = dictMessages[filePath].copy()\n",
    "    return len(dfMessages.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def queryNumberOfMessagesByAttEqTrue(filePath, attKey):\n",
    "    dfMessages = dictMessages[filePath].copy()\n",
    "    dfMessages = dfMessages[dfMessages[attKey] == True]\n",
    "    return len(dfMessages.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Execute queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfQueryMeta = pd.DataFrame(dfInputFiles.inputPath)\n",
    "\n",
    "dfQueryMeta[\"qryChatId\"]                        = dfQueryMeta.inputPath.apply(queryChatId)\n",
    "dfQueryMeta[\"qryChatName\"]                      = dfQueryMeta.inputPath.apply(queryChatName)\n",
    "dfQueryMeta[\"qryChatType\"]                      = dfQueryMeta.inputPath.apply(queryChatType)\n",
    "dfQueryMeta[\"qryNumberOfMessages\"]              = dfQueryMeta.inputPath.apply(queryNumberOfMessages)\n",
    "\n",
    "dfQueryMeta[\"qryNumberOfFormattedTextMessages\"] = dfQueryMeta.apply(lambda x: queryNumberOfMessagesByAttEqTrue(x.inputPath, \"procIsJsonFormatted\"), axis=1)\n",
    "\n",
    "dfQueryMeta[\"qryNumberOfValidTextMessages\"]     = dfQueryMeta.apply(lambda x: queryNumberOfMessagesByAttEqTrue(x.inputPath, \"procEvalIsValidText\"), axis=1)\n",
    "\n",
    "dfQueryMeta[\"qryNumberOfPhotos\"]                = dfQueryMeta.apply(lambda x: queryNumberOfMessagesByAttEqTrue(x.inputPath, \"procEvalContainsPhoto\"), axis=1)\n",
    "dfQueryMeta[\"qryNumberOfFiles\"]                 = dfQueryMeta.apply(lambda x: queryNumberOfMessagesByAttEqTrue(x.inputPath, \"procEvalContainsFile\"), axis=1)\n",
    "dfQueryMeta[\"qryNumberOfEditedMessages\"]        = dfQueryMeta.apply(lambda x: queryNumberOfMessagesByAttEqTrue(x.inputPath, \"procEvalIsEdited\"), axis=1)\n",
    "dfQueryMeta[\"qryNumberOfForwardedMessages\"]     = dfQueryMeta.apply(lambda x: queryNumberOfMessagesByAttEqTrue(x.inputPath, \"procEvalIsForwarded\"), axis=1)\n",
    "\n",
    "dfQueryMeta[\"qryNumberOfMessagesWithUrl\"]           = dfQueryMeta.apply(lambda x: queryNumberOfMessagesByAttEqTrue(x.inputPath, \"procEvalContainsUrl\"), axis=1)\n",
    "dfQueryMeta[\"qryNumberOfMessagesWithHashtag\"]       = dfQueryMeta.apply(lambda x: queryNumberOfMessagesByAttEqTrue(x.inputPath, \"procEvalContainsHashtag\"), axis=1)\n",
    "dfQueryMeta[\"qryNumberOfMessagesWithBold\"]          = dfQueryMeta.apply(lambda x: queryNumberOfMessagesByAttEqTrue(x.inputPath, \"procEvalContainsBoldItem\"), axis=1)\n",
    "dfQueryMeta[\"qryNumberOfMessagesWithItalic\"]        = dfQueryMeta.apply(lambda x: queryNumberOfMessagesByAttEqTrue(x.inputPath, \"procEvalContainsItalicItem\"), axis=1)\n",
    "dfQueryMeta[\"qryNumberOfMessagesWithUnderline\"]     = dfQueryMeta.apply(lambda x: queryNumberOfMessagesByAttEqTrue(x.inputPath, \"procEvalContainsUnderlineItem\"), axis=1)\n",
    "dfQueryMeta[\"qryNumberOfMessagesWithEmail\"]         = dfQueryMeta.apply(lambda x: queryNumberOfMessagesByAttEqTrue(x.inputPath, \"procEvalContainsEmailItem\"), axis=1)\n",
    "dfQueryMeta[\"qryNumberOfMessagesWithEmoji\"]         = dfQueryMeta.apply(lambda x: queryNumberOfMessagesByAttEqTrue(x.inputPath, \"procEvalContainsEmojiItem\"), axis=1)\n",
    "\n",
    "dfQueryMeta.sort_values(by=\"qryNumberOfMessages\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Plot meta queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Auto label query plot\n",
    "def autolabelAx(rects, ax):\n",
    "    \"\"\"\n",
    "    Attach a text label above each bar in *rects*, displaying its height.\n",
    "    Copied from https://matplotlib.org/3.1.1/gallery/lines_bars_and_markers/barchart.html (22.12.2020)\n",
    "    \"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# param inputDescFilter set \"\" == no filter\n",
    "# param outputFilename set \"\" = no output\n",
    "def queryMetaPlotter(inputDescFilter, configPlotWidth, configPlotHeight, configBarWidth, outputFilename):\n",
    "    # Init data\n",
    "    dataLabels                          = list()\n",
    "    dataNumberOfMesssages               = list()\n",
    "    dataNumberOfFormattedTextMessages   = list()\n",
    "    dataNumberOfValidTextMessages       = list()\n",
    "    dataNumberOfEditedMessages          = list()\n",
    "    dataNumberOfForwardedMessages       = list()\n",
    "    dataNumberOfPhotos                  = list()\n",
    "    dataNumberOfFiles                   = list()\n",
    "    dataNumberOfMessagesWUrl            = list()\n",
    "    dataNumberOfMessagesWHashtag        = list()\n",
    "    dataNumberOfMessagesWBold           = list()\n",
    "    dataNumberOfMessagesWItalic         = list()\n",
    "    dataNumberOfMessagesWUnderline      = list()\n",
    "    dataNumberOfMessagesWEmail          = list()\n",
    "    dataNumberOfMessagesWEmoji          = list()\n",
    "\n",
    "    # Iterate over Meta DataFrame\n",
    "    for index, row in dfQueryMeta.sort_values(by=\"qryNumberOfMessages\", ascending=False).iterrows():\n",
    "\n",
    "        # Get attributes (check filter)\n",
    "        if(inputDescFilter == \"\" or dfInputFiles[dfInputFiles.inputPath == row.inputPath].inputDesc.iloc[0] == inputDescFilter):\n",
    "            dataLabels                          .append(row.qryChatName)\n",
    "            dataNumberOfMesssages               .append(row.qryNumberOfMessages)\n",
    "            dataNumberOfFormattedTextMessages   .append(row.qryNumberOfFormattedTextMessages)\n",
    "            dataNumberOfValidTextMessages       .append(row.qryNumberOfValidTextMessages)\n",
    "            dataNumberOfEditedMessages          .append(row.qryNumberOfEditedMessages)\n",
    "            dataNumberOfForwardedMessages       .append(row.qryNumberOfForwardedMessages)\n",
    "            dataNumberOfPhotos                  .append(row.qryNumberOfPhotos)\n",
    "            dataNumberOfFiles                   .append(row.qryNumberOfFiles)\n",
    "            dataNumberOfMessagesWUrl            .append(row.qryNumberOfMessagesWithUrl)\n",
    "            dataNumberOfMessagesWHashtag        .append(row.qryNumberOfMessagesWithHashtag)\n",
    "            dataNumberOfMessagesWBold           .append(row.qryNumberOfMessagesWithBold)\n",
    "            dataNumberOfMessagesWItalic         .append(row.qryNumberOfMessagesWithItalic)\n",
    "            dataNumberOfMessagesWUnderline      .append(row.qryNumberOfMessagesWithUnderline)\n",
    "            dataNumberOfMessagesWEmail          .append(row.qryNumberOfMessagesWithEmail)\n",
    "            dataNumberOfMessagesWEmoji          .append(row.qryNumberOfMessagesWithEmoji)\n",
    "\n",
    "    # Convert list to array\n",
    "    dataLabels                          = np.array(dataLabels)\n",
    "    dataNumberOfMesssages               = np.array(dataNumberOfMesssages)\n",
    "    dataNumberOfFormattedTextMessages   = np.array(dataNumberOfFormattedTextMessages)\n",
    "    dataNumberOfValidTextMessages       = np.array(dataNumberOfValidTextMessages)\n",
    "    dataNumberOfEditedMessages          = np.array(dataNumberOfEditedMessages)\n",
    "    dataNumberOfForwardedMessages       = np.array(dataNumberOfForwardedMessages)\n",
    "    dataNumberOfPhotos                  = np.array(dataNumberOfPhotos)\n",
    "    dataNumberOfFiles                   = np.array(dataNumberOfFiles)\n",
    "    dataNumberOfMessagesWUrl            = np.array(dataNumberOfMessagesWUrl)\n",
    "    dataNumberOfMessagesWHashtag        = np.array(dataNumberOfMessagesWHashtag)\n",
    "    dataNumberOfMessagesWBold           = np.array(dataNumberOfMessagesWBold)\n",
    "    dataNumberOfMessagesWItalic         = np.array(dataNumberOfMessagesWItalic)\n",
    "    dataNumberOfMessagesWUnderline      = np.array(dataNumberOfMessagesWUnderline)\n",
    "    dataNumberOfMessagesWEmail          = np.array(dataNumberOfMessagesWEmail)\n",
    "    dataNumberOfMessagesWEmoji          = np.array(dataNumberOfMessagesWEmoji)\n",
    "\n",
    "    # Draw\n",
    "    with sns.color_palette(\"tab10\", 11):\n",
    "        fig, ax = plt.subplots()\n",
    "    x = np.arange(len(dataLabels))\n",
    "\n",
    "    barWidth = configBarWidth\n",
    "\n",
    "    fig.set_figwidth(configPlotWidth)\n",
    "    fig.set_figheight(configPlotHeight)\n",
    "\n",
    "    r1 = x\n",
    "    r2 = [x + barWidth for x in r1]\n",
    "    r3 = [x + barWidth for x in r2]\n",
    "    r4 = [x + barWidth for x in r3]\n",
    "    r5 = [x + barWidth for x in r4]\n",
    "    r6 = [x + barWidth for x in r5]\n",
    "    r7 = [x + barWidth for x in r6]\n",
    "    r8 = [x + barWidth for x in r7]\n",
    "    r9 = [x + barWidth for x in r8]\n",
    "    r10 = [x + barWidth for x in r9]\n",
    "    r11 = [x + barWidth for x in r10]\n",
    "    r12 = [x + barWidth for x in r11]\n",
    "    r13 = [x + barWidth for x in r12]\n",
    "    r14 = [x + barWidth for x in r13]\n",
    "\n",
    "    rects1 = ax.bar(r1, dataNumberOfMesssages, barWidth, label='Messages')\n",
    "    rects2 = ax.bar(r2, dataNumberOfFormattedTextMessages, barWidth, label='Formatted Messsages')\n",
    "    rects3 = ax.bar(r3, dataNumberOfValidTextMessages, barWidth, label='Valid Text Messages')\n",
    "    rects4 = ax.bar(r4, dataNumberOfEditedMessages, barWidth, label='Edited Messages')\n",
    "    rects5 = ax.bar(r5, dataNumberOfForwardedMessages, barWidth, label='Forwarded Messages')\n",
    "    rects6 = ax.bar(r6, dataNumberOfPhotos, barWidth, label='with Photo')\n",
    "    rects7 = ax.bar(r7, dataNumberOfFiles, barWidth, label='with File')\n",
    "    rects8 = ax.bar(r8, dataNumberOfMessagesWUrl, barWidth, label='with Url')\n",
    "    rects9 = ax.bar(r9, dataNumberOfMessagesWHashtag, barWidth, label='with Hashtag')\n",
    "    rects10 = ax.bar(r10, dataNumberOfMessagesWBold, barWidth, label='with Bold Items')\n",
    "    rects11 = ax.bar(r11, dataNumberOfMessagesWItalic, barWidth, label='with Italic Items')\n",
    "    rects12 = ax.bar(r12, dataNumberOfMessagesWUnderline, barWidth, label='with Underlined Items')\n",
    "    rects13 = ax.bar(r13, dataNumberOfMessagesWEmail, barWidth, label='with E-Mails')\n",
    "    rects14 = ax.bar(r14, dataNumberOfMessagesWEmoji, barWidth, label='with Emojis')\n",
    "\n",
    "    chartTitle = \"\"\n",
    "    if(inputDescFilter != \"\"):\n",
    "        chartTitle = \" (\" + inputDescFilter + \")\"\n",
    "\n",
    "    ax.set_ylabel(\"Number of\")\n",
    "    ax.set_title(\"Meta Overview\" + chartTitle)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(dataLabels)\n",
    "    ax.legend()\n",
    "\n",
    "    rects = [rects1, rects2, rects3, rects4, rects5, rects6, rects7, rects8, rects9, rects10, rects11, rects12, rects13, rects14]\n",
    "\n",
    "    for rect in rects:\n",
    "        autolabelAx(rect, ax)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    #plt.xticks(rotation=30)\n",
    "    \n",
    "    if(outputFilename != \"\"):\n",
    "        plt.savefig(dir_var_output + outputFilename)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "queryMetaPlotter(\n",
    "    inputDescFilter = \"dataSet0\",\n",
    "    configPlotWidth = 32,\n",
    "    configPlotHeight = 9,\n",
    "    configBarWidth = 0.065,\n",
    "    outputFilename = \"meta-overview-dataSet0.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if(\"dataSet1\" in C_LOAD_DATASET):\n",
    "    queryMetaPlotter(\n",
    "        inputDescFilter = \"dataSet1\",\n",
    "        configPlotWidth = 100,\n",
    "        configPlotHeight = 9,\n",
    "        configBarWidth = 0.065,\n",
    "        outputFilename = \"meta-overview-dataSet1.png\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if(\"dataSet1a\" in C_LOAD_DATASET):\n",
    "    queryMetaPlotter(\n",
    "        inputDescFilter = \"dataSet1a\",\n",
    "        configPlotWidth = 16,\n",
    "        configPlotHeight = 9,\n",
    "        configBarWidth = 0.065,\n",
    "        outputFilename = \"meta-overview-dataSet1a.png\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if(\"dataSet2\" in C_LOAD_DATASET):\n",
    "    queryMetaPlotter(\n",
    "        inputDescFilter = \"dataSet2\",\n",
    "        configPlotWidth = 34,\n",
    "        configPlotHeight = 9,\n",
    "        configBarWidth = 0.065,\n",
    "        outputFilename = \"meta-overview-dataSet2.png\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Get text-length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def removeTextLengthOutliersFromDataFrame(df, interval, maxTextLength):\n",
    "    df = df.copy()\n",
    "    df = df[df.procTDTextLength < maxTextLength]\n",
    "    # https://stackoverflow.com/questions/23199796/detect-and-exclude-outliers-in-pandas-data-frame\n",
    "    # keep only the ones that are within <interval> to -<interval> standard deviations in the column 'Data'.\n",
    "    return df[np.abs(df.procTDTextLength-df.procTDTextLength.mean()) <= (interval*df.procTDTextLength.std())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# param outputFilename set \"\" == no output file\n",
    "def textLengthHistPlotter(outputFilename):\n",
    "    dfMessages = dfAllDataMessages.copy()\n",
    "    print(\"Number of all messages:\\t\\t\\t\\t\\t\\t\" + str(len(dfMessages.index)))\n",
    "\n",
    "    dfMessages = dfMessages[dfMessages.procEvalIsValidText == True]\n",
    "    print(\"Number of valid text messages:\\t\\t\\t\\t\\t\" + str(len(dfMessages.index)))\n",
    "\n",
    "    dfMessages = removeTextLengthOutliersFromDataFrame(\n",
    "        dfMessages,\n",
    "        interval = 3,               #Default is 3\n",
    "        maxTextLength = 999999999   #TODO: Maybe enable max text length\n",
    "        )\n",
    "    print(\"Number of valid text messages (after outliers filtering):\\t\" + str(len(dfMessages.index)))\n",
    "\n",
    "    print()\n",
    "    print(\"Text Length Hist (after outliers filtering)\")\n",
    "    plt.figure(figsize=(8,4.5))\n",
    "    _ = dfMessages.procTDTextLength.hist(bins=40)\n",
    "    plt.title('Histogram Text Length')\n",
    "\n",
    "    if(outputFilename != \"\"):\n",
    "        plt.savefig(dir_var_output + outputFilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "textLengthHistPlotter(outputFilename = \"meta-text-length-hist.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Compare ids and labels (has chat name changed?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compareIdsAndLabels(df):\n",
    "\n",
    "    gloStartStopwatch(\"Compare ids and labels\")\n",
    "\n",
    "    dictFromTranslator  = {}\n",
    "    dictActorTranslator = {}\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    \n",
    "    df = df.set_index(\"date\")\n",
    "    df = df.sort_index()\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        n_from      = row[\"from\"]\n",
    "        n_from_id   = row[\"from_id\"]\n",
    "\n",
    "        n_actor      = row[\"actor\"]\n",
    "        n_actor_id   = row[\"actor_id\"]\n",
    "\n",
    "        if(str(n_from) != \"nan\"):\n",
    "            if(n_from_id not in dictFromTranslator):\n",
    "                # Add new key\n",
    "                #print(\"- Add from \" + str(n_from_id) + \"/\" + str(n_from))\n",
    "                dictFromTranslator[n_from_id] = n_from\n",
    "            else:\n",
    "                # Has changed?\n",
    "                oValue = dictFromTranslator[n_from_id]\n",
    "                if(oValue != n_from):\n",
    "                    print(\"- Add changed attribute in from (prev=\" + str(oValue) + \"/new=\" + str(n_from) + \")\")\n",
    "                    dictFromTranslator[n_from_id] = n_from\n",
    "\n",
    "        if(str(n_actor) != \"nan\"):\n",
    "            if(n_actor_id not in dictActorTranslator):\n",
    "                # Add new key\n",
    "                #print(\"- Add actor \" + str(n_actor_id) + \"/\" + str(n_actor))\n",
    "                dictActorTranslator[n_actor_id] = n_actor\n",
    "            else:\n",
    "                # Has changed?\n",
    "                oValue = dictActorTranslator[n_actor_id]\n",
    "                if(oValue != n_actor):\n",
    "                    print(\"- Add changed attribute in actor (prev=\" + str(oValue) + \"/new=\" + str(n_actor) + \")\")\n",
    "                    dictActorTranslator[n_actor_id] = n_actor\n",
    "\n",
    "    gloStopStopwatch(\"Compare ids and labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    compareIdsAndLabels(dfAllDataMessages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Extract Social Graph Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extractImportantHashtags(df):\n",
    "    dfMessages = df.copy()\n",
    "    dfMessages = dfMessages[dfMessages.procEvalContainsHashtag == True]\n",
    "\n",
    "    hashTagList = list()\n",
    "    for index, row in dfMessages.iterrows():\n",
    "        for hashtagItem in row[\"procTDHashtags\"]:\n",
    "            hashTagList.append(hashtagItem)\n",
    "\n",
    "    return hashTagList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# return combinations\n",
    "def extractImportantEmojis(df):\n",
    "\n",
    "    dfMessages = df.copy()\n",
    "    dfMessages = dfMessages[dfMessages.procEvalContainsEmojiItem == True]\n",
    "\n",
    "    li = dfMessages.procTDEmojisDesc.values.tolist()\n",
    "\n",
    "    retLi = list()\n",
    "\n",
    "    for l in li:\n",
    "        aString = \"\"\n",
    "        for e in l:\n",
    "            aString = aString + \":\" + e \n",
    "        retLi.append(aString)\n",
    "\n",
    "    return retLi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# param flagResolveNewUrls  Flag (see config above)\n",
    "def resolveUrl(completeUrl, flagResolveNewUrls):\n",
    "    \n",
    "    if \"bit.ly\" in completeUrl:\n",
    "\n",
    "        if(gloCheckIsAlreadyCached(\"resolved-urls.csv\", completeUrl)):\n",
    "            return gloGetCached(\"resolved-urls.csv\", completeUrl)\n",
    "        else:\n",
    "\n",
    "            if(flagResolveNewUrls == False):\n",
    "                print(\"(Disable resolve new urls (return completeUrl) >>\" + completeUrl + \"<<)\")\n",
    "                return completeUrl\n",
    "\n",
    "            print(\"(Resolve now >>\" + completeUrl + \"<<)\")\n",
    "            try:\n",
    "                r = requests.get(completeUrl, timeout = 5)\n",
    "                u = r.url\n",
    "                gloAddToCache(\"resolved-urls.csv\", completeUrl, u)\n",
    "                return u\n",
    "            except:\n",
    "                print(\"(- Warn: Can not resolve (return completeUrl))\")\n",
    "                return completeUrl\n",
    "\n",
    "    else:\n",
    "        return completeUrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Return\n",
    "# a = urlList,\n",
    "# b = refList\n",
    "# c = hostList\n",
    "def extractImportantUrls(df):\n",
    "    dfMessages = df.copy()\n",
    "    dfMessages = dfMessages[dfMessages.procEvalContainsUrl == True]\n",
    "\n",
    "    hostList        = list()\n",
    "    urList          = list()\n",
    "    refList         = list()\n",
    "\n",
    "    counterSucHostname = 0\n",
    "    counterErrHostname = 0\n",
    "\n",
    "    for index, row in dfMessages.iterrows():\n",
    "        for urlItem in row[\"procTDURLs\"]:\n",
    "            \n",
    "            urlData = urlparse(str(urlItem))\n",
    "\n",
    "            completeUrl      = urlData.geturl()\n",
    "\n",
    "            rUrl     = resolveUrl(completeUrl, flagResolveNewUrls=C_RESOLVE_NEW_URLS)\n",
    "            rUrlData = urlparse(rUrl)\n",
    "            rCompleteUrl = rUrlData.geturl()\n",
    "            rCompleteHostname = rUrlData.hostname\n",
    "\n",
    "            if(str(rCompleteHostname) != \"None\"):\n",
    "                counterSucHostname = counterSucHostname + 1\n",
    "\n",
    "                hostList.append(str(rCompleteHostname))\n",
    "\n",
    "                urList.append(str(rCompleteUrl))\n",
    "\n",
    "                if \"t.me\" in str(rCompleteHostname):\n",
    "                    refList.append(str(rCompleteUrl))\n",
    "            else:\n",
    "                counterErrHostname = counterErrHostname + 1\n",
    "\n",
    "    print(\"Got Hostnames (suc=\" + str(counterSucHostname) + \"/err=\" + str(counterErrHostname) + \")\")\n",
    "\n",
    "    return (urList, refList, hostList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# param flagResolveNewUrls  Flag (see config above)\n",
    "def resolveImportantYoutubeVideos(urlList, flagResolveNewUrls):\n",
    "\n",
    "    # Thanks https://gist.github.com/rodrigoborgesdeoliveira/987683cfbfcc8d800192da1e73adc486\n",
    "\n",
    "    ytList = list()\n",
    "\n",
    "    for url in urlList:\n",
    "\n",
    "        url = str(url)\n",
    "\n",
    "        if(\"youtube.com\" in url or \"youtu.be\" in url or \"youtube-nocookie.com\" in url):\n",
    "            if(gloCheckIsAlreadyCached(\"resolved-youtube.csv\", url)):\n",
    "                ytList.append(gloGetCached(\"resolved-youtube.csv\", url)) \n",
    "            else:\n",
    "\n",
    "                if(flagResolveNewUrls == False):\n",
    "                    print(\"(Disable resolve new youtube urls (return completeUrl) >>\" + url + \"<<)\")\n",
    "                    ytList.append(url)\n",
    "\n",
    "                print(\"Resolve now youtube >>\" + url + \"<<\")\n",
    "                try:\n",
    "                    r = requests.get(url, timeout = 5)\n",
    "                    t = fromstring(r.content)\n",
    "                    a = str(t.findtext('.//title'))\n",
    "                    ytList.append(a)\n",
    "                    gloAddToCache(\"resolved-youtube.csv\", url, a)\n",
    "                except:\n",
    "                    print(\"(- Warn: Can not resolve youtube url (return completeUrl))\")\n",
    "                    ytList.append(url)\n",
    "\n",
    "    return ytList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Bug: No Hostname detected if string startsWith ! \"http\" in urlparse\n",
    "# TODO: Check: Refs ins both directions\n",
    "\n",
    "# Returns\n",
    "# a = Counter forwardedFromList\n",
    "# b = Counter refList\n",
    "# c = Counter hashtagList\n",
    "# d = Counter hostList\n",
    "# e = Counter emojiList\n",
    "def extractSocialGraph(filePath, debugPrint, debugPrintCount):\n",
    "\n",
    "    dfMessages = dictMessages[filePath].copy()\n",
    "\n",
    "    hashtagList = extractImportantHashtags(dfMessages)\n",
    "    emojiList = extractImportantEmojis(dfMessages)\n",
    "\n",
    "    urlList, refList, hostList = extractImportantUrls(dfMessages)\n",
    "\n",
    "    ytList = resolveImportantYoutubeVideos(urlList, flagResolveNewUrls = C_RESOLVE_NEW_URLS)\n",
    "            \n",
    "    forwardedFromList = list()\n",
    "    if(\"forwarded_from\" in dfMessages.columns):\n",
    "        df = dfMessages.copy()\n",
    "        df = df[df.procEvalIsForwarded == True]\n",
    "    \n",
    "        for index, row in df.iterrows():        \n",
    "            forwardedFromList.append(str(row[\"forwarded_from\"]))\n",
    "            \n",
    "    actorList = list()\n",
    "    if(\"actor\" in dfMessages.columns):\n",
    "        for index, row in dfMessages.iterrows():\n",
    "            actorList.append(str(row[\"actor\"]))\n",
    "    \n",
    "    memberList = list()\n",
    "    if(\"members\" in dfMessages.columns):\n",
    "        for index, row in dfMessages.iterrows():\n",
    "            if(str(row[\"members\"]) != \"nan\"):\n",
    "                for memberItem in row[\"members\"]:\n",
    "                    memberList.append(str(memberItem))\n",
    "                    \n",
    "    fromList = list()\n",
    "    if(\"from\" in dfMessages.columns):\n",
    "        for index, row in dfMessages.iterrows():\n",
    "            fromList.append(str(row[\"from\"]))\n",
    "            \n",
    "    savedFromList = list()\n",
    "    if(\"saved_from\" in dfMessages.columns):\n",
    "        for index, row in dfMessages.iterrows():\n",
    "            savedFromList.append(str(row[\"saved_from\"]))\n",
    "\n",
    "    configTopN = debugPrintCount\n",
    "\n",
    "    if(debugPrint):\n",
    "\n",
    "        print()\n",
    "        print(\"Set top n to \" + str(debugPrintCount))\n",
    "        print()\n",
    "\n",
    "        print(\"- Top Hosts (resovled) -\")\n",
    "        print (\"\\n\".join(map(str, Counter(hostList).most_common(configTopN))))\n",
    "        print()\n",
    "        print(\"- Top URLs (resolved) -\")\n",
    "        print (\"\\n\".join(map(str, Counter(urlList).most_common(configTopN))))\n",
    "        print()\n",
    "        print(\"- Top Refs from text (resolved) -\")\n",
    "        print (\"\\n\".join(map(str, Counter(refList).most_common(configTopN))))\n",
    "        print()\n",
    "        print(\"- Top Refs (forwarded_from) -\")\n",
    "        print (\"\\n\".join(map(str, Counter(forwardedFromList).most_common(configTopN))))\n",
    "        print()\n",
    "        print(\"- Top Refs (actor) -\")\n",
    "        print (\"\\n\".join(map(str, Counter(actorList).most_common(configTopN))))\n",
    "        print()\n",
    "        print(\"- Top Refs (members) -\")\n",
    "        print (\"\\n\".join(map(str, Counter(memberList).most_common(configTopN))))\n",
    "        print()\n",
    "        print(\"- Top Refs (from) -\")\n",
    "        print (\"\\n\".join(map(str, Counter(fromList).most_common(configTopN))))\n",
    "        print()\n",
    "        print(\"- Top Refs (saved_from) -\")\n",
    "        print (\"\\n\".join(map(str, Counter(savedFromList).most_common(configTopN))))\n",
    "        print()\n",
    "        print(\"- Top hashtags -\")\n",
    "        print (\"\\n\".join(map(str, Counter(hashtagList).most_common(configTopN))))\n",
    "        print()\n",
    "        print(\"- Top emojis -\")\n",
    "        print (\"\\n\".join(map(str, Counter(emojiList).most_common(configTopN))))\n",
    "        print()\n",
    "        print(\"- Top yt -\")\n",
    "        print (\"\\n\".join(map(str, Counter(ytList).most_common(configTopN))))\n",
    "        print()\n",
    "    \n",
    "    return (Counter(forwardedFromList), Counter(refList), Counter(hashtagList),  Counter(hostList), Counter(emojiList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dictSGD_ForwardedFrom = {}\n",
    "dictSGD_Ref           = {}\n",
    "dictSGD_Hashtag       = {}\n",
    "dictSGD_Host          = {}\n",
    "dictSGD_Emoji         = {}\n",
    "\n",
    "gloStartStopwatch(\"Extract Social Graph Data\")\n",
    "\n",
    "for fP in dfInputFiles.inputPath:\n",
    "\n",
    "    gloStartStopwatch(\"Extract Social Graph Data >>\" + fP + \"<<\")\n",
    "\n",
    "    a, b, c, d, e = extractSocialGraph(fP, debugPrint=False, debugPrintCount = 0)\n",
    "\n",
    "    dictSGD_ForwardedFrom[fP]   = a\n",
    "    dictSGD_Ref[fP]             = b\n",
    "    dictSGD_Hashtag[fP]         = c\n",
    "    dictSGD_Host[fP]            = d\n",
    "    dictSGD_Emoji[fP]           = e\n",
    "\n",
    "    gloStopStopwatch(\"Extract Social Graph Data >>\" + fP + \"<<\")\n",
    "\n",
    "gloStopStopwatch(\"Extract Social Graph Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def printSocialGraphDebug(filePathList):\n",
    "    for fP in filePathList:\n",
    "        print(\"Analyse now >>\" + fP + \"<<\")\n",
    "        _ = extractSocialGraph(fP, debugPrint=True, debugPrintCount=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    printSocialGraphDebug(dfInputFiles[dfInputFiles.inputDesc == \"dataSet0\"].inputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    printSocialGraphDebug(dfInputFiles[dfInputFiles.inputDesc == \"dataSet1a\"].inputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    printSocialGraphDebug(dfInputFiles[dfInputFiles.inputDesc == \"dataSet2\"].inputPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Get Top Influencer (Downloaded?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get Top Influencer\n",
    "# param fPList      filePath List\n",
    "# param configTopN  Get Top n influencer e.g. 10\n",
    "def getTopInfluencer(fPList, configTopN):\n",
    "\n",
    "    for fP in fPList:\n",
    "\n",
    "        chatName = queryChatName(fP)\n",
    "\n",
    "        print()\n",
    "        print(\"Analyse Chat (Forwarded From) >>\" + chatName + \"<<\")\n",
    "        \n",
    "        socialGraphData = dictSGD_ForwardedFrom[fP]\n",
    "        socialGraphData = socialGraphData.most_common(configTopN)\n",
    "\n",
    "        counter = 1\n",
    "\n",
    "        # Iterate over data\n",
    "        for oChatName, oChatRefs in socialGraphData:\n",
    "            \n",
    "            # Query other params\n",
    "            oChatName    = gloConvertToSafeChatName(str(oChatName))\n",
    "            oChatRefs    = oChatRefs\n",
    "\n",
    "            # Already downloaded?\n",
    "            flagDownloaded = False\n",
    "            if oChatName in dfQueryMeta.qryChatName.values:\n",
    "                flagDownloaded = True\n",
    "\n",
    "            if(oChatName != \"nan\"):\n",
    "\n",
    "                print(str(counter) + \": (downloaded=\" + str(flagDownloaded) + \") (refs=\" + str(oChatRefs) + \")\\t\\t>>\" + str(oChatName) + \"<<\")\n",
    "                counter = counter + 1\n",
    "\n",
    "\n",
    "        print()\n",
    "        print(\"Analyse Chat (Refs) >>\" + chatName + \"<<\")\n",
    "        \n",
    "        socialGraphData = dictSGD_Ref[fP]\n",
    "        socialGraphData = socialGraphData.most_common(configTopN)\n",
    "\n",
    "        counter = 1\n",
    "\n",
    "        # Iterate over data\n",
    "        for oChatName, oChatRefs in socialGraphData:\n",
    "            \n",
    "            # Query other params\n",
    "            oChatName    = str(oChatName)\n",
    "            oChatRefs    = oChatRefs\n",
    "\n",
    "            if(oChatName != \"nan\"):\n",
    "\n",
    "                print(str(counter) + \" (refs=\" + str(oChatRefs) + \")\\t\\t>>\" + str(oChatName) + \"<<\")\n",
    "                counter = counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Can not get all items in dataSet1\n",
    "\n",
    "\"\"\"\n",
    "# Attila Hildmann #\n",
    "- Anonymous Germany - not found\n",
    "- https://t.me/DEMOKRATENCHAT - no entries\n",
    "- https://t.me/ChatDerFreiheit - no entries\n",
    "- https://t.me/FREIHEITSCHAT2020 - not found\n",
    "\n",
    "# Oliver Janich #\n",
    "- Oliver Janich Premium - not found\n",
    "\n",
    "# Xavier Naidoo #\n",
    "- Xavier(Der VereiNiger)Naidoo😎 - not found\n",
    "- https://t.me/PostAppender_bot - bot chat\n",
    "\"\"\"\n",
    "getTopInfluencer(list(dfInputFiles[dfInputFiles.inputDesc == \"dataSet0\"].inputPath), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Plot Social Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Social Graph Layout Selector\n",
    "\n",
    "param G Graph\n",
    "param layoutSelector:\n",
    "\n",
    "1 = Kamda Kawai Layout\n",
    "2 = Spring Layout\n",
    "3 = Graphviz Layout\n",
    "\"\"\"\n",
    "def getSocialGraphLayout(layoutSelector, G):\n",
    "    switcher = {\n",
    "        1: nx.kamada_kawai_layout(G.to_undirected()),\n",
    "        2: nx.spring_layout(G.to_undirected(), k = 0.15, iterations=200),\n",
    "        3: nx.nx_pydot.graphviz_layout(G)\n",
    "    }\n",
    "    return switcher.get(layoutSelector, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Try different arrows (see below): https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.patches.ArrowStyle.html\n",
    "# TODO: Check distances between nodes\n",
    "\n",
    "\"\"\"\n",
    "Draw social grah\n",
    "\n",
    "param   G                           graph\n",
    "param   layoutSelector              see above\n",
    "param   configFactorEdge            e.g. 100 => weight / 100\n",
    "param   configFactorNode            e.g. 10  => weight / 10\n",
    "param   configArrowSize             e.g. 5\n",
    "param   configPlotWidth             e.g. 16\n",
    "param   configPlotHeight            e.g. 9\n",
    "param   outputFilename              e.g. test.png (set \"\" == no output file)\n",
    "param   outputTitle                 e.g. Graph (required)\n",
    "\"\"\"\n",
    "def drawSocialGraph(G, layoutSelector, configFactorEdge, configFactorNode, configArrowSize, configPlotWidth, configPlotHeight, outputFilename, outputTitle):\n",
    "    \n",
    "    gloStartStopwatch(\"Social Graph Plot\")\n",
    "    \n",
    "    plt.figure(figsize=(configPlotWidth,configPlotHeight))\n",
    "        \n",
    "    pos = getSocialGraphLayout(layoutSelector = layoutSelector, G = G)\n",
    "    \n",
    "    # Clean edges\n",
    "    edges       = nx.get_edge_attributes(G, \"weight\")\n",
    "    edgesTLabel = nx.get_edge_attributes(G, \"tLabel\")\n",
    "\n",
    "    clean_edges         = dict()\n",
    "    clean_edges_labels  = dict()\n",
    "    \n",
    "    for key in edges:\n",
    "        \n",
    "        #Set edge weight\n",
    "        clean_edges[key]        = (100 - edges[key]) / configFactorEdge\n",
    "\n",
    "        #set edge layout\n",
    "        clean_edges_labels[key] = edgesTLabel[key]\n",
    "    \n",
    "    # Clean nodes\n",
    "    nodes       = nx.get_node_attributes(G,'weight')\n",
    "    nodesTLabel = nx.get_node_attributes(G,'tLabel')\n",
    "    nodesTColor = nx.get_node_attributes(G,'tColor')\n",
    "\n",
    "    clean_nodes         = dict()\n",
    "    clean_nodes_labels  = dict()\n",
    "    clean_nodes_color   = dict()\n",
    "    \n",
    "    for key in nodes:\n",
    "        \n",
    "        #Set node weight        \n",
    "        clean_nodes[key]        = nodes[key] / configFactorNode\n",
    "\n",
    "        #Set node layout\n",
    "        clean_nodes_labels[key] = nodesTLabel[key]\n",
    "        clean_nodes_color[key]  = nodesTColor[key]\n",
    "    \n",
    "    # Revert DiGraph (arrows direction)\n",
    "    G_rev = nx.DiGraph.reverse(G)    \n",
    "\n",
    "    # Draw\n",
    "    nx.draw(G_rev,\n",
    "        pos,\n",
    "        with_labels=True,\n",
    "        width=list(clean_edges.values()),\n",
    "        node_size=list(clean_nodes.values()),\n",
    "        labels=clean_nodes_labels,\n",
    "        node_color=list(clean_nodes_color.values()),\n",
    "        arrowsize=configArrowSize,\n",
    "        arrowstyle=\"wedge\"\n",
    "        #connectionstyle=\"arc3, rad = 0.1\"\n",
    "    )\n",
    "    \n",
    "    # Set labels\n",
    "    _ = nx.draw_networkx_edge_labels(G_rev, pos, edge_labels=clean_edges_labels)\n",
    "\n",
    "    plt.title(outputTitle)\n",
    "\n",
    "    # Save and show fig\n",
    "    if(outputFilename != \"\"):\n",
    "        plt.savefig(dir_var_output + outputFilename)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    gloStopStopwatch(\"Social Graph Plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generates Test Graph\n",
    "def generateTestGraph():\n",
    "\n",
    "    G_weighted = nx.DiGraph()\n",
    "\n",
    "    G_weighted.add_edge(\"N1\", \"N2\", weight=100-30,  tLabel = \"(≙\" + str(100-30) + \")\")\n",
    "    G_weighted.add_edge(\"N1\", \"N3\", weight=100-10,  tLabel = \"(≙\" + str(100-10) + \")\")\n",
    "    G_weighted.add_edge(\"N1\", \"N4\", weight=100-60,  tLabel = \"(≙\" + str(100-60) + \")\")\n",
    "\n",
    "    G_weighted.add_edge(\"N4\", \"N5\", weight=100-80,  tLabel = \"(≙\" + str(100-80) + \")\")\n",
    "    G_weighted.add_edge(\"N4\", \"N6\", weight=100-10,  tLabel = \"(≙\" + str(100-10) + \")\")\n",
    "\n",
    "    G_weighted.add_edge(\"N4\", \"N7\", weight=100-30,   tLabel = \"(≙\" + str(100-30) + \")\")\n",
    "    G_weighted.add_edge(\"N7\", \"N4\", weight=100-70,   tLabel = \"(≙\" + str(100-70) + \")\")\n",
    "\n",
    "    G_weighted.add_node(\"N1\", weight=500.0, tLabel = \"N1-T\", tColor=\"red\")\n",
    "    G_weighted.add_node(\"N2\", weight=500.0, tLabel = \"N2-T\", tColor=\"blue\")\n",
    "    G_weighted.add_node(\"N3\", weight=500.0, tLabel = \"N3-T\", tColor=\"blue\")\n",
    "    G_weighted.add_node(\"N4\", weight=500.0, tLabel = \"N4-T\", tColor=\"red\")\n",
    "    G_weighted.add_node(\"N5\", weight=500.0, tLabel = \"N5-T\", tColor=\"red\")\n",
    "    G_weighted.add_node(\"N6\", weight=500.0, tLabel = \"N6-T\", tColor=\"red\")\n",
    "    G_weighted.add_node(\"N7\", weight=500.0, tLabel = \"N7-T\", tColor=\"blue\")\n",
    "\n",
    "    return G_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add node weight to dict\n",
    "# Only adds new weight if newWeight > oldWeight\n",
    "def addSocialGraphNodeWeight(chatName, chatWeight, targetDict):\n",
    "    \n",
    "    if(chatName in targetDict):\n",
    "        oldWeight = targetDict[chatName]\n",
    "        if(chatWeight > oldWeight):\n",
    "            targetDict[chatName] = chatWeight\n",
    "    else:\n",
    "        targetDict[chatName] = chatWeight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate social graph\n",
    "\n",
    "param   configTopNInfluencer        e.g. For top 10 = 10\n",
    "param   configMinRefs               e.g. 1 must have > 1 % forwarded messages\n",
    "param   listFilePaths               List process filePaths\n",
    "param   socialGraphTargetDict       e.g. forwarded from dict or hashtag dict\n",
    "param   socialGraphTargetAttribute  e.g. procEvalIsForwarded (for calc percent)\n",
    "param   configFlagDebugLabel        e.g. show debug info on label\n",
    "\"\"\"\n",
    "def generateSocialGraph(configTopNInfluencer, configMinRefs, listFilePaths, socialGraphTargetDict, socialGraphTargetAttribute, configFlagDebugLabel):\n",
    "    \n",
    "    # Save node weights to dict\n",
    "    dictSocialNodeWeights   = dict()\n",
    "\n",
    "    # Flag downloaded nodes (exact node weight)\n",
    "    dictExactNodesLabels    = {}\n",
    "    \n",
    "    gloStartStopwatch(\"Social Graph\")\n",
    "    \n",
    "    # Generate directed graph\n",
    "    G_weighted = nx.DiGraph()\n",
    "    \n",
    "    print(\"- Add edges\")\n",
    "    for fP in listFilePaths:\n",
    "        \n",
    "        # Query own params\n",
    "        chatName                        = queryChatName(fP)\n",
    "        chatNumberOfMessages            = queryNumberOfMessages(fP)\n",
    "        chatNumberOfTargetMessages      = queryNumberOfMessagesByAttEqTrue(fP, socialGraphTargetAttribute)\n",
    "\n",
    "        gloStartStopwatch(\"SG-Extract \" + chatName + \"(\" + str(chatNumberOfTargetMessages) + \"/\" + str(chatNumberOfMessages) + \" messages)\")\n",
    "        \n",
    "        # Add exact node size (chat downloaded) and flag node\n",
    "        addSocialGraphNodeWeight(chatName, chatNumberOfMessages, dictSocialNodeWeights)\n",
    "        dictExactNodesLabels[chatName] = str(chatName) + \"\\n=[\" + str(chatNumberOfTargetMessages) + \"/\" + str(chatNumberOfMessages) + \"]\"\n",
    "\n",
    "        # Extract social graph data and get top influencer\n",
    "        socialGraphData = socialGraphTargetDict[fP]\n",
    "        socialGraphData = socialGraphData.most_common(configTopNInfluencer)\n",
    "        \n",
    "        # Iterate over forwarder\n",
    "        for oChatName, oChatRefs in socialGraphData:\n",
    "            \n",
    "            # Query other params\n",
    "            oChatName    = gloConvertToSafeChatName(str(oChatName))\n",
    "            oChatRefs    = oChatRefs\n",
    "\n",
    "            # If has forwarder\n",
    "            if(oChatName != \"nan\"):\n",
    "        \n",
    "                # Calc percent (forwarded_messages)\n",
    "                per = (oChatRefs/chatNumberOfTargetMessages) * 100\n",
    "\n",
    "                # Filter unimportant forwarders\n",
    "                if(per > configMinRefs):\n",
    "                \n",
    "                    # Add estimanted node size (chat not downloaded)\n",
    "                    addSocialGraphNodeWeight(oChatName, oChatRefs, dictSocialNodeWeights)\n",
    "\n",
    "                    # Invert percent (distance)\n",
    "                    wei = 100 - per\n",
    "\n",
    "                    # Label\n",
    "                    if(configFlagDebugLabel):\n",
    "                        lab = str(round(per, 3)) + \"% (\" + str(oChatRefs) + \"/\" + str(chatNumberOfTargetMessages) + \"≙\" + str(round(wei, 3)) + \")\"\n",
    "                    else:\n",
    "                        lab = str(round(per, 3)) + \"% (\" + str(oChatRefs) + \"/\" + str(chatNumberOfTargetMessages) + \")\"\n",
    "\n",
    "                    # Add edge\n",
    "                    G_weighted.add_edge(\n",
    "                        chatName,\n",
    "                        oChatName,\n",
    "                        weight=wei,\n",
    "                        tLabel = lab\n",
    "                    )\n",
    "\n",
    "        gloStopStopwatch(\"SG-Extract \" + chatName + \"(\" + str(chatNumberOfTargetMessages) + \"/\" + str(chatNumberOfMessages) + \" messages)\")\n",
    "        \n",
    "    print(\"- Add different nodes\")\n",
    "    for aNode in dictSocialNodeWeights:\n",
    "        \n",
    "        # Query node params\n",
    "        nodeName   = str(aNode)\n",
    "        nodeWeight = dictSocialNodeWeights[aNode]\n",
    "\n",
    "        # Set defaults\n",
    "        tValueColor = \"#ff8000\"\n",
    "        tLabel = str(nodeName) + \"\\n≈[\" + str(nodeWeight) + \"]\"\n",
    "\n",
    "        # Overwrite (if chat downloaded = exact weight)\n",
    "        if(nodeName in dictExactNodesLabels):\n",
    "            tValueColor = \"#0080ff\"\n",
    "            tLabel = dictExactNodesLabels[nodeName]\n",
    "        \n",
    "        G_weighted.add_node(\n",
    "            nodeName,\n",
    "            weight=nodeWeight,\n",
    "            tLabel = tLabel,\n",
    "            tColor=tValueColor\n",
    "        )\n",
    "        \n",
    "    gloStopStopwatch(\"Social Graph\")\n",
    "        \n",
    "    return G_weighted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### Test Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "generatedTestGraph = generateTestGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "drawSocialGraph(\n",
    "    G = generatedTestGraph,\n",
    "    layoutSelector=1,\n",
    "    configFactorEdge = 10,\n",
    "    configFactorNode = 1,\n",
    "    configArrowSize = 15,\n",
    "    configPlotWidth = 8,\n",
    "    configPlotHeight = 4.5,\n",
    "    outputFilename = \"\",\n",
    "    outputTitle = \"Test Graph Kamda Kawai Layout\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "drawSocialGraph(\n",
    "    G = generatedTestGraph,\n",
    "    layoutSelector=2,\n",
    "    configFactorEdge = 10,\n",
    "    configFactorNode = 1,\n",
    "    configArrowSize = 15,\n",
    "    configPlotWidth = 8,\n",
    "    configPlotHeight = 4.5,\n",
    "    outputFilename = \"\",\n",
    "    outputTitle = \"Test Graph Spring Layout\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "drawSocialGraph(\n",
    "    G = generatedTestGraph,\n",
    "    layoutSelector=3,\n",
    "    configFactorEdge = 10,\n",
    "    configFactorNode = 1,\n",
    "    configArrowSize = 15,\n",
    "    configPlotWidth = 8,\n",
    "    configPlotHeight = 4.5,\n",
    "    outputFilename = \"\",\n",
    "    outputTitle = \"Test Graph Graphviz Layout\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### Real Graph (ForwardedFrom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "drawSocialGraph(\n",
    "    generateSocialGraph(\n",
    "        configTopNInfluencer = 25,  \n",
    "        configMinRefs = 0,       \n",
    "        listFilePaths = list(dfInputFiles[dfInputFiles.inputDesc == \"dataSet0\"].inputPath),\n",
    "        socialGraphTargetDict = dictSGD_ForwardedFrom,\n",
    "        socialGraphTargetAttribute = \"procEvalIsForwarded\",\n",
    "        configFlagDebugLabel = False\n",
    "    ),\n",
    "    layoutSelector = 1,\n",
    "    configFactorEdge = 100,\n",
    "    configFactorNode = 10,\n",
    "    configArrowSize = 15,\n",
    "    configPlotWidth = 32,\n",
    "    configPlotHeight = 32,\n",
    "    outputFilename = \"social-graph-dataSet0-forwarded-from.png\",\n",
    "    outputTitle = \"Forwarded From (Top 25) (dataSet0)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if(\"dataSet2\" in C_LOAD_DATASET):\n",
    "    drawSocialGraph(\n",
    "        generateSocialGraph(\n",
    "            configTopNInfluencer = 20,  \n",
    "            configMinRefs = 0,       \n",
    "            listFilePaths = list(dfInputFiles[dfInputFiles.inputDesc == \"dataSet2\"].inputPath),\n",
    "            socialGraphTargetDict = dictSGD_ForwardedFrom,\n",
    "            socialGraphTargetAttribute = \"procEvalIsForwarded\",\n",
    "            configFlagDebugLabel = False\n",
    "        ),\n",
    "        layoutSelector = 1,\n",
    "        configFactorEdge = 100,\n",
    "        configFactorNode = 10,\n",
    "        configArrowSize = 15,\n",
    "        configPlotWidth = 32,\n",
    "        configPlotHeight = 32,\n",
    "        outputFilename = \"social-graph-dataSet2-forwarded-from.png\",\n",
    "        outputTitle = \"Forwarded From (Top 20) (dataSet2)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Enable\n",
    "\"\"\"\n",
    "drawSocialGraph(\n",
    "    generateSocialGraph(\n",
    "        configTopNInfluencer = 25,  \n",
    "        configMinRefs = 0,       \n",
    "        listFilePaths = list(dfInputFiles.inputPath),\n",
    "        socialGraphTargetDict = dictSGD_ForwardedFrom,\n",
    "        socialGraphTargetAttribute = \"procEvalIsForwarded\",\n",
    "        configFlagDebugLabel = False\n",
    "    ),\n",
    "    layoutSelector = 3,\n",
    "    configFactorEdge = 100,\n",
    "    configFactorNode = 10,\n",
    "    configArrowSize = 15,\n",
    "    configPlotWidth = 32,\n",
    "    configPlotHeight = 32,\n",
    "    outputFilename = \"social-graph-global-graphviz-forwarded-from.png\",\n",
    "    outputTitle = \"Forwarded From (Top 25) (global - graphviz)\"\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### Real Graph (Hashtag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "drawSocialGraph(\n",
    "    generateSocialGraph(\n",
    "        configTopNInfluencer = 30,  \n",
    "        configMinRefs = 0,        \n",
    "        listFilePaths = list(dfInputFiles[dfInputFiles.inputDesc == \"dataSet0\"].inputPath),\n",
    "        socialGraphTargetDict = dictSGD_Hashtag,\n",
    "        socialGraphTargetAttribute = \"procEvalContainsHashtag\",\n",
    "        configFlagDebugLabel = False\n",
    "    ),\n",
    "    layoutSelector = 1,\n",
    "    configFactorEdge = 100,\n",
    "    configFactorNode = 10,\n",
    "    configArrowSize = 15,\n",
    "    configPlotWidth = 32,\n",
    "    configPlotHeight = 32,\n",
    "    outputFilename = \"social-graph-dataSet0-hashtag.png\",\n",
    "    outputTitle = \"Hashtags (Top 30) (dataSet0)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if(\"dataSet2\" in C_LOAD_DATASET and False): #TODO: Fix Graph\n",
    "    drawSocialGraph(\n",
    "        generateSocialGraph(\n",
    "            configTopNInfluencer = 30,  \n",
    "            configMinRefs = 4,        \n",
    "            listFilePaths = list(dfInputFiles[dfInputFiles.inputDesc == \"dataSet2\"].inputPath),\n",
    "            socialGraphTargetDict = dictSGD_Hashtag,\n",
    "            socialGraphTargetAttribute = \"procEvalContainsHashtag\",\n",
    "            configFlagDebugLabel = False\n",
    "        ),\n",
    "        layoutSelector = 1,\n",
    "        configFactorEdge = 100,\n",
    "        configFactorNode = 10,\n",
    "        configArrowSize = 15,\n",
    "        configPlotWidth = 32,\n",
    "        configPlotHeight = 32,\n",
    "        outputFilename = \"social-graph-dataSet2-hashtag.png\",\n",
    "        outputTitle = \"Hashtags (Top 30) (dataSet2)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### Real Graph (Hosts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "drawSocialGraph(\n",
    "    generateSocialGraph(\n",
    "        configTopNInfluencer = 25,  \n",
    "        configMinRefs = 0,        \n",
    "        listFilePaths = list(dfInputFiles[dfInputFiles.inputDesc == \"dataSet0\"].inputPath),\n",
    "        socialGraphTargetDict = dictSGD_Host,\n",
    "        socialGraphTargetAttribute = \"procEvalContainsUrl\",\n",
    "        configFlagDebugLabel = False\n",
    "    ),\n",
    "    layoutSelector = 1,\n",
    "    configFactorEdge = 100,\n",
    "    configFactorNode = 10,\n",
    "    configArrowSize = 15,\n",
    "    configPlotWidth = 32,\n",
    "    configPlotHeight = 32,\n",
    "    outputFilename = \"social-graph-dataSet0-host.png\",\n",
    "    outputTitle = \"Hosts (Top 25) (dataSet0)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if(\"dataSet2\" in C_LOAD_DATASET):\n",
    "    drawSocialGraph(\n",
    "        generateSocialGraph(\n",
    "            configTopNInfluencer = 15,  \n",
    "            configMinRefs = 0,        \n",
    "            listFilePaths = list(dfInputFiles[dfInputFiles.inputDesc == \"dataSet2\"].inputPath),\n",
    "            socialGraphTargetDict = dictSGD_Host,\n",
    "            socialGraphTargetAttribute = \"procEvalContainsUrl\",\n",
    "            configFlagDebugLabel = False\n",
    "        ),\n",
    "        layoutSelector = 1,\n",
    "        configFactorEdge = 100,\n",
    "        configFactorNode = 10,\n",
    "        configArrowSize = 15,\n",
    "        configPlotWidth = 32,\n",
    "        configPlotHeight = 32,\n",
    "        outputFilename = \"social-graph-dataSet2-host.png\",\n",
    "        outputTitle = \"Hosts (Top 15) (dataSet2)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### Real Graph (Emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "drawSocialGraph(\n",
    "    generateSocialGraph(\n",
    "        configTopNInfluencer = 25,  \n",
    "        configMinRefs = 0,        \n",
    "        listFilePaths = list(dfInputFiles[dfInputFiles.inputDesc == \"dataSet0\"].inputPath),\n",
    "        socialGraphTargetDict = dictSGD_Emoji,\n",
    "        socialGraphTargetAttribute = \"procEvalContainsEmojiItem\",\n",
    "        configFlagDebugLabel = False\n",
    "    ),\n",
    "    layoutSelector = 1,\n",
    "    configFactorEdge = 100,\n",
    "    configFactorNode = 10,\n",
    "    configArrowSize = 15,\n",
    "    configPlotWidth = 32,\n",
    "    configPlotHeight = 32,\n",
    "    outputFilename = \"social-graph-dataSet0-emoji.png\",\n",
    "    outputTitle = \"Emojis (Top 25) (dataSet0)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if(\"dataSet2\" in C_LOAD_DATASET):\n",
    "    drawSocialGraph(\n",
    "        generateSocialGraph(\n",
    "            configTopNInfluencer = 25,  \n",
    "            configMinRefs = 0,        \n",
    "            listFilePaths = list(dfInputFiles[dfInputFiles.inputDesc == \"dataSet2\"].inputPath),\n",
    "            socialGraphTargetDict = dictSGD_Emoji,\n",
    "            socialGraphTargetAttribute = \"procEvalContainsEmojiItem\",\n",
    "            configFlagDebugLabel = False\n",
    "        ),\n",
    "        layoutSelector = 1,\n",
    "        configFactorEdge = 100,\n",
    "        configFactorNode = 10,\n",
    "        configArrowSize = 15,\n",
    "        configPlotWidth = 32,\n",
    "        configPlotHeight = 32,\n",
    "        outputFilename = \"social-graph-dataSet2-emoji.png\",\n",
    "        outputTitle = \"Emojis (Top 25) (dataSet2)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Time Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "param   targetDate      e.g. 1970-01-01\n",
    "param   fP              filePath\n",
    "param   highlightWord   set \"\" = no filter\n",
    "\"\"\"\n",
    "def queryNumberOfMessagesByDate(targetDate, fP, highlightWord):\n",
    "\n",
    "    df = dictMessages[fP].copy()\n",
    "\n",
    "    df = df[df.procEvalIsValidText == True]\n",
    "\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    \n",
    "    df = df[df.date <= targetDate]\n",
    "\n",
    "    if(highlightWord != \"\"):\n",
    "        df = df[df.procTDSafeLowercaseText.str.contains(highlightWord)]\n",
    "\n",
    "    l = len(df.index)\n",
    "\n",
    "    if(l > 0):\n",
    "        return l\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "param   filePathList\n",
    "param   outputFilename  set \"\" = no output file\n",
    "param   highlightWords  list of highlight words (leave empty if not used)\n",
    "param   configFrequency e.g. 1D or 1M\n",
    "\"\"\"\n",
    "# TODO: Add percent to label\n",
    "def drawTimePlot(filePathList, outputFilename, highlightWords, configFrequency):\n",
    "\n",
    "    gloStartStopwatch(\"Time Plot\")\n",
    "\n",
    "    plt.figure(figsize=(16, 9))\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        index=pd.date_range( #m/d/y\n",
    "            start='9/1/2018',\n",
    "            end='2/1/2021',\n",
    "            freq=configFrequency\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Add date to process\n",
    "    df[\"date\"] = df.index\n",
    "\n",
    "    vLineHeight = -1\n",
    "\n",
    "    for fP in filePathList:\n",
    "        gloStartStopwatch(\"Time Plot >>\" + fP + \"<<\")\n",
    "\n",
    "        # Plot Graph Var 1\n",
    "        if not highlightWords:\n",
    "            # Plot\n",
    "            plt.plot(\n",
    "                df.index, #x\n",
    "                df.apply(lambda x: queryNumberOfMessagesByDate(x.date, fP, highlightWord = \"\"), axis=1), #y\n",
    "                label = queryChatName(fP) #label\n",
    "            )\n",
    "            # Set vline height\n",
    "            currentHeight = queryNumberOfMessagesByAttEqTrue(fP, \"procEvalIsValidText\")\n",
    "            if(currentHeight > vLineHeight):\n",
    "                vLineHeight = currentHeight\n",
    "\n",
    "        # Plot High Light Word Graph / Var 2\n",
    "        for hWord in highlightWords:\n",
    "            y = df.apply(lambda x: queryNumberOfMessagesByDate(x.date, fP, highlightWord = hWord), axis=1)\n",
    "            # Plot\n",
    "            plt.plot(\n",
    "                df.index, #x\n",
    "                y, #y\n",
    "                label = queryChatName(fP) + \" usage of '\" + hWord + \"'\" #label\n",
    "            )\n",
    "            # Set vline height\n",
    "            currentHeight = y.max()\n",
    "            if(currentHeight > vLineHeight):\n",
    "                vLineHeight = currentHeight\n",
    "\n",
    "        gloStopStopwatch(\"Time Plot >>\" + fP + \"<<\")\n",
    "\n",
    "    # yy - mm - dd\n",
    "    # TODO: Double check https://www.bundesgesundheitsministerium.de/coronavirus/chronik-coronavirus.html?stand=20210104\n",
    "    plt.vlines(x = [\"2018-12-10\"], ymin=0, ymax=vLineHeight, color=\"orange\", ls='--', label=\"Global Compact for Migration (2018-12-10)\")\n",
    "    plt.vlines(x = [\"2020-01-27\"], ymin=0, ymax=vLineHeight, color=\"grey\", ls='--', label=\"Corona Patient Zero Germany\")\n",
    "    plt.vlines(x = [\"2020-03-23\"], ymin=0, ymax=vLineHeight, color=\"purple\", ls='--', label=\"1. Lockdown Germany (2020-03-23)\")\n",
    "    plt.vlines(x = [\"2020-11-02\"], ymin=0, ymax=vLineHeight, color=\"purple\", ls='--', label=\"2. Lockdown light Germany (2020-11-02)\")\n",
    "    plt.vlines(x = [\"2020-12-16\"], ymin=0, ymax=vLineHeight, color=\"purple\", ls='--', label=\"3. Lockdown Germany (2020-12-16)\")\n",
    "\n",
    "    _ = plt.legend()\n",
    "\n",
    "    if(outputFilename != \"\"):\n",
    "        plt.savefig(dir_var_output + outputFilename)\n",
    "\n",
    "    gloStopStopwatch(\"Time Plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    drawTimePlot(\n",
    "        filePathList = list(dfInputFiles[dfInputFiles.inputDesc == \"dataSet0\"].inputPath),\n",
    "        outputFilename = \"time-plot-dataSet0.png\",\n",
    "        highlightWords = [],\n",
    "        configFrequency=C_TIME_PLOT_FREQ\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    if(\"dataSet1\" in C_LOAD_DATASET):\n",
    "        drawTimePlot(\n",
    "            filePathList = list(dfInputFiles[dfInputFiles.inputDesc == \"dataSet1\"].inputPath),\n",
    "            outputFilename = \"time-plot-dataSet1.png\",\n",
    "            highlightWords = [],\n",
    "            configFrequency=C_TIME_PLOT_FREQ\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    if(\"dataSet1a\" in C_LOAD_DATASET):\n",
    "        drawTimePlot(\n",
    "            filePathList = list(dfInputFiles[dfInputFiles.inputDesc == \"dataSet1a\"].inputPath),\n",
    "            outputFilename = \"time-plot-dataSet1a.png\",\n",
    "            highlightWords = [],\n",
    "            configFrequency=C_TIME_PLOT_FREQ\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    if(\"dataSet2\" in C_LOAD_DATASET):\n",
    "        drawTimePlot(\n",
    "            filePathList = list(dfInputFiles[dfInputFiles.inputDesc == \"dataSet2\"].inputPath),\n",
    "            outputFilename = \"time-plot-dataSet2.png\",\n",
    "            highlightWords = [],\n",
    "            configFrequency=C_TIME_PLOT_FREQ\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getStopWordsList(filterList):\n",
    "\n",
    "    stopwWorldsList = []\n",
    "\n",
    "    deWordsList = nltk.corpus.stopwords.words('german')\n",
    "\n",
    "    enWordsList = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    aStopwords = []\n",
    "    with open(dir_var + \"additionalStopwords.txt\") as file:\n",
    "        for line in file: \n",
    "            line = line.strip()\n",
    "            if(line != \"\"):\n",
    "                aStopwords.append(line)\n",
    "\n",
    "    for s in filterList:\n",
    "        s = gloReplaceGermanChars(s)\n",
    "        stopwWorldsList.append(s)\n",
    "\n",
    "    for s in deWordsList:\n",
    "        s = gloReplaceGermanChars(s)\n",
    "        stopwWorldsList.append(s)\n",
    "\n",
    "    for s in enWordsList:\n",
    "        stopwWorldsList.append(s)\n",
    "\n",
    "    for s in aStopwords:\n",
    "        s = gloReplaceGermanChars(s)\n",
    "        stopwWorldsList.append(s)\n",
    "\n",
    "    return stopwWorldsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generateTextFromChat(df):\n",
    "    df = df.copy()\n",
    "    df = df[df.procEvalIsValidText == True]\n",
    "    \n",
    "    # Iterate over text (global text from group)\n",
    "    textList = []\n",
    "    for index, row in df.iterrows():\n",
    "        textList.append(\" \" + row[\"procTDSafeText\"])\n",
    "        \n",
    "    textString = ''.join(textList)\n",
    "\n",
    "    return textString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Context?\n",
    "# TODO: Improve stop words\n",
    "\n",
    "\"\"\"\n",
    "WordCloud\n",
    "\n",
    "param   targetDataFrame     DataFrame\n",
    "param   outputFilename      filename in outputdir (set \"\" == no output file)\n",
    "param   filterList          Exclude list\n",
    "param   flagShow            Set true == show wordcloud\n",
    "param   configPlotWidth     e.g. 1920\n",
    "param   configPlotHeight    e.g. 1080\n",
    "\"\"\"\n",
    "def generateWordCloud(targetDataFrame, outputFilename, filterList, flagShow, configPlotWidth, configPlotHeight):\n",
    "    \n",
    "    gloStartStopwatch(\"Word Cloud\")\n",
    "\n",
    "    dfMessages = targetDataFrame.copy()\n",
    "    \n",
    "    print(\"- Start transform text to global text string\")\n",
    "    textString = generateTextFromChat(dfMessages)\n",
    "    \n",
    "    stopWordsList = getStopWordsList(filterList)\n",
    "    \n",
    "    # Generate word cloud and save it to file\n",
    "    print(\"- Start generate cloud\")\n",
    "    wordcloud = WordCloud(\n",
    "                background_color=\"black\",\n",
    "                width=configPlotWidth,\n",
    "                height=configPlotHeight,\n",
    "                stopwords=stopWordsList\n",
    "            ).generate(textString)\n",
    "\n",
    "    if(outputFilename != \"\"):\n",
    "        wordcloud.to_file(dir_var_output + outputFilename)\n",
    "    \n",
    "    if(flagShow):\n",
    "        # Show top 20\n",
    "        print()\n",
    "        print(\"Top 20 occ:\\n\" + str(pd.Series(wordcloud.words_).head(20)))\n",
    "        print()\n",
    "        \n",
    "        # Show word cloud\n",
    "        print(\"- Start generate figure\")\n",
    "        plt.figure(figsize=(14, 14))\n",
    "        plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "        plt.show()\n",
    "    \n",
    "    gloStopStopwatch(\"Word Cloud\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Single Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Oliver Janich öffentlich (public_channel - dataSet0)\n",
    "if(C_SHORT_RUN == False):\n",
    "    generateWordCloud(\n",
    "        dictMessages[\"DS-05-01-2021/ChatExport_2021-01-05-janich\"],\n",
    "        \"wordcloud-oliver-janich.png\",\n",
    "        [],\n",
    "        flagShow = True,\n",
    "        configPlotWidth = 1920,\n",
    "        configPlotHeight = 1080\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ATTILA HILDMANN OFFICIAL (public_channel - dataSet0)\n",
    "if(C_SHORT_RUN == False):\n",
    "    generateWordCloud(\n",
    "        dictMessages[\"DS-05-01-2021/ChatExport_2021-01-05-hildmann\"],\n",
    "        \"wordcloud-attila-hildmann.png\",\n",
    "        [],\n",
    "        flagShow = True,\n",
    "        configPlotWidth = 1920,\n",
    "        configPlotHeight = 1080\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Eva Herman Offiziell (public_channel - dataSet0)\n",
    "if(C_SHORT_RUN == False):\n",
    "    generateWordCloud(\n",
    "        dictMessages[\"DS-05-01-2021/ChatExport_2021-01-05-evaherman\"],\n",
    "        \"wordcloud-eva-herman.png\",\n",
    "        [],\n",
    "        flagShow = True,\n",
    "        configPlotWidth = 1920,\n",
    "        configPlotHeight = 1080\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Xavier Naidoo (public_channel - dataSet0)\n",
    "if(C_SHORT_RUN == False):\n",
    "    generateWordCloud(\n",
    "        dictMessages[\"DS-05-01-2021/ChatExport_2021-01-05-xavier\"],\n",
    "        \"wordcloud-xavier-naidoo.png\",\n",
    "        [],\n",
    "        flagShow = True,\n",
    "        configPlotWidth = 1920,\n",
    "        configPlotHeight = 1080\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    if(\"dataSet2\" in C_LOAD_DATASET):\n",
    "        generateWordCloud(\n",
    "            dictMessages[\"DS-13-01-2021/ChatExport_2021-01-13-querdenken089\"],\n",
    "            \"wordcloud-querdenken-089-group.png\",\n",
    "            [],\n",
    "            flagShow = True,\n",
    "            configPlotWidth = 1920,\n",
    "            configPlotHeight = 1080\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    if(\"dataSet2\" in C_LOAD_DATASET):\n",
    "        generateWordCloud(\n",
    "            dictMessages[\"DS-13-01-2021/ChatExport_2021-01-13-querdenken591Info\"],\n",
    "            \"wordcloud-querdenken-591-info.png\",\n",
    "            [],\n",
    "            flagShow = True,\n",
    "            configPlotWidth = 1920,\n",
    "            configPlotHeight = 1080\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    if(\"dataSet2\" in C_LOAD_DATASET):\n",
    "        generateWordCloud(\n",
    "            dictMessages[\"DS-13-01-2021/ChatExport_2021-01-13-querdenken773\"],\n",
    "            \"wordcloud-querdenken-773-group.png\",\n",
    "            [],\n",
    "            flagShow = True,\n",
    "            configPlotWidth = 1920,\n",
    "            configPlotHeight = 1080\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    if(\"dataSet2\" in C_LOAD_DATASET):\n",
    "        generateWordCloud(\n",
    "            dictMessages[\"DS-13-01-2021/ChatExport_2021-01-13-querdenken773Info\"],\n",
    "            \"wordcloud-querdenken-773-info.png\",\n",
    "            [],\n",
    "            flagShow = True,\n",
    "            configPlotWidth = 1920,\n",
    "            configPlotHeight = 1080\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    if(\"dataSet2\" in C_LOAD_DATASET):\n",
    "        generateWordCloud(\n",
    "            dictMessages[\"DS-13-01-2021/ChatExport_2021-01-13-querdenken711\"],\n",
    "            \"wordcloud-querdenken-711-group.png\",\n",
    "            [],\n",
    "            flagShow = True,\n",
    "            configPlotWidth = 1920,\n",
    "            configPlotHeight = 1080\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    if(\"dataSet2\" in C_LOAD_DATASET):\n",
    "        generateWordCloud(\n",
    "            dictMessages[\"DS-13-01-2021/ChatExport_2021-01-13-querdenken711Info\"],\n",
    "            \"wordcloud-querdenken-711-info.png\",\n",
    "            [],\n",
    "            flagShow = True,\n",
    "            configPlotWidth = 1920,\n",
    "            configPlotHeight = 1080\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    if(\"dataSet2\" in C_LOAD_DATASET):\n",
    "        generateWordCloud(\n",
    "            dictMessages[\"DS-13-01-2021/ChatExport_2021-01-13-querdenken69\"],\n",
    "            \"wordcloud-querdenken-69-group.png\",\n",
    "            [],\n",
    "            flagShow = True,\n",
    "            configPlotWidth = 1920,\n",
    "            configPlotHeight = 1080\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    if(\"dataSet2\" in C_LOAD_DATASET):\n",
    "        generateWordCloud(\n",
    "            dictMessages[\"DS-13-01-2021/ChatExport_2021-01-13-querdenken69Info\"],\n",
    "            \"wordcloud-querdenken-69-info.png\",\n",
    "            [],\n",
    "            flagShow = True,\n",
    "            configPlotWidth = 1920,\n",
    "            configPlotHeight = 1080\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Just for test purposes\n",
    "if(C_SHORT_RUN == False and False): #TODO: Enable\n",
    "    generateWordCloud(\n",
    "        dfAllDataMessages,\n",
    "        \"\",\n",
    "        [],\n",
    "        flagShow = True,\n",
    "        configPlotWidth = 1920,\n",
    "        configPlotHeight = 1080\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Auto Mode (Batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extractTimePeriodDataFrame(df, timeStart, timeStop):\n",
    "\n",
    "    print(\"- Got Start \" + str(timeStart) + \" and Stop \" + str(timeStop))\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    \n",
    "    dfNew = df[df.date <= timeStop]\n",
    "    dfNew = dfNew[dfNew.date >= timeStart]\n",
    "\n",
    "    dfNew = dfNew.set_index(\"date\")\n",
    "    dfNew = dfNew.sort_index()\n",
    "\n",
    "    return dfNew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generateWCPeriod():\n",
    "    return list(pd.date_range( #m/d/y\n",
    "            start='1/1/2018',\n",
    "            end='2/1/2021',\n",
    "            #freq=\"W-MON\",\n",
    "            freq=\"1M\"\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Wrapper WordCloud\n",
    "\n",
    "param   fP  filePath\n",
    "param   label e.g. chatName\n",
    "param   filterList additional stopWords\n",
    "\"\"\"\n",
    "def generateWordCloudAuto(fP, label, filterList):\n",
    "\n",
    "    gloStartStopwatch(\"Generate World Cloud Auto >>\" + fP + \"<<\")\n",
    "\n",
    "    periods = generateWCPeriod()\n",
    "\n",
    "    dictSaved = {}\n",
    "\n",
    "    prevStart = periods[0]\n",
    "\n",
    "    for period in periods:\n",
    "\n",
    "        stop = period\n",
    "\n",
    "        e = extractTimePeriodDataFrame(dictMessages[fP], timeStart = prevStart, timeStop = stop)\n",
    "\n",
    "        if(prevStart != stop and len(e.index) > 0):\n",
    "            fileName = \"autoWordCloud/\" + queryChatName(fP) + \"-\" + str(prevStart) + \"-\" + str(stop) + \".png\"\n",
    "            generateWordCloud(\n",
    "                e,\n",
    "                fileName,\n",
    "                filterList,\n",
    "                flagShow = False,\n",
    "                configPlotWidth = 1280,\n",
    "                configPlotHeight = 720\n",
    "            )\n",
    "            print(\"- Save file \" + fileName)\n",
    "            dictSaved[fileName] = str(prevStart) + \" - \" + str(stop)\n",
    "\n",
    "        else:\n",
    "            print(\"- Start and Stop equal or no message found\")\n",
    "\n",
    "        prevStart = stop\n",
    "\n",
    "    gloWriteDictToFile(\"auto-wordcloud-\" + label + \".csv\", dictSaved)\n",
    "\n",
    "    gloStopStopwatch(\"Generate World Cloud Auto >>\" + fP + \"<<\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    generateWordCloudAuto(\n",
    "        fP = \"DS-05-01-2021/ChatExport_2021-01-05-janich\",\n",
    "        label = \"oliver-janich\",\n",
    "        filterList = []\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    generateWordCloudAuto(\n",
    "        fP = \"DS-05-01-2021/ChatExport_2021-01-05-hildmann\",\n",
    "        label = \"attila-hildmann\",\n",
    "        filterList = []\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    generateWordCloudAuto(\n",
    "        fP = \"DS-05-01-2021/ChatExport_2021-01-05-evaherman\",\n",
    "        label = \"eva-herman\",\n",
    "        filterList = []\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    generateWordCloudAuto(\n",
    "        fP = \"DS-05-01-2021/ChatExport_2021-01-05-xavier\",\n",
    "        label = \"xavier-naidoo\",\n",
    "        filterList = []\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## N Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generateNGram(text, n):\n",
    "    # https://albertauyeung.github.io/2018/06/03/generating-ngrams.html\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "    tokens = [token for token in text.split(\" \") if token != \"\"]\n",
    "    \n",
    "    return list(ngrams(tokens, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generateNGramChat(fP, n, mostCommon):\n",
    "    return Counter(\n",
    "        generateNGram(\n",
    "            generateTextFromChat(dictMessages[fP]),\n",
    "            n = n\n",
    "        )\n",
    "    ).most_common(mostCommon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generateNGramAuto(filePathList, n, mostCommon):\n",
    "    for fP in filePathList:\n",
    "\n",
    "        print()\n",
    "        print(\"Analyse now >>\" + fP + \"<<\")\n",
    "\n",
    "        c = generateNGramChat(\n",
    "            fP,\n",
    "            n = n,\n",
    "            mostCommon = mostCommon\n",
    "        )\n",
    "\n",
    "        print (\"\\n\".join(map(str, c)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### DataSet0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "generateNGramAuto(\n",
    "    dfInputFiles[dfInputFiles.inputDesc == \"dataSet0\"].inputPath,\n",
    "    n = 2,\n",
    "    mostCommon = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "generateNGramAuto(\n",
    "    dfInputFiles[dfInputFiles.inputDesc == \"dataSet0\"].inputPath,\n",
    "    n = 3,\n",
    "    mostCommon = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "generateNGramAuto(\n",
    "    dfInputFiles[dfInputFiles.inputDesc == \"dataSet0\"].inputPath,\n",
    "    n = 4,\n",
    "    mostCommon = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "generateNGramAuto(\n",
    "    dfInputFiles[dfInputFiles.inputDesc == \"dataSet0\"].inputPath,\n",
    "    n = 5,\n",
    "    mostCommon = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "generateNGramAuto(\n",
    "    dfInputFiles[dfInputFiles.inputDesc == \"dataSet0\"].inputPath,\n",
    "    n = 6,\n",
    "    mostCommon = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Excerpt DataSet2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if(\"dataSet2\" in C_LOAD_DATASET):\n",
    "    generateNGramAuto(\n",
    "        dfInputFiles[dfInputFiles.inputDesc == \"dataSet2\"].inputPath,\n",
    "        n = 2,\n",
    "        mostCommon = 10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if(\"dataSet2\" in C_LOAD_DATASET):\n",
    "    generateNGramAuto(\n",
    "        dfInputFiles[dfInputFiles.inputDesc == \"dataSet2\"].inputPath,\n",
    "        n = 3,\n",
    "        mostCommon = 10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if(\"dataSet2\" in C_LOAD_DATASET):\n",
    "    generateNGramAuto(\n",
    "        dfInputFiles[dfInputFiles.inputDesc == \"dataSet2\"].inputPath,\n",
    "        n = 5,\n",
    "        mostCommon = 10\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Topic Modeling\n",
    "\n",
    "Overview Topic Modeling\n",
    "https://nlpforhackers.io/topic-modeling/\n",
    "\n",
    "- LDA (Probabilistic Graphical Models)\n",
    "- LSA or LSI (Linear Algebra Singular Value Decomposition)\n",
    "- NMF (Linear Algebra Non-Negative Matrix Factorization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Latent Dirichlet Allocation (LDA) with Gensim\n",
    "\n",
    "https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Get clean words from it (arrays)\n",
    "def sendToWords(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc= removes punctuations\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "        \n",
    "def removeStopwords(texts, stop_words):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "return  (lda_model, corpus, id2word)\n",
    "\"\"\"\n",
    "def processLda(df, num_topics, debugPrint, filterList):\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    df = df[df.procEvalIsValidText == True]\n",
    "\n",
    "    df = df[[\"date\", \"procTDSafeLowercaseText\"]]\n",
    "\n",
    "    df = df.set_index(\"date\")\n",
    "    df = df.sort_index()\n",
    "\n",
    "    # ###############################\n",
    "    # ### Transform Data ############\n",
    "    # ###############################\n",
    "    stops_words = getStopWordsList(filterList)\n",
    "\n",
    "    msgList     = df.procTDSafeLowercaseText.values.tolist()\n",
    "\n",
    "    # Create words\n",
    "    msg_words   = list(sendToWords(msgList))\n",
    "    msg_word    = removeStopwords(texts = msg_words, stop_words = stops_words)\n",
    "\n",
    "    # Create Dictionary (id to word)\n",
    "    id2word = corpora.Dictionary(msg_word)\n",
    "\n",
    "    # Create Corpus\n",
    "    texts = msg_word\n",
    "\n",
    "    # Term Document Frequency (dict to bag of words)\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "    # Build LDA model\n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics)\n",
    "\n",
    "    if(debugPrint):\n",
    "        pprint(lda_model.print_topics())\n",
    "        #doc_lda = lda_model[corpus] # TODO: ?\n",
    "\n",
    "    return (lda_model, corpus, id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "param   outputLabel required\n",
    "\"\"\"\n",
    "def ldaToHtml(lda_model, corpus, id2word, outputLabel):\n",
    "\n",
    "    # pyLDAvis.enable_notebook()\n",
    "\n",
    "    LDAvis_data_filepath = os.path.join(dir_var_output + 'pyLDAvis/' + outputLabel + '-data')\n",
    "\n",
    "    # # this is a bit time consuming - make the if statement True\n",
    "    # # if you want to execute visualization prep yourself\n",
    "    if 1 == 1:\n",
    "        LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "        with open(LDAvis_data_filepath, 'wb') as f:\n",
    "            pickle.dump(LDAvis_prepared, f)\n",
    "\n",
    "    # load the pre-prepared pyLDAvis data from disk\n",
    "    with open(LDAvis_data_filepath, 'rb') as f:\n",
    "        LDAvis_prepared = pickle.load(f)\n",
    "\n",
    "    pyLDAvis.save_html(LDAvis_prepared, dir_var_output + 'pyLDAvis/' + outputLabel + '-report.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# param outputLabel required\n",
    "def autoLda(df, num_topics, debugPrint, outputLabel, filterList):\n",
    "\n",
    "    gloStartStopwatch(\"Process LDA >> \"+ outputLabel + \"<<\")\n",
    "    lda_model, corpus, id2word = processLda(\n",
    "        df = df,\n",
    "        num_topics = num_topics,\n",
    "        debugPrint = debugPrint,\n",
    "        filterList = filterList\n",
    "    )\n",
    "    gloStopStopwatch(\"Process LDA >> \"+ outputLabel + \"<<\")\n",
    "\n",
    "    gloStartStopwatch(\"Process LDA to html >> \"+ outputLabel + \"<<\")\n",
    "    ldaToHtml(\n",
    "        lda_model = lda_model,\n",
    "        corpus = corpus,\n",
    "        id2word = id2word,\n",
    "        outputLabel = outputLabel\n",
    "    )\n",
    "    gloStopStopwatch(\"Process LDA to html >> \"+ outputLabel + \"<<\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "autoLda(\n",
    "    df = dictMessages[\"DS-05-01-2021/ChatExport_2021-01-05-janich\"],\n",
    "    num_topics = 5,\n",
    "    debugPrint = True,\n",
    "    outputLabel = \"oliver-janich\",\n",
    "    filterList = []\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "autoLda(\n",
    "    df = dictMessages[\"DS-05-01-2021/ChatExport_2021-01-05-hildmann\"],\n",
    "    num_topics = 5,\n",
    "    debugPrint = True,\n",
    "    outputLabel = \"attila-hildmann\",\n",
    "    filterList = []\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "autoLda(\n",
    "    df = dictMessages[\"DS-05-01-2021/ChatExport_2021-01-05-evaherman\"],\n",
    "    num_topics = 5,\n",
    "    debugPrint = True,\n",
    "    outputLabel = \"eva-herman\",\n",
    "    filterList = []\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "autoLda(\n",
    "    df = dictMessages[\"DS-05-01-2021/ChatExport_2021-01-05-xavier\"],\n",
    "    num_topics = 5,\n",
    "    debugPrint = True,\n",
    "    outputLabel = \"xavier-naidoo\",\n",
    "    filterList = []\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if(\"dataSet1a\" in C_LOAD_DATASET):\n",
    "    autoLda(\n",
    "        df = dictMessages[\"DS-05-01-2021a/ChatExport_2021-01-05-freiheitsChat\"],\n",
    "        num_topics = 25,\n",
    "        debugPrint = True,\n",
    "        outputLabel = \"group-freiheitsChat\",\n",
    "        filterList = []\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if(\"dataSet1a\" in C_LOAD_DATASET):\n",
    "    autoLda(\n",
    "        df = dictMessages[\"DS-05-01-2021a/ChatExport_2021-01-05-freiheitsChatBlitz\"],\n",
    "        num_topics = 25,\n",
    "        debugPrint = True,\n",
    "        outputLabel = \"group-freiheitsChatBlitz\",\n",
    "        filterList = []\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if(\"dataSet1a\" in C_LOAD_DATASET):\n",
    "    autoLda(\n",
    "        df = dictMessages[\"DS-05-01-2021a/ChatExport_2021-01-05-liveFuerDeOsSc\"],\n",
    "        num_topics = 25,\n",
    "        debugPrint = True,\n",
    "        outputLabel = \"group-liveFuerDeOsSc\",\n",
    "        filterList = []\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Word Tracer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://www.bundestag.de/parlament/plenum/sitzverteilung_19wp\n",
    "highlightwords = [\"cdu\", \"spd\", \"afd\", \"fdp\", \"linke\", \"gruenen\", \"merkel\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    drawTimePlot(\n",
    "        filePathList = list([\"DS-05-01-2021/ChatExport_2021-01-05-janich\"]),\n",
    "        outputFilename = \"word-tracer-oliver-janich.png\",\n",
    "        highlightWords = highlightwords,\n",
    "        configFrequency=C_TIME_PLOT_FREQ\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    drawTimePlot(\n",
    "        filePathList = list([\"DS-05-01-2021/ChatExport_2021-01-05-hildmann\"]),\n",
    "        outputFilename = \"word-tracer-attila-hildmann.png\",\n",
    "        highlightWords = highlightwords,\n",
    "        configFrequency=C_TIME_PLOT_FREQ\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    drawTimePlot(\n",
    "        filePathList = list([\"DS-05-01-2021/ChatExport_2021-01-05-evaherman\"]),\n",
    "        outputFilename = \"word-tracer-eva-herman.png\",\n",
    "        highlightWords = highlightwords,\n",
    "        configFrequency=C_TIME_PLOT_FREQ\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    drawTimePlot(\n",
    "        filePathList = list([\"DS-05-01-2021/ChatExport_2021-01-05-xavier\"]),\n",
    "        outputFilename = \"word-tracer-xavier-naidoo.png\",\n",
    "        highlightWords = highlightwords,\n",
    "        configFrequency=C_TIME_PLOT_FREQ\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Future Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gloStopStopwatch(\"Global notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Next Steps\n",
    "\"\"\"\n",
    "- Add freq words to word tracer\n",
    "- Python NatrualLanguage Toolkit tools?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://towardsai.net/p/data-mining/text-mining-in-python-steps-and-examples-78b3f8fd913b\n",
    "\"\"\"\n",
    "- Concordance (and Kookkurrenz and Correl?)\n",
    "- Tokenization\n",
    "- Finding frequency distinct in the text\n",
    "- Stemming\n",
    "- Lemmatization\n",
    "- Stop words?\n",
    "- Part of speech tagging (POS)\n",
    "- Named entity recognition\n",
    "- Chunking\n",
    "\"\"\"\n",
    "\n",
    "# https://towardsdatascience.com/text-mining-for-dummies-text-classification-with-python-98e47c3a9deb\n",
    "\"\"\"\n",
    "- Sentiment Analysis\n",
    "\"\"\"\n",
    "\n",
    "# https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-deal-text-data-predictive-python/\n",
    "\"\"\"\n",
    "- Features: (Number of words, Number of characters, Average word length, Number of stopwords, Number of special characters, Number of nummberics, Number of uppercase words)\n",
    "- Pre-processing (Lower casing, Punctuation removal, Stopwrods removal, Frequent word removal, Rare words removal, Spelling correction, Tokenization, Stemming, Lemmatization)\n",
    "- Adv-processing (N-grams, Term Frequency, Inverse Document Frequency, Term Frequency-Inverse Document Frequency 'TF-IDF', Bag of words, Sentiment Analysis, Word Embedding)\n",
    "\"\"\"\n",
    "\n",
    "# https://realpython.com/python-keras-text-classification/\n",
    "\"\"\"\n",
    "- Text Analysis with Keras\n",
    "\"\"\"\n",
    "\n",
    "# https://www.tidytextmining.com/ngrams.html\n",
    "\"\"\"\n",
    "- Relationships between words: n-grams and correlations\n",
    "\"\"\"\n",
    "\n",
    "# http://seaborn.pydata.org/tutorial/categorical.html?highlight=bar%20plot\n",
    "\"\"\"\n",
    "- Plotting with categorical data (Box Plot)\n",
    "\"\"\"\n",
    "\n",
    "# https://towardsdatascience.com/visualizing-data-with-pair-plots-in-python-f228cf529166\n",
    "\"\"\"\n",
    "- Visualizing Data with Pairs Plots in Python\n",
    "\"\"\"\n",
    "\n",
    "# https://www.kirenz.com/post/2019-08-13-network_analysis/\n",
    "\"\"\"\n",
    "- Social Network Analysis with Python\n",
    "\"\"\"\n",
    "\n",
    "# https://tgstat.com\n",
    "\"\"\"\n",
    "- Compare website with my analyse\n",
    "\"\"\"\n",
    "\n",
    "# https://huggingface.co/bert-base-german-cased\n",
    "\"\"\"\n",
    "- Language Model Bert German\n",
    "\"\"\"\n",
    "\n",
    "# https://github.com/sekhansen/text-mining-tutorial/tree/master\n",
    "\"\"\"\n",
    "- An Introduction to Topic Modelling via Gibbs sampling\n",
    "\"\"\"\n",
    "\n",
    "# https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
    "\"\"\"\n",
    "- Topic Modeling with Gensim (Python)\n",
    "\"\"\"\n",
    "\n",
    "# https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0\n",
    "\"\"\"\n",
    "- Topic Modeling in Python: Latent Dirichlet Allocation (LDA)\n",
    "\"\"\"\n",
    "\n",
    "# https://github.com/sekhansen/text-mining-tutorial/blob/master/tutorial_notebook.ipynb\n",
    "\"\"\"\n",
    "- Text Mining Python Tutorial\n",
    "\"\"\"\n",
    "\n",
    "# https://textmining.wp.hs-hannover.de/Preprocessing.html\n",
    "\"\"\"\n",
    "- Preprocessing samples\n",
    "\"\"\"\n",
    "\n",
    "# https://likegeeks.com/nlp-tutorial-using-python-nltk/\n",
    "\"\"\"\n",
    "- NLTK and NLP Tutorial\n",
    "\"\"\"\n",
    "\n",
    "# https://www.datacamp.com/community/tutorials/text-analytics-beginners-nltk\n",
    "\"\"\"\n",
    "- Another NLTK Tutorial\n",
    "\"\"\"\n",
    "\n",
    "# https://data-flair.training/blogs/nltk-python-tutorial/\n",
    "\"\"\"\n",
    "- NLTK Overview\n",
    "\"\"\"\n",
    "\n",
    "# https://github.com/expectocode/telegram-analysis\n",
    "\"\"\"\n",
    "- Telegram Python Sample\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "work/notebooks/Telegram.ipynb",
   "output_path": "work/notebooks/Telegram-out.ipynb",
   "parameters": {},
   "start_time": "2021-01-18T07:34:04.293581",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}