{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telegram Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Urls\n",
    "from urllib.parse import urlparse\n",
    "from collections import Counter\n",
    "\n",
    "#Use notebook for interactive plots\n",
    "#%matplotlib notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: demjson in /opt/conda/lib/python3.8/site-packages (2.2.4)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# JSON Lib\n",
    "! pip install demjson\n",
    "import demjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.8/site-packages (3.5)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from nltk) (4.48.2)\r\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.8/site-packages (from nltk) (2020.11.13)\r\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from nltk) (7.1.2)\r\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from nltk) (0.16.0)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Natural Language Toolkit\n",
    "! pip install nltk\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in /opt/conda/lib/python3.8/site-packages (1.8.1)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.8/site-packages (from wordcloud) (3.2.2)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.8/site-packages (from wordcloud) (7.2.0)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /opt/conda/lib/python3.8/site-packages (from wordcloud) (1.19.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->wordcloud) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->wordcloud) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->wordcloud) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.1->matplotlib->wordcloud) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "# WordCloud\n",
    "! pip install wordcloud\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 56\r\n",
      "drwxr-xr-x 6 jovyan users   192 Dec 20 18:21 .\r\n",
      "drwxr-xr-x 8 jovyan users   256 Dec 20 10:43 ..\r\n",
      "drwxr-xr-x 5 jovyan users   160 Dec 20 10:22 data\r\n",
      "-rw-r--r-- 1 jovyan users  5342 Dec 20 10:42 inputFiles.csv\r\n",
      "drwxr-xr-x 3 jovyan users    96 Dec 20 10:30 .ipynb_checkpoints\r\n",
      "-rwxr-xr-x 1 jovyan users 48199 Dec 20 18:21 Telegram.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "# Set vars\n",
    "dir_var = \"./\"\n",
    "! ls -al ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfInputFiles = pd.read_csv(dir_var + \"inputFiles.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### !!!!!!!!!!!!!!!! Quick Filter !!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take only samples\n",
    "#dfFilter = pd.DataFrame()\n",
    "\n",
    "#dfFilter = dfFilter.append(dfInputFiles[dfInputFiles.inputName.str.contains(\"Xavier\")])\n",
    "#dfFilter = dfFilter.append(dfInputFiles[dfInputFiles.inputName.str.contains(\"Janich\")])\n",
    "#dfFilter = dfFilter.append(dfInputFiles[dfInputFiles.inputName.str.contains(\"Eva\")])\n",
    "#dfFilter = dfFilter.append(dfInputFiles[dfInputFiles.inputName.str.contains(\"HILDMANN\")])\n",
    "\n",
    "#dfInputFiles = dfFilter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputName</th>\n",
       "      <th>inputPath</th>\n",
       "      <th>inputType</th>\n",
       "      <th>inputId</th>\n",
       "      <th>inputDesc</th>\n",
       "      <th>inputDownloadType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Xavier Naidoo (inoffiziell)</td>\n",
       "      <td>DS-08-10-2020/ChatExport_2020-09-25-xavier</td>\n",
       "      <td>public_channel</td>\n",
       "      <td>9874390332</td>\n",
       "      <td>NaN</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Regellese und Diskussionsgruppe zum Xavier's M...</td>\n",
       "      <td>DS-22-10-2020/ChatExport_2020-10-13-xavierChat</td>\n",
       "      <td>private_supergroup</td>\n",
       "      <td>9907103286</td>\n",
       "      <td>NaN</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Oliver Janich oeffentlich</td>\n",
       "      <td>DS-08-10-2020/ChatExport_2020-09-25-janich</td>\n",
       "      <td>public_channel</td>\n",
       "      <td>9808932799</td>\n",
       "      <td>NaN</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Eva Herman Offiziell</td>\n",
       "      <td>DS-08-10-2020/ChatExport_2020-09-27-evaherman</td>\n",
       "      <td>public_channel</td>\n",
       "      <td>9915108907</td>\n",
       "      <td>NaN</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ATTILA HILDMANN OFFICIAL</td>\n",
       "      <td>DS-08-10-2020/ChatExport_2020-09-25-hildmann</td>\n",
       "      <td>public_channel</td>\n",
       "      <td>10034163583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            inputName  \\\n",
       "12                        Xavier Naidoo (inoffiziell)   \n",
       "37  Regellese und Diskussionsgruppe zum Xavier's M...   \n",
       "1                           Oliver Janich oeffentlich   \n",
       "11                               Eva Herman Offiziell   \n",
       "2                            ATTILA HILDMANN OFFICIAL   \n",
       "\n",
       "                                         inputPath           inputType  \\\n",
       "12      DS-08-10-2020/ChatExport_2020-09-25-xavier      public_channel   \n",
       "37  DS-22-10-2020/ChatExport_2020-10-13-xavierChat  private_supergroup   \n",
       "1       DS-08-10-2020/ChatExport_2020-09-25-janich      public_channel   \n",
       "11   DS-08-10-2020/ChatExport_2020-09-27-evaherman      public_channel   \n",
       "2     DS-08-10-2020/ChatExport_2020-09-25-hildmann      public_channel   \n",
       "\n",
       "        inputId inputDesc inputDownloadType  \n",
       "12   9874390332       NaN               all  \n",
       "37   9907103286       NaN               all  \n",
       "1    9808932799       NaN               all  \n",
       "11   9915108907       NaN               all  \n",
       "2   10034163583       NaN               all  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfInputFiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToDataFrameMeta(filePath):\n",
    "    dF = pd.read_json(dir_var + \"data/\" + filePath + \"/result.json\", encoding='utf-8')\n",
    "    return dF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToDataFrameMessages(filePath, dictMeta):\n",
    "    dF = pd.json_normalize(dictMeta[filePath].messages)\n",
    "    return dF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkIsFormattedText(text):\n",
    "    textString = str(text)\n",
    "    if(textString.startswith(\"[\") == True and textString.endswith(\"]\") == True):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See below\n",
    "def getExtractedParam(param, processedTextData):\n",
    "    a,b,c,d,e,f,g = processedTextData\n",
    "    switcher = {\n",
    "        0: a,\n",
    "        1: b,\n",
    "        2: c,\n",
    "        3: d,\n",
    "        4: e,\n",
    "        5: f,\n",
    "        6: g\n",
    "    }\n",
    "    return switcher.get(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: What href in normal text?\n",
    "\n",
    "# Return types (see above)\n",
    "\n",
    "# a = processedText\n",
    "# b = Items 'processedURLs'\n",
    "# c = Items 'processedHashtags'\n",
    "# d = Items 'processedBolds'\n",
    "# e = Items 'processedItalics'\n",
    "# f = Items 'processedUnderlines'\n",
    "# g = Items 'processedEmails'\n",
    "\n",
    "def extractTextData(processedIsFormattedText, text):\n",
    "    \n",
    "    # 3 returns!\n",
    "    \n",
    "    processedURLs       = list()\n",
    "    processedHashtags   = list()\n",
    "    processedBolds      = list()\n",
    "    processedItalics    = list()\n",
    "    processedUnderlines = list()\n",
    "    processedEmails     = list()\n",
    "    \n",
    "    if(processedIsFormattedText != True):\n",
    "        #Is no JSON\n",
    "        return (text, processedURLs, processedHashtags, processedBolds, processedItalics, processedUnderlines, processedEmails)\n",
    "    else:\n",
    "        #Is json try to parse\n",
    "        try:\n",
    "            jsonList = demjson.decode(str(text), encoding='utf8')\n",
    "\n",
    "            returnList = []\n",
    "\n",
    "            # Do for each item in list\n",
    "            for lItem in jsonList:\n",
    "\n",
    "                messageString = str(lItem)\n",
    "\n",
    "                isFormatted = messageString.startswith(\"{\") and messageString.endswith(\"}\")\n",
    "\n",
    "                if(isFormatted):\n",
    "                    # Is Json Sub String\n",
    "                    subJsonString = demjson.decode(str(messageString), encoding='utf8')\n",
    "                    subJsonType = subJsonString[\"type\"]\n",
    "\n",
    "                    if(subJsonType == \"bold\"):\n",
    "                        #text included\n",
    "                        processedBolds.append(subJsonString[\"text\"])\n",
    "                        returnList.append(subJsonString[\"text\"])\n",
    "                        \n",
    "                    elif(subJsonType == \"italic\"):\n",
    "                        #text included\n",
    "                        processedItalics.append(subJsonString[\"text\"])\n",
    "                        returnList.append(subJsonString[\"text\"])\n",
    "                        \n",
    "                    elif(subJsonType == \"underline\"):\n",
    "                        #text included\n",
    "                        processedUnderlines.append(subJsonString[\"text\"])\n",
    "                        returnList.append(subJsonString[\"text\"])\n",
    "                    \n",
    "                    elif(subJsonType == \"email\"):\n",
    "                        #text included\n",
    "                        processedEmails.append(subJsonString[\"text\"])\n",
    "                        \n",
    "                    elif(subJsonType == \"text_link\"):\n",
    "                        #text and href included\n",
    "                        processedURLs.append(subJsonString[\"href\"])\n",
    "                        returnList.append(subJsonString[\"text\"])\n",
    "                        \n",
    "                    elif(subJsonType == \"link\"):\n",
    "                        #text included\n",
    "                        processedURLs.append(subJsonString[\"text\"])\n",
    "                        \n",
    "                    elif(subJsonType == \"hashtag\"):\n",
    "                        #text included\n",
    "                        processedHashtags.append(subJsonString[\"text\"])\n",
    "                        returnList.append(subJsonString[\"text\"])\n",
    "                        \n",
    "                    elif(subJsonType == \"mention\"):\n",
    "                        #text included\n",
    "                        returnList.append(subJsonString[\"text\"])\n",
    "                        \n",
    "                    elif(subJsonType == \"mention_name\"):\n",
    "                        #text and user_id included\n",
    "                        returnList.append(subJsonString[\"text\"])\n",
    "                        \n",
    "                    elif(subJsonType == \"bot_command\"):\n",
    "                        #text included\n",
    "                        returnList = returnList \n",
    "                        \n",
    "                    elif(subJsonType == \"code\"):\n",
    "                        #text included\n",
    "                        returnList = returnList\n",
    "                        \n",
    "                    elif(subJsonType == \"phone\"):\n",
    "                        #text included\n",
    "                        returnList = returnList\n",
    "                        \n",
    "                    elif(subJsonType == \"strikethrough\"):\n",
    "                        #text included\n",
    "                        returnList.append(subJsonString[\"text\"])\n",
    "                        \n",
    "                    elif(subJsonType == \"pre\"):\n",
    "                        #text and language included\n",
    "                        returnList.append(subJsonString[\"text\"])\n",
    "                        \n",
    "                    elif(subJsonType == \"bank_card\"):\n",
    "                        #text included\n",
    "                        returnList = returnList\n",
    "                        \n",
    "                    else:\n",
    "                        print(\"- Error: Unknown type \" + subJsonType)\n",
    "                        returnList = returnList\n",
    "\n",
    "                else:\n",
    "                    # Is Normal Sub String\n",
    "                    returnList.append(messageString)\n",
    "\n",
    "            return (''.join(returnList), processedURLs, processedHashtags, processedBolds, processedItalics, processedUnderlines, processedEmails)\n",
    "        \n",
    "        except:\n",
    "            #Parser error\n",
    "            print(\"- Warn: Json parser error (set return text to inputText) >>\" + text + \"<<\")\n",
    "            return (text, processedURLs, processedHashtags, processedBolds, processedItalics, processedUnderlines, processedEmails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Process now DS-08-10-2020/ChatExport_2020-09-25-xavier]\n",
      "16.378s\n",
      "[Process now DS-22-10-2020/ChatExport_2020-10-13-xavierChat]\n",
      "22.595s\n",
      "[Process now DS-08-10-2020/ChatExport_2020-09-25-janich]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"list\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-c2bff765da55>\u001b[0m in \u001b[0;36mextractTextData\u001b[0;34m(processedIsFormattedText, text)\u001b[0m\n\u001b[1;32m     42\u001b[0m                     \u001b[0;31m# Is Json Sub String\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                     \u001b[0msubJsonString\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdemjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessageString\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                     \u001b[0msubJsonType\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubJsonString\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"type\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/demjson.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(txt, encoding, **kwargs)\u001b[0m\n\u001b[1;32m   5695\u001b[0m     \u001b[0;31m# Now do the actual JSON decoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5696\u001b[0;31m     result = j.decode( txt,\n\u001b[0m\u001b[1;32m   5697\u001b[0m                        \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/demjson.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, txt, encoding, return_errors, return_stats)\u001b[0m\n\u001b[1;32m   4887\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4888\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_decode\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m)\u001b[0m    \u001b[0;31m# DECODE!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4889\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mJSONException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/demjson.py\u001b[0m in \u001b[0;36m_do_decode\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m   4979\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mdec_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4980\u001b[0;31m                 \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecodeobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mat_document_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/demjson.py\u001b[0m in \u001b[0;36mdecodeobj\u001b[0;34m(self, state, identifier_as_string, at_document_start)\u001b[0m\n\u001b[1;32m   4841\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_depth_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4842\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_composite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4843\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/demjson.py\u001b[0m in \u001b[0;36mdecode_composite\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m   4747\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4748\u001b[0;31m                     \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecodeobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4749\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskipws\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/demjson.py\u001b[0m in \u001b[0;36mdecodeobj\u001b[0;34m(self, state, identifier_as_string, at_document_start)\u001b[0m\n\u001b[1;32m   4849\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_string_quotes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4850\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4851\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m'.+-'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/demjson.py\u001b[0m in \u001b[0;36mdecode_string\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m   4084\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msaw_final_quote\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshould_stop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4085\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat_end\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4086\u001b[0m                 state.push_error(\"String literal is not terminated\",\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/demjson.py\u001b[0m in \u001b[0;36mat_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1742\u001b[0m         \"\"\"\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/demjson.py\u001b[0m in \u001b[0;36mpeek\u001b[0;34m(self, offset)\u001b[0m\n\u001b[1;32m   1781\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1782\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__rawbuf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-9f2ee71085a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mdfMessages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"processedIsFormattedText\"\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mdfMessages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckIsFormattedText\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mdfMessages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"extractedTextData\"\u001b[0m\u001b[0;34m]\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0mdfMessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mextractTextData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessedIsFormattedText\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mdfMessages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"processedValidText\"\u001b[0m\u001b[0;34m]\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mdfMessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgetExtractedParam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractedTextData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mdfMessages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"processedValidTextSize\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfMessages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"processedValidText\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwds)\u001b[0m\n\u001b[1;32m   7543\u001b[0m             \u001b[0mkwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7544\u001b[0m         )\n\u001b[0;32m-> 7545\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7547\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;31m# wrap results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    282\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m                     \u001b[0;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m                     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m                         \u001b[0;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-9f2ee71085a3>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mdfMessages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"processedIsFormattedText\"\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mdfMessages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckIsFormattedText\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mdfMessages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"extractedTextData\"\u001b[0m\u001b[0;34m]\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0mdfMessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mextractTextData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessedIsFormattedText\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mdfMessages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"processedValidText\"\u001b[0m\u001b[0;34m]\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mdfMessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgetExtractedParam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractedTextData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mdfMessages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"processedValidTextSize\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfMessages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"processedValidText\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-c2bff765da55>\u001b[0m in \u001b[0;36mextractTextData\u001b[0;34m(processedIsFormattedText, text)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;31m#Parser error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"- Warn: Json parser error (set return text to inputText) >>\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"<<\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessedURLs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessedHashtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessedBolds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessedItalics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessedUnderlines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessedEmails\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"list\") to str"
     ]
    }
   ],
   "source": [
    "# Timer Start\n",
    "timeStartGlobal = time.time()\n",
    "\n",
    "# Add Key = filePath / Value = DataFrame (Metadata)\n",
    "dictMeta = {}\n",
    "for fP in dfInputFiles.inputPath:\n",
    "    \n",
    "    dictMeta[fP] = convertToDataFrameMeta(fP)\n",
    "\n",
    "# Add Key = filePath / Value = DataFrame (Messages)\n",
    "dictMessages = {}\n",
    "for fP in dfInputFiles.inputPath:\n",
    "\n",
    "    timeStartSingle = time.time()\n",
    "    print(\"[Process now \" + fP + \"]\")\n",
    "    dfMessages = convertToDataFrameMessages(fP, dictMeta)\n",
    "    \n",
    "    dfMessages[\"processedChannelFilePath\"]  = fP\n",
    "    dfMessages[\"processedChannelType\"]      = dictMeta[fP].type.iloc[0]\n",
    "    dfMessages[\"processedRawTextSize\"]      = dfMessages[\"text\"].str.len()\n",
    "    dfMessages[\"processedIsFormattedText\"]  = dfMessages[\"text\"].apply(checkIsFormattedText)\n",
    "    \n",
    "    dfMessages[\"extractedTextData\"]      = dfMessages.apply(lambda x: extractTextData(x.processedIsFormattedText, x.text), axis=1)\n",
    "    dfMessages[\"processedValidText\"]     = dfMessages.apply(lambda x: getExtractedParam(0, x.extractedTextData), axis=1)\n",
    "    dfMessages[\"processedValidTextSize\"] = dfMessages[\"processedValidText\"].str.len()\n",
    "    \n",
    "    dfMessages[\"processedTextDataURLs\"]       = dfMessages.apply(lambda x: getExtractedParam(1, x.extractedTextData), axis=1)\n",
    "    dfMessages[\"processedTextDataHashtags\"]   = dfMessages.apply(lambda x: getExtractedParam(2, x.extractedTextData), axis=1)\n",
    "    dfMessages[\"processedTextDataBolds\"]      = dfMessages.apply(lambda x: getExtractedParam(3, x.extractedTextData), axis=1)\n",
    "    dfMessages[\"processedTextDataItalics\"]    = dfMessages.apply(lambda x: getExtractedParam(4, x.extractedTextData), axis=1)\n",
    "    dfMessages[\"processedTextDataUnderlines\"] = dfMessages.apply(lambda x: getExtractedParam(5, x.extractedTextData), axis=1)\n",
    "    dfMessages[\"processedTextDataEmails\"]     = dfMessages.apply(lambda x: getExtractedParam(6, x.extractedTextData), axis=1)\n",
    "    \n",
    "    dictMessages[fP] = dfMessages\n",
    "    timeEndSingle = time.time()\n",
    "    print('{:5.3f}s'.format(timeEndSingle-timeStartSingle))\n",
    "\n",
    "# All Messages to DataFrame\n",
    "dfAllDataMessages = pd.DataFrame()\n",
    "for fP in dfInputFiles.inputPath:\n",
    "    \n",
    "    #print(\"[Append now \" + fP + \"]\")\n",
    "    dfMessages        = dictMessages[fP].copy()\n",
    "    dfAllDataMessages = dfAllDataMessages.append(dfMessages)\n",
    "\n",
    "# Print Time\n",
    "timeEndGlobal = time.time()\n",
    "print()\n",
    "print(\"[Finished global]\")\n",
    "print('{:5.3f}s'.format(timeEndGlobal-timeStartGlobal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfMessages = dfAllDataMessages.copy()\n",
    "#dfMessages = dfMessages[dfMessages.astype(str)[\"processedTextDataEmails\"] != \"[]\"]\n",
    "#t = dfMessages.sort_values(by=\"processedValidTextSize\", ascending=False).iloc[3]\n",
    "#print(\">>\" + str(t.text) + \"<<\")\n",
    "#print()\n",
    "#print(\">>\" + str(t.processedTextDataBolds) + \"<<\")\n",
    "#print(\">>\" + str(t.processedTextDataURLs) + \"<<\")\n",
    "#print(\">>\" + str(t.processedTextDataHashtags) + \"<<\")\n",
    "#print(\">>\" + str(t.processedTextDataItalics) + \"<<\")\n",
    "#print(\">>\" + str(t.processedTextDataUnderlines) + \"<<\")\n",
    "#print(\">>\" + str(t.processedTextDataEmails) + \"<<\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type of channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfInputFiles.inputType.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only in different types of channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAllDataMessages.columns.difference(\n",
    "    dfAllDataMessages[dfAllDataMessages.processedChannelType == \"public_channel\"].dropna(how='all', axis=1).columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfAllDataMessages.columns.difference(\n",
    "    dfAllDataMessages[dfAllDataMessages.processedChannelType == \"public_supergroup\"].dropna(how='all', axis=1).columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vorsicht: Wenige Daten\n",
    "dfAllDataMessages.columns.difference(\n",
    "    dfAllDataMessages[dfAllDataMessages.processedChannelType == \"private_supergroup\"].dropna(how='all', axis=1).columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryChannelId(filePath):\n",
    "    dfMeta = dictMeta[filePath].copy()\n",
    "    return str(dfMeta[\"id\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryChannelName(filePath):\n",
    "    dfMeta = dictMeta[filePath].copy()\n",
    "    inputStr  = str(dfMeta[\"name\"].iloc[0])\n",
    "    outputStr = inputStr.encode('ascii', 'ignore')\n",
    "    outputStr = outputStr.decode('ascii')\n",
    "    return outputStr[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryChannelType(filePath):\n",
    "    dfMeta = dictMeta[filePath].copy()\n",
    "    return str(dfMeta[\"type\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryChannelCountEntries(filePath):\n",
    "    dfMessages = dictMessages[filePath].copy()\n",
    "    return len(dfMessages.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryChannelCountRawText(filePath):\n",
    "    dfMessages = dictMessages[filePath].copy()\n",
    "    dfMessages = dfMessages[dfMessages.processedRawTextSize > 0]\n",
    "    return len(dfMessages.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryChannelCountIsFormattedText(filePath):\n",
    "    dfMessages = dictMessages[filePath].copy()\n",
    "    dfMessages = dfMessages[dfMessages.processedIsFormattedText == True]\n",
    "    return len(dfMessages.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryChannelCountValidText(filePath):\n",
    "    dfMessages = dictMessages[filePath].copy()\n",
    "    dfMessages = dfMessages[dfMessages.processedValidTextSize > 0]\n",
    "    return len(dfMessages.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO mit process\n",
    "def queryChannelCountPhoto(filePath):\n",
    "    dfMessages = dictMessages[filePath].copy()\n",
    "    if \"photo\" not in dfMessages.columns:\n",
    "        return 0\n",
    "    else:\n",
    "        dfMessages = dfMessages.photo.dropna()\n",
    "        return len(dfMessages.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO mit process\n",
    "def queryChannelCountFile(filePath):\n",
    "    dfMessages = dictMessages[filePath].copy()\n",
    "    if \"file\" not in dfMessages.columns:\n",
    "        return 0\n",
    "    else:\n",
    "        dfMessages = dfMessages.file.dropna()\n",
    "        return len(dfMessages.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO mit process\n",
    "def queryChannelCountEdited(filePath):\n",
    "    dfMessages = dictMessages[filePath].copy()\n",
    "    if \"edited\" not in dfMessages.columns:\n",
    "        return 0\n",
    "    else:\n",
    "        dfMessages = dfMessages.edited.dropna()\n",
    "        return len(dfMessages.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryCalcPercent(countFiltered, countTotal):\n",
    "    return (countFiltered / countTotal) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refactor\n",
    "\n",
    "dfQueryMeta = pd.DataFrame(dfInputFiles.inputPath)\n",
    "\n",
    "dfQueryMeta[\"queryChannelId\"]                     = dfQueryMeta.inputPath.apply(queryChannelId)\n",
    "dfQueryMeta[\"queryChannelName\"]                   = dfQueryMeta.inputPath.apply(queryChannelName)\n",
    "dfQueryMeta[\"queryChannelType\"]                   = dfQueryMeta.inputPath.apply(queryChannelType)\n",
    "dfQueryMeta[\"queryChannelCountEntries\"]           = dfQueryMeta.inputPath.apply(queryChannelCountEntries)\n",
    "\n",
    "dfQueryMeta[\"queryChannelCountRawText\"]           = dfQueryMeta.inputPath.apply(queryChannelCountRawText)\n",
    "dfQueryMeta[\"queryChannelCountIsFormattedText\"]   = dfQueryMeta.inputPath.apply(queryChannelCountIsFormattedText)\n",
    "dfQueryMeta[\"queryChannelCountValidText\"]         = dfQueryMeta.inputPath.apply(queryChannelCountValidText)\n",
    "dfQueryMeta[\"queryChannelCountPhoto\"]             = dfQueryMeta.inputPath.apply(queryChannelCountPhoto)\n",
    "dfQueryMeta[\"queryChannelCountFile\"]              = dfQueryMeta.inputPath.apply(queryChannelCountFile)\n",
    "dfQueryMeta[\"queryChannelCountEdited\"]            = dfQueryMeta.inputPath.apply(queryChannelCountEdited)\n",
    "\n",
    "dfQueryMeta[\"queryChannelPercentRawText\"]         = queryCalcPercent(dfQueryMeta[\"queryChannelCountRawText\"], dfQueryMeta[\"queryChannelCountEntries\"])\n",
    "dfQueryMeta[\"queryChannelPercentIsFormattedText\"] = queryCalcPercent(dfQueryMeta[\"queryChannelCountIsFormattedText\"], dfQueryMeta[\"queryChannelCountEntries\"])\n",
    "dfQueryMeta[\"queryChannelPercentValidText\"]       = queryCalcPercent(dfQueryMeta[\"queryChannelCountValidText\"], dfQueryMeta[\"queryChannelCountEntries\"])\n",
    "dfQueryMeta[\"queryChannelPercentPhoto\"]           = queryCalcPercent(dfQueryMeta[\"queryChannelCountPhoto\"], dfQueryMeta[\"queryChannelCountEntries\"])\n",
    "dfQueryMeta[\"queryChannelPercentFile\"]            = queryCalcPercent(dfQueryMeta[\"queryChannelCountFile\"], dfQueryMeta[\"queryChannelCountEntries\"])\n",
    "dfQueryMeta[\"queryChannelPercentEdited\"]          = queryCalcPercent(dfQueryMeta[\"queryChannelCountEdited\"], dfQueryMeta[\"queryChannelCountEntries\"])\n",
    "\n",
    "dfQueryMeta.sort_values(by=\"queryChannelCountEntries\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryPlotter(attributeName):\n",
    "    dfFilter = dfQueryMeta.copy()\n",
    "    sns.catplot(\n",
    "        y=\"queryChannelName\",\n",
    "        x=attributeName,\n",
    "        data=dfFilter,\n",
    "        hue=\"queryChannelType\",\n",
    "        #aspect=1.2,\n",
    "        #palette=\"rocket\",\n",
    "        kind=\"bar\",\n",
    "        height=7,\n",
    "        order=dfFilter.sort_values(attributeName, ascending=False).queryChannelName\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryPlotter(\"queryChannelCountEntries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#queryPlotter(\"queryChannelPercentRawText\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryPlotter(\"queryChannelPercentIsFormattedText\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#queryPlotter(\"queryChannelPercentValidText\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#queryPlotter(\"queryChannelPercentPhoto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#queryPlotter(\"queryChannelPercentFile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryPlotter(\"queryChannelPercentEdited\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get valid text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizedValidTextSize(df):\n",
    "    df = df.copy()\n",
    "    # https://stackoverflow.com/questions/23199796/detect-and-exclude-outliers-in-pandas-data-frame\n",
    "    # keep only the ones that are within +3 to -3 standard deviations in the column 'Data'.\n",
    "    return df[np.abs(df.processedValidTextSize-df.processedValidTextSize.mean()) <= (3*df.processedValidTextSize.std())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMessages = dfAllDataMessages.copy()\n",
    "print(\"Before all filters\\t\\t\" + str(len(dfMessages.index)))\n",
    "\n",
    "dfMessages = dfMessages[dfMessages.processedValidTextSize > 10]\n",
    "print(\"Before normalizedValidTextSize\\t\" + str(len(dfMessages.index)))\n",
    "\n",
    "dfMessages = normalizedValidTextSize(dfMessages)\n",
    "print(\"After normalizedValidTextSize\\t\" + str(len(dfMessages.index)))\n",
    "\n",
    "_ = dfMessages.processedValidTextSize.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Extract hashtags in non formatted text\n",
    "\n",
    "def extractImportantHashtags(filePath):\n",
    "    dfMessages = dictMessages[filePath].copy()\n",
    "    dfMessages = dfMessages[dfMessages.astype(str)[\"processedTextDataHashtags\"] != \"[]\"]\n",
    "\n",
    "    hashTagList = list()\n",
    "    for index, row in dfMessages.iterrows():\n",
    "        for hashtagItem in row[\"processedTextDataHashtags\"]:\n",
    "            hashTagList.append(hashtagItem)\n",
    "\n",
    "    return Counter(hashTagList).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractImportantHashtags(\"DS-08-10-2020/ChatExport_2020-09-25-janich\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractImportantHashtags(\"DS-08-10-2020/ChatExport_2020-09-27-evaherman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractImportantHashtags(\"DS-08-10-2020/ChatExport_2020-09-25-hildmann\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractImportantHashtags(\"DS-08-10-2020/ChatExport_2020-09-25-xavier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: No Hostname if string startsWith ! \"http\"\n",
    "# TODO: Url in non formatted text\n",
    "# TODO: Add mention and other attributes\n",
    "# TODO: mention in non formatted text? and other attributes\n",
    "\n",
    "# TODO: Check if both set (from and from_id, actor, ...)\n",
    "# TODO: Add cache attributes ^^ from_id -> from (map with validator)\n",
    "\n",
    "# TODO: Duplicates in refs from text\n",
    "\n",
    "# Text ref important for finding groups\n",
    "# forwared_from important for graphs\n",
    "\n",
    "# Return  Counter forwarded_from\n",
    "def extractImportantUrls(filePath):\n",
    "    dfMessages = dictMessages[filePath].copy()\n",
    "\n",
    "    hostList = list()\n",
    "    urList   = list()\n",
    "    refList = list()\n",
    "    for index, row in dfMessages.iterrows():\n",
    "        if(str(row[\"processedTextDataURLs\"]) != \"[]\"):\n",
    "            for urlItem in row[\"processedTextDataURLs\"]:\n",
    "                urlData = urlparse(str(urlItem))\n",
    "\n",
    "                completeHostname = urlData.hostname\n",
    "                completeUrl      = urlData.geturl()\n",
    "\n",
    "                hostList.append(str(completeHostname))\n",
    "                urList.append(str(completeUrl))\n",
    "\n",
    "                if \"t.me\" in str(completeHostname):\n",
    "                    refList.append(str(completeUrl))\n",
    "            \n",
    "    forwardedFromList = list()\n",
    "    if(\"forwarded_from\" in dfMessages.columns):\n",
    "        for index, row in dfMessages.iterrows():        \n",
    "            forwardedFromList.append(str(row[\"forwarded_from\"]))\n",
    "            \n",
    "    actorList = list()\n",
    "    if(\"actor\" in dfMessages.columns):\n",
    "        for index, row in dfMessages.iterrows():\n",
    "            actorList.append(str(row[\"actor\"]))\n",
    "    \n",
    "    memberList = list()\n",
    "    if(\"members\" in dfMessages.columns):\n",
    "        for index, row in dfMessages.iterrows():\n",
    "            if(str(row[\"members\"]) != \"nan\"):\n",
    "                for memberItem in row[\"members\"]:\n",
    "                    memberList.append(str(memberItem))\n",
    "                    \n",
    "    fromList = list()\n",
    "    if(\"from\" in dfMessages.columns):\n",
    "        for index, row in dfMessages.iterrows():\n",
    "            fromList.append(str(row[\"from\"]))\n",
    "            \n",
    "    savedFromList = list()\n",
    "    if(\"saved_from\" in dfMessages.columns):\n",
    "        for index, row in dfMessages.iterrows():\n",
    "            savedFromList.append(str(row[\"saved_from\"]))\n",
    "        \n",
    "    print(\"########################################\")\n",
    "    print(\"###### Top 20 Hosts ####################\")\n",
    "    print(\"########################################\")\n",
    "    print (\"\\n\".join(map(str, Counter(hostList).most_common(20))))\n",
    "    print()\n",
    "\n",
    "    print(\"########################################\")\n",
    "    print(\"###### Top 20 URLs #####################\")\n",
    "    print(\"########################################\")\n",
    "    print (\"\\n\".join(map(str, Counter(urList).most_common(20))))\n",
    "    print()\n",
    "\n",
    "    print(\"########################################\")\n",
    "    print(\"###### Top 20 Refs from text ###########\")\n",
    "    print(\"########################################\")\n",
    "    print (\"\\n\".join(map(str, Counter(refList).most_common(20))))\n",
    "    print()\n",
    "\n",
    "    print(\"########################################\")\n",
    "    print(\"###### Top 20 Refs (forwarded_from) ####\")\n",
    "    print(\"########################################\")\n",
    "    print (\"\\n\".join(map(str, Counter(forwardedFromList).most_common(20))))\n",
    "    print()\n",
    "    \n",
    "    print(\"########################################\")\n",
    "    print(\"###### Top 20 Refs (actor) #############\")\n",
    "    print(\"########################################\")\n",
    "    print (\"\\n\".join(map(str, Counter(actorList).most_common(20))))\n",
    "    print()\n",
    "    \n",
    "    print(\"########################################\")\n",
    "    print(\"###### Top 20 Refs (members) ###########\")\n",
    "    print(\"########################################\")\n",
    "    print (\"\\n\".join(map(str, Counter(memberList).most_common(20))))\n",
    "    print()\n",
    "    \n",
    "    print(\"########################################\")\n",
    "    print(\"###### Top 20 Refs (from) ##############\")\n",
    "    print(\"########################################\")\n",
    "    print (\"\\n\".join(map(str, Counter(fromList).most_common(20))))\n",
    "    print()\n",
    "    \n",
    "    print(\"########################################\")\n",
    "    print(\"###### Top 20 Refs (saved_from) ########\")\n",
    "    print(\"########################################\")\n",
    "    print (\"\\n\".join(map(str, Counter(savedFromList).most_common(20))))\n",
    "    print()\n",
    "    \n",
    "    return Counter(forwardedFromList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_ = extractImportantUrls(\"DS-08-10-2020/ChatExport_2020-09-25-janich\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_ = extractImportantUrls(\"DS-08-10-2020/ChatExport_2020-09-27-evaherman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "counterSampleChat = extractImportantUrls(\"DS-08-10-2020/ChatExport_2020-09-25-hildmann\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_ = extractImportantUrls(\"DS-08-10-2020/ChatExport_2020-09-25-xavier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Just for test purposes\n",
    "_ = extractImportantUrls(\"DS-22-10-2020/ChatExport_2020-10-13-xavierChat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Difference capital letters?\n",
    "#TODO Context?\n",
    "\n",
    "# Use \"global\" for all DataFrames\n",
    "def plotChannelWordCloud(filePath, label, filterList):\n",
    "    timeStart = time.time()\n",
    "    \n",
    "    if(filePath != \"global\"):\n",
    "        dfMessages = dictMessages[filePath].copy()\n",
    "    else:\n",
    "        dfMessages = dfAllDataMessages.copy()\n",
    "    \n",
    "    print(\"[Start transform text to global text string]\")\n",
    "    textList = []\n",
    "    for index, row in dfMessages.iterrows():\n",
    "        textList.append(\" \" + row[\"processedValidText\"])\n",
    "        \n",
    "    textString = ''.join(textList)\n",
    "    \n",
    "    germanStopWordsList = nltk.corpus.stopwords.words('german')\n",
    "    germanStopWordsList.append(\"http\")\n",
    "    germanStopWordsList.append(\"https\")\n",
    "    germanStopWordsList.append(\"ja\") #TODO: split to local - maybe?\n",
    "    germanStopWordsList.append(\"nein\")\n",
    "    germanStopWordsList.append(\"mehr\")\n",
    "    germanStopWordsList.append(\"mal\")\n",
    "    germanStopWordsList.append(\"schon\")\n",
    "    germanStopWordsList.append(\"immer\")\n",
    "    germanStopWordsList.append(\"wurde\")\n",
    "    germanStopWordsList.append(\"wurden\")\n",
    "    germanStopWordsList.append(\"sei\")\n",
    "    germanStopWordsList.append(\"sein\")\n",
    "    germanStopWordsList.append(\"viel\")\n",
    "    germanStopWordsList.append(\"viele\")\n",
    "    germanStopWordsList.append(\"wegen\")\n",
    "    germanStopWordsList.append(\"müssen\")\n",
    "    germanStopWordsList.append(\"geht\")\n",
    "    germanStopWordsList.append(\"gibt\")\n",
    "    germanStopWordsList.append(\"wer\")\n",
    "    germanStopWordsList.append(\"wie\")\n",
    "    germanStopWordsList.append(\"was\")\n",
    "    germanStopWordsList.append(\"macht\")\n",
    "    germanStopWordsList.append(\"machen\")\n",
    "    germanStopWordsList.append(\"machte\")\n",
    "    germanStopWordsList.append(\"kommen\")\n",
    "    germanStopWordsList.append(\"kommt\")\n",
    "    germanStopWordsList.append(\"glaube\")\n",
    "    germanStopWordsList.append(\"glaubst\")\n",
    "    germanStopWordsList.append(\"tun\")\n",
    "    germanStopWordsList.append(\"wäre\")\n",
    "    germanStopWordsList.append(\"sagte\")\n",
    "    germanStopWordsList.append(\"sagten\")\n",
    "    germanStopWordsList.append(\"hat\")\n",
    "    germanStopWordsList.append(\"hast\")\n",
    "    germanStopWordsList.append(\"haben\")\n",
    "    germanStopWordsList.append(\"habt\")\n",
    "    germanStopWordsList.append(\"statt\")\n",
    "    germanStopWordsList.append(\"genau\")\n",
    "    germanStopWordsList.append(\"sagen\")\n",
    "    germanStopWordsList.append(\"sagte\")\n",
    "    germanStopWordsList.append(\"sagten\")\n",
    "    germanStopWordsList.append(\"bitte\")\n",
    "    germanStopWordsList.append(\"bitten\")\n",
    "    germanStopWordsList.append(\"danke\")\n",
    "    germanStopWordsList.append(\"dank\")\n",
    "    germanStopWordsList.append(\"sollen\")\n",
    "    germanStopWordsList.append(\"soll\")\n",
    "    germanStopWordsList.append(\"sollte\")\n",
    "    germanStopWordsList.append(\"sehen\")\n",
    "    germanStopWordsList.append(\"seht\")\n",
    "    germanStopWordsList.append(\"zeigen\")\n",
    "    germanStopWordsList.append(\"zeigt\")\n",
    "    germanStopWordsList.append(\"sei\")\n",
    "    germanStopWordsList.append(\"sein\")\n",
    "    germanStopWordsList.append(\"seid\")\n",
    "    germanStopWordsList.append(\"seit\")\n",
    "    germanStopWordsList.append(\"laut\")\n",
    "    germanStopWordsList.append(\"lauten\")\n",
    "    germanStopWordsList.append(\"sehen\")\n",
    "    germanStopWordsList.append(\"seht\")\n",
    "    germanStopWordsList.append(\"haben\")\n",
    "    germanStopWordsList.append(\"hat\")\n",
    "    germanStopWordsList.append(\"hätten\")\n",
    "    germanStopWordsList.append(\"sagte\")\n",
    "    germanStopWordsList.append(\"sag\")\n",
    "    germanStopWordsList.append(\"sagt\")\n",
    "    germanStopWordsList.append(\"ab\")\n",
    "    germanStopWordsList.append(\"bei\")\n",
    "    germanStopWordsList.append(\"beim\")\n",
    "    germanStopWordsList.append(\"denen\")\n",
    "    germanStopWordsList.append(\"gab\")\n",
    "    germanStopWordsList.append(\"ab\")\n",
    "    \n",
    "    for fItem in filterList:\n",
    "        germanStopWordsList.append(fItem)\n",
    "    \n",
    "    print(\"[Start generate wordCloud]\")\n",
    "    wordcloud = WordCloud(\n",
    "                background_color=\"black\",\n",
    "                width=1920,\n",
    "                height=1080,\n",
    "                stopwords=germanStopWordsList\n",
    "            ).generate(textString)\n",
    "    wordcloud.to_file(\"wordcloud-\" + label + \".png\")\n",
    "    \n",
    "    print(\"Top 20 occ:\\n\" + str(pd.Series(wordcloud.words_).head(20)))\n",
    "    \n",
    "    print(\"[Start generate figure]\")\n",
    "    plt.figure(figsize=(14, 14))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.show()\n",
    "    \n",
    "    timeEnd = time.time()\n",
    "    print(\"[Finished]\")\n",
    "    print('{:5.3f}s'.format(timeEnd-timeStart))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Public channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oliver Janich öffentlich (public_channel)\n",
    "plotChannelWordCloud(\n",
    "    \"DS-08-10-2020/ChatExport_2020-09-25-janich\",\n",
    "    \"pc-janich\",\n",
    "    []\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eva Herman Offiziell (public_channel)\n",
    "plotChannelWordCloud(\n",
    "    \"DS-08-10-2020/ChatExport_2020-09-27-evaherman\",\n",
    "    \"pc-evaHerman\",\n",
    "    []\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATTILA HILDMANN OFFICIAL (public_channel)\n",
    "plotChannelWordCloud(\n",
    "    \"DS-08-10-2020/ChatExport_2020-09-25-hildmann\",\n",
    "    \"pc-hildmann\",\n",
    "    [\"ATTILAHILDMANN CHAT\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Xavier Naidoo (public_channel)\n",
    "plotChannelWordCloud(\n",
    "    \"DS-08-10-2020/ChatExport_2020-09-25-xavier\",\n",
    "    \"pc-xavier\",\n",
    "    [\"xavier_naidoo\", \"Xavier_Naidoo\", \"politische_bildersprueche\", \"einmal_hin_alles_drin\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerechtigkeit für das Vaterland (public_channel)\n",
    "#plotChannelWordCloud(\n",
    "#    \"DS-08-10-2020/ChatExport_2020-09-26-gerechtigkeitVaterland\",\n",
    "#    \"pc-GerechtigkeitfuerdasVaterland\",\n",
    "#    [\"gerechtigkeitfuersvaterland\"]\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corona Virus Informationen (public_channel)\n",
    "#plotChannelWordCloud(\n",
    "#    \"DS-08-10-2020/ChatExport_2020-09-26-cvirusinfo\",\n",
    "#    \"pc-cVirusInfo\",\n",
    "#    [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liberté (public_channel)\n",
    "#plotChannelWordCloud(\n",
    "#    \"DS-08-10-2020/ChatExport_2020-09-26-liberte\",\n",
    "#    \"pc-liberte\",\n",
    "#    []\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for test purposes\n",
    "plotChannelWordCloud(\n",
    "    \"global\",\n",
    "    \"global\",\n",
    "    []\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concordance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tbd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#G_weighted = nx.DiGraph()\n",
    "\n",
    "#G_weighted.add_node('A', weight=1500)\n",
    "#G_weighted.add_node('B', weight=800)\n",
    "#G_weighted.add_node('C', weight=200)\n",
    "#G_weighted.add_node('D', weight=500)\n",
    "\n",
    "#G_weighted.add_edge('A', 'B', weight=8)\n",
    "#G_weighted.add_edge('A', 'C', weight=2)\n",
    "#G_weighted.add_edge('A', 'D', weight=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_weighted = nx.DiGraph()\n",
    "\n",
    "globalSize = 0\n",
    "\n",
    "for aKey in counterSampleChat:\n",
    "    \n",
    "    groupCount = counterSampleChat[aKey]\n",
    "    \n",
    "    inputStr  = str(aKey)\n",
    "    outputStr = inputStr.encode('ascii', 'ignore')\n",
    "    groupName = outputStr.decode('ascii')\n",
    "    groupName = groupName[:25]\n",
    "    \n",
    "    if(groupCount > 3 and groupCount < 99999999 and groupName != \"nan\"):\n",
    "        globalSize = globalSize + groupCount\n",
    "\n",
    "G_weighted.add_node(\"target\", weight=globalSize)\n",
    "        \n",
    "# TODO: Refactor\n",
    "    \n",
    "for aKey in counterSampleChat:\n",
    "    \n",
    "    groupCount = counterSampleChat[aKey]\n",
    "    \n",
    "    inputStr  = str(aKey)\n",
    "    outputStr = inputStr.encode('ascii', 'ignore')\n",
    "    groupName = outputStr.decode('ascii')\n",
    "    groupName = groupName[:25]\n",
    "    \n",
    "    if(groupCount > 3 and groupCount < 99999999 and groupName != \"nan\"):\n",
    "    \n",
    "        print(\"Add \" + str(groupCount) + \"\\t\" + str(groupName))\n",
    "    \n",
    "        G_weighted.add_node(groupName, weight=groupCount * 5)\n",
    "        G_weighted.add_edge(\"target\", groupName, weight=groupCount / 100)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "# TODO https://networkx.org/documentation/stable/reference/drawing.html#module-networkx.drawing.layout\n",
    "# (try different layouts e.g. circular_layout)\n",
    "\n",
    "pos = nx.spring_layout(G_weighted)\n",
    "\n",
    "nx.draw(G_weighted,\n",
    "        pos,\n",
    "        #node_color='lightgreen',\n",
    "        with_labels=True,\n",
    "        #edge_color=colors, \n",
    "        width=list(nx.get_edge_attributes(G_weighted, \"weight\").values()),\n",
    "        #edge_labels=edge_labels,\n",
    "        node_size=list(nx.get_node_attributes(G_weighted,'weight').values()),\n",
    "        arrowsize=1,\n",
    "       )\n",
    "\n",
    "edge_labels = nx.get_edge_attributes(G_weighted, \"weight\")\n",
    "\n",
    "_ = nx.draw_networkx_edge_labels(G_weighted, pos, edge_labels=edge_labels)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from ipywidgets import interact\n",
    "#import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interact(plot_random_graph, n=(2,30), m=(1,10), k=(1,10), p=(0.0, 1.0, 0.001),\n",
    "#         generator={\n",
    "#             'lobster': random_lobster,\n",
    "#             'power law': powerlaw_cluster,\n",
    "#             'Newman-Watts-Strogatz': newman_watts_strogatz,\n",
    "#             u'Erdős-Rényi': erdos_renyi,\n",
    "#         });"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
