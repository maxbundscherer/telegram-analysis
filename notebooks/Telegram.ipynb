{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telegram Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Set flag to true, if you work in visual studio code (connected to custom docker)\n",
    "Set flag to false, if you work in browser (jupyter notebook ui from custom docker)\n",
    "\"\"\"\n",
    "C_LOCAL                 = True\n",
    "\n",
    "\"\"\"\n",
    "Set flag to true, if you want process no long term running tasks and take spot check from data\n",
    "\"\"\"\n",
    "C_SHORT_RUN             = False\n",
    "C_NUMBER_SAMPLES        = 1000 #only if C_SHORT_RUN set to true and if you dont use cache\n",
    "\n",
    "\"\"\"\n",
    "Resolve new urls?\n",
    "Set flag to false, if you dont want to resolve new urls\n",
    "\"\"\"\n",
    "C_RESOLVE_NEW_URLS      = False\n",
    "\n",
    "\"\"\"\n",
    "Load DataSets (global)\n",
    "Ava:    [\"dataSet0\", \"dataSet1\", \"dataSet1a\", \"dataSet2\"]\n",
    "Htdocs: [\"dataSet0\", \"dataSet1a\", \"dataSet2\"]\n",
    "Req:    [\"dataSet0]\n",
    "\"\"\"\n",
    "C_LOAD_DATASET          = [\"dataSet0\", \"dataSet1\", \"dataSet1a\", \"dataSet2\"]\n",
    "\n",
    "\"\"\"\n",
    "Load Piplelines? (HuggingFace transformers)\n",
    "\"\"\"\n",
    "C_LOAD_PIPELINES        = True\n",
    "C_PIPELINE_DATASET      = [\"dataSet0\"]\n",
    "\n",
    "\"\"\"\n",
    "Time Plot Freq\n",
    "e.g. 1M = 1 Month\n",
    "e.g. 1W = 1 Week\n",
    "e.g. 1D = 1 Day\n",
    "\"\"\"\n",
    "C_TIME_PLOT_FREQ        = \"1D\"\n",
    "\n",
    "\"\"\"\n",
    "Cache?\n",
    "Set C_USE_CACHE_FILE to \"\", if you want to use no cache!\n",
    "Set C_NEW_CACHE_FILE to \"\", if you want to create no cache!\n",
    "Please set only one value!\n",
    "Please create new cache if you change params above\n",
    "# e.g\n",
    "# - local-run-temp.pkl          (Short run, with hf, with htdocs-datasets) - deprecated\n",
    "# - local-run-ht-temp.pkl       (Long run, without hf, with htdocs-datasets) - deprecated\n",
    "# - long-run-server-21-01.pkl   (Long run, with hf, with htdocs-datasets) - deprecated\n",
    "#\n",
    "# - long-run-server-28-01.pkl   (Long run, with hf, with htdocs-datasets, updated with sen-pipe-2)\n",
    "# - long-run-server-07-02.pkl   (Long run, with hf, with all datasets, updated with sen-pipe-2)\n",
    "# - local-run-28-01.pkl         (Short run, with hf, with htdocs-datasets, updated with sen-pipe-2)\n",
    "# - test.pkl                    (Test file)\n",
    "\"\"\"\n",
    "#e.g. data-cache.pkl\n",
    "C_USE_CACHE_FILE        = \"long-run-server-07-02.pkl\"\n",
    "C_NEW_CACHE_FILE        = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import default libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Import url libs\n",
    "from urllib.parse import urlparse\n",
    "from collections import Counter\n",
    "import requests\n",
    "\n",
    "# File Handler Lib\n",
    "from pathlib import Path\n",
    "\n",
    "# Set graph widget (used by jupyter notebook)\n",
    "#%matplotlib notebook   #interactive graphs\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and import Graph Lib\n",
    "import networkx as nx\n",
    "#! pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and import  JSON Lib\n",
    "#! pip install demjson\n",
    "import demjson\n",
    "#import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and import Natural Language Toolkit\n",
    "#! pip install nltk\n",
    "import nltk\n",
    "\n",
    "# Ngrams\n",
    "import re\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim and pyLDAvis\n",
    "#! pip install gensim\n",
    "#! pip install pyLDAvis\n",
    "\n",
    "import gensim\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "\n",
    "import os\n",
    "\n",
    "# TODO Set to ignore?\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.simplefilter(\"once\")\n",
    "\n",
    "# SET TOKENIZERS_PARALLELISM\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and import demoji\n",
    "# Dont exists on conda\n",
    "import sys\n",
    "!{sys.executable} -m pip install demoji\n",
    "\n",
    "import demoji\n",
    "demoji.download_codes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and import WordCloud\n",
    "#! pip install wordcloud\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip install lxml\n",
    "from lxml.html import fromstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert and co.\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Hanover Tagger\n",
    "# Dont exists on conda\n",
    "!{sys.executable} -m pip install HanTa\n",
    "from HanTa import HanoverTagger as ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Install via conda\n",
    "!{sys.executable} -m pip install textblob-de\n",
    "from textblob_de import TextBlobDE as TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all columns (pandas hides columns by default)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plot style\n",
    "# TODO: Test different style\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set env vars\n",
    "if(C_LOCAL == True):\n",
    "    dir_var = \"./work/notebooks/\"\n",
    "else:\n",
    "    dir_var = \"./\"\n",
    "\n",
    "dir_var_output = dir_var + \"output/\"\n",
    "\n",
    "dir_var_cache= dir_var + \"cache/\"\n",
    "\n",
    "dir_var_pandas_cache = dir_var + \"cache/pandas/\"\n",
    "\n",
    "# Debug output\n",
    "! echo \"- Workdir -\"\n",
    "! ls -al $dir_var\n",
    "\n",
    "! echo\n",
    "! echo \"- Outputdir -\"\n",
    "! ls -al $dir_var_output\n",
    "\n",
    "! echo\n",
    "! echo \"- Cachedir -\"\n",
    "! ls -al $dir_var_cache\n",
    "\n",
    "! echo\n",
    "! echo \"- Pandas -\"\n",
    "! ls -al $dir_var_pandas_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictGloStopwatches = dict()\n",
    "\n",
    "# Start timer (for reporting)\n",
    "def gloStartStopwatch(key):\n",
    "    print(\"[Stopwatch started >>\" + str(key) + \"<<]\")\n",
    "    dictGloStopwatches[key] = time.time()\n",
    "\n",
    "# Stop timer (for reporting)\n",
    "def gloStopStopwatch(key):\n",
    "    endTime     = time.time()\n",
    "    startTime   = dictGloStopwatches[key]\n",
    "    print(\"[Stopwatch stopped >>\" + str(key) + \"<< (\" + '{:5.3f}s'.format(endTime-startTime) + \")]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Check if text is json formatted\n",
    "\n",
    "param   text        InputText\n",
    "param   singleMode  Boolean (set to true, if text is part of a message)\n",
    "\"\"\"\n",
    "def gloCheckIsTextJsonFormatted(text, singleMode):\n",
    "    textString = str(text)\n",
    "    if      (singleMode == False and textString.startswith(\"[\") == True and textString.endswith(\"]\") == True):\n",
    "        return True\n",
    "    elif    (singleMode == True and textString.startswith(\"{\") == True and textString.endswith(\"}\") == True):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gloReplaceGermanChars(inputText):\n",
    "\n",
    "    inputText = inputText.replace(\"ö\", \"oe\")\n",
    "    inputText = inputText.replace(\"ü\", \"ue\")\n",
    "    inputText = inputText.replace(\"ä\", \"ae\")\n",
    "\n",
    "    inputText = inputText.replace(\"Ö\", \"Oe\")\n",
    "    inputText = inputText.replace(\"Ü\", \"Ue\")\n",
    "    inputText = inputText.replace(\"Ä\", \"Ae\")\n",
    "\n",
    "    inputText = inputText.replace(\"ß\", \"ss\")\n",
    "    \n",
    "    return inputText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rm unsafe chars\n",
    "def gloConvertToSafeString(text):\n",
    "    text = demoji.replace(text, \"\")\n",
    "    text = gloReplaceGermanChars(text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    #text = text.encode('ascii', 'ignore')\n",
    "    #text = text.decode('ascii')\n",
    "    return text\n",
    "\n",
    "# Generate unique chat name\n",
    "def gloConvertToSafeChatName(chatName):\n",
    "    chatName = gloConvertToSafeString(chatName)\n",
    "    return chatName[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gloGetStopWordsList(filterList):\n",
    "\n",
    "    stopwWorldsList = []\n",
    "\n",
    "    deWordsList = nltk.corpus.stopwords.words('german')\n",
    "\n",
    "    enWordsList = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    aStopwords = []\n",
    "    with open(dir_var + \"additionalStopwords.txt\") as file:\n",
    "        for line in file: \n",
    "            line = line.strip()\n",
    "            if(line != \"\"):\n",
    "                aStopwords.append(line)\n",
    "\n",
    "    for s in filterList:\n",
    "        s = gloReplaceGermanChars(s)\n",
    "        stopwWorldsList.append(s)\n",
    "\n",
    "    for s in deWordsList:\n",
    "        s = gloReplaceGermanChars(s)\n",
    "        stopwWorldsList.append(s)\n",
    "\n",
    "    for s in enWordsList:\n",
    "        stopwWorldsList.append(s)\n",
    "\n",
    "    for s in aStopwords:\n",
    "        s = gloReplaceGermanChars(s)\n",
    "        stopwWorldsList.append(s)\n",
    "\n",
    "    return stopwWorldsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict File Cache\n",
    "dictFileCache = {}\n",
    "\n",
    "# Write dict to file (CSV)\n",
    "def gloWriteDictToFile(filename, targetDict):\n",
    "    dictFileCache = {} #Clear cache\n",
    "    d = pd.DataFrame.from_dict(targetDict, orient=\"index\")\n",
    "    d.to_csv(dir_var_cache + filename, header=False)\n",
    "\n",
    "# Read dict from file (CSV)\n",
    "def gloReadDictFromFile(filename):\n",
    "    # Cache?\n",
    "    if(filename in dictFileCache):\n",
    "        return dictFileCache[filename]\n",
    "\n",
    "    d = pd.read_csv(dir_var_cache + filename, header=None, index_col=0, squeeze=True)\n",
    "    retDict = d.to_dict()\n",
    "\n",
    "    dictFileCache[filename] = retDict #Add to cache\n",
    "\n",
    "    return retDict\n",
    "\n",
    "# Init csv file if not exists\n",
    "def gloInitFileDict(filename):\n",
    "    f = Path(dir_var_cache + filename)\n",
    "    if(f.exists() == False):\n",
    "        print(\"Init cache file >>\" + filename + \"<<\")\n",
    "        f.touch()\n",
    "        gloWriteDictToFile(filename, {\"initKey\": \"initValue\"})\n",
    "    else:\n",
    "        print(\"Cache already exists >>\" + filename + \"<<\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if is already cached\n",
    "def gloCheckIsAlreadyCached(filename, targetKey):\n",
    "    targetDict = gloReadDictFromFile(filename)\n",
    "    if(targetKey in targetDict.keys()):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Add key to cache\n",
    "def gloAddToCache(filename, targetKey, targetValue):\n",
    "    targetDict = gloReadDictFromFile(filename)\n",
    "    targetDict[targetKey] = targetValue\n",
    "    gloWriteDictToFile(filename, targetDict)\n",
    "\n",
    "# Get key from cache\n",
    "def gloGetCached(filename, targetKey):\n",
    "    targetDict = gloReadDictFromFile(filename)\n",
    "    return targetDict[targetKey]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param rowID e.g. procTDSafeText\n",
    "def gloGenerateTextFromChat(df, rowID):\n",
    "    df = df.copy()\n",
    "    df = df[df.procEvalIsValidText == True]\n",
    "    \n",
    "    # Iterate over text (global text from group)\n",
    "    textList = []\n",
    "    for index, row in df.iterrows():\n",
    "        textList.append(\" \" + row[rowID])\n",
    "        \n",
    "    textString = ''.join(textList)\n",
    "\n",
    "    return textString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Init Cache Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gloInitFileDict(\"resolved-urls.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gloInitFileDict(\"resolved-youtube.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictPipelines = {}\n",
    "\n",
    "def loadPipelines():\n",
    "\n",
    "    if(C_LOAD_PIPELINES == False):\n",
    "        print(\"Skip loading pipelines\")\n",
    "        return list()\n",
    "\n",
    "    gloStartStopwatch(\"Load Pipelines\")\n",
    "    \n",
    "\n",
    "    gloStartStopwatch(\"Load ner-xlm-Roberta\")\n",
    "    dictPipelines[\"ner-xlm-roberta\"] = pipeline(\n",
    "        'ner', \n",
    "        model='xlm-roberta-large-finetuned-conll03-german',\n",
    "        tokenizer='xlm-roberta-large-finetuned-conll03-german'\n",
    "    )\n",
    "    gloStopStopwatch(\"Load ner-xlm-Roberta\")\n",
    "\n",
    "    gloStartStopwatch(\"Load ner-Bert\")\n",
    "    dictPipelines[\"ner-bert\"] = pipeline(\n",
    "        'ner', \n",
    "        model='fhswf/bert_de_ner',\n",
    "        tokenizer='fhswf/bert_de_ner'\n",
    "    )\n",
    "    gloStopStopwatch(\"Load ner-Bert\")\n",
    "\n",
    "    gloStartStopwatch(\"Load sen-Bert\")\n",
    "    dictPipelines[\"sen-bert\"] = pipeline(\n",
    "        'sentiment-analysis', \n",
    "        model='nlptown/bert-base-multilingual-uncased-sentiment',\n",
    "        tokenizer='nlptown/bert-base-multilingual-uncased-sentiment'\n",
    "    )\n",
    "    gloStopStopwatch(\"Load sen-Bert\")\n",
    "\n",
    "    gloStartStopwatch(\"Load text-gen-gpt2\")\n",
    "    dictPipelines[\"text-gen-gpt2\"] = pipeline(\n",
    "        'text-generation', \n",
    "        model='dbmdz/german-gpt2',\n",
    "        tokenizer='dbmdz/german-gpt2'\n",
    "    )\n",
    "    gloStopStopwatch(\"Load text-gen-gpt2\")\n",
    "\n",
    "    gloStartStopwatch(\"Load text-gen-gpt2-faust\")\n",
    "    dictPipelines[\"text-gen-gpt2-faust\"] = pipeline(\n",
    "        'text-generation', \n",
    "        model='dbmdz/german-gpt2-faust',\n",
    "        tokenizer='dbmdz/german-gpt2-faust'\n",
    "    )\n",
    "    gloStopStopwatch(\"Load text-gen-gpt2-faust\")\n",
    "\n",
    "\n",
    "    gloStopStopwatch(\"Load Pipelines\")\n",
    "\n",
    "    return dictPipelines.keys()\n",
    "\n",
    "pipelineKeys = loadPipelines()\n",
    "\n",
    "print()\n",
    "print(str(pipelineKeys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process input jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gloStartStopwatch(\"Global notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read jobs from file\n",
    "dfInputFiles = pd.read_csv(dir_var + \"inputFiles.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFilter = pd.DataFrame()\n",
    "\n",
    "for dS in C_LOAD_DATASET:\n",
    "    dfFilter = dfFilter.append(dfInputFiles[dfInputFiles.inputDesc == dS])\n",
    "\n",
    "dfInputFiles = dfFilter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview input jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfInputFiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform data into DataFrmaes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame Meta (Chat Meta)\n",
    "def convertToDataFrameMeta(filePath):\n",
    "    dF = pd.read_json(dir_var + \"data/\" + filePath + \"/result.json\", encoding='utf-8')\n",
    "    return dF\n",
    "\n",
    "# Convert to DataFrame Messages (Chat Messages)\n",
    "def convertToDataFrameMessages(filePath):\n",
    "    dF = pd.json_normalize(dictMeta[filePath].messages)\n",
    "    return dF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/6718633/python-regular-expression-again-match-url\n",
    "def getUrlRegex():\n",
    "    return \"((?:https?://)?(?:(?:www\\.)?(?:[\\da-z\\.-]+)\\.(?:[a-z]{2,6})|(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)|(?:(?:[0-9a-fA-F]{1,4}:){7,7}[0-9a-fA-F]{1,4}|(?:[0-9a-fA-F]{1,4}:){1,7}:|(?:[0-9a-fA-F]{1,4}:){1,6}:[0-9a-fA-F]{1,4}|(?:[0-9a-fA-F]{1,4}:){1,5}(?::[0-9a-fA-F]{1,4}){1,2}|(?:[0-9a-fA-F]{1,4}:){1,4}(?::[0-9a-fA-F]{1,4}){1,3}|(?:[0-9a-fA-F]{1,4}:){1,3}(?::[0-9a-fA-F]{1,4}){1,4}|(?:[0-9a-fA-F]{1,4}:){1,2}(?::[0-9a-fA-F]{1,4}){1,5}|[0-9a-fA-F]{1,4}:(?:(?::[0-9a-fA-F]{1,4}){1,6})|:(?:(?::[0-9a-fA-F]{1,4}){1,7}|:)|fe80:(?::[0-9a-fA-F]{0,4}){0,4}%[0-9a-zA-Z]{1,}|::(?:ffff(?::0{1,4}){0,1}:){0,1}(?:(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])|(?:[0-9a-fA-F]{1,4}:){1,4}:(?:(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])))(?::[0-9]{1,4}|[1-5][0-9]{4}|6[0-4][0-9]{3}|65[0-4][0-9]{2}|655[0-2][0-9]|6553[0-5])?(?:/[\\w\\.-]*)*/?)\"\n",
    "\n",
    "def getHashtagRegex():\n",
    "    return \"#(\\w+)\"\n",
    "\n",
    "def hashTagExtractHashTags(inputText):\n",
    "\n",
    "    inputText = str(inputText)\n",
    "\n",
    "    inputText = re.sub('\\n', ' ', inputText) # Replace \\n\n",
    "    inputText = demoji.replace(inputText, \" \") # Rm emoji\n",
    "    inputText = gloReplaceGermanChars(inputText) # Replace german chars\n",
    "\n",
    "    return re.findall(getHashtagRegex(), inputText)\n",
    "\n",
    "def urlExtractUrls(inputText):\n",
    "    return re.findall(getUrlRegex(), str(inputText))\n",
    "\n",
    "def urlRemoveUrls(inputText):\n",
    "    return re.sub(getUrlRegex(), \" \", str(inputText))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get params from extractedTextData\n",
    "See cell below (key)\n",
    "\"\"\"\n",
    "def getExtractedTextDataParam(key, extractedTextData):\n",
    "\n",
    "    a,b,c,d,e,f,g = extractedTextData\n",
    "\n",
    "    if(key == 0):\n",
    "\n",
    "        return urlRemoveUrls(a)\n",
    "\n",
    "    elif(key == 1):\n",
    "\n",
    "        before = b\n",
    "        extracted = urlExtractUrls(a)\n",
    "\n",
    "        after = before\n",
    "        after.extend(extracted)\n",
    "\n",
    "        \"\"\"\n",
    "        if(str(extracted) != \"[]\"):\n",
    "            # TODO: Fix return bug\n",
    "            print(\"Debug >>\" + str(before) + \"/\" + str(extracted) + \">>\" + str(after) + \"<<\")\n",
    "        \"\"\"\n",
    "\n",
    "        return after\n",
    "\n",
    "    elif(key == 2):\n",
    "\n",
    "        # TODO: Refactor dont take it from extractedTextData\n",
    "        return hashTagExtractHashTags(a)\n",
    "\n",
    "    else:\n",
    "        switcher = {\n",
    "            3: d,\n",
    "            4: e,\n",
    "            5: f,\n",
    "            6: g\n",
    "        }\n",
    "        return switcher.get(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract text data (see cell above key)\n",
    "See cell above (key)\n",
    "\n",
    "param   procIsJsonFormatted Boolean (is text json formatted?)\n",
    "param   text                String  (text from message) \n",
    "\n",
    "return\n",
    "a   procText            Plain Text\n",
    "b   processedURLs       Array of URLs in Text\n",
    "c   processedHashtags   Array of Hashtags in Text #TODO: RM\n",
    "d   processedBolds      Array of Bold Items in Text\n",
    "e   processedItalics    Array of Italic Items in Text\n",
    "f   processedUnderlines Array of Underlined Items in Text\n",
    "g   processedEmails     Array of E-Mails in Text\n",
    "\"\"\"\n",
    "def extractTextData(procIsJsonFormatted, text):\n",
    "    \n",
    "    # 3 returns in this function...\n",
    "    \n",
    "    processedURLs       = list()\n",
    "    processedHashtags   = list() # TODO: RM\n",
    "    processedBolds      = list()\n",
    "    processedItalics    = list()\n",
    "    processedUnderlines = list()\n",
    "    processedEmails     = list()\n",
    "    \n",
    "    if(procIsJsonFormatted != True):\n",
    "        #Is not JSON formatted (return normal text)\n",
    "        return (text, processedURLs, processedHashtags, processedBolds, processedItalics, processedUnderlines, processedEmails)\n",
    "    else:\n",
    "        #Is is JSON formatted (try to parse)\n",
    "        try:\n",
    "            returnList = []\n",
    "            jsonList = demjson.decode(str(text), encoding='utf8')\n",
    "\n",
    "            # Do for each item in list\n",
    "            for lItem in jsonList:\n",
    "\n",
    "                messageString = str(lItem)\n",
    "\n",
    "                isJsonSubString = gloCheckIsTextJsonFormatted(messageString, singleMode = True)\n",
    "\n",
    "                if(isJsonSubString):\n",
    "                    # Is Json Sub String\n",
    "                    subJsonString = demjson.decode(str(messageString), encoding='utf8')\n",
    "                    subJsonType = subJsonString[\"type\"]\n",
    "\n",
    "                    if(subJsonType == \"bold\"):\n",
    "                        #text included\n",
    "                        processedBolds.append(subJsonString[\"text\"])\n",
    "                        returnList.append(subJsonString[\"text\"])\n",
    "                        \n",
    "                    elif(subJsonType == \"italic\"):\n",
    "                        #text included\n",
    "                        processedItalics.append(subJsonString[\"text\"])\n",
    "                        returnList.append(subJsonString[\"text\"])\n",
    "                        \n",
    "                    elif(subJsonType == \"underline\"):\n",
    "                        #text included\n",
    "                        processedUnderlines.append(subJsonString[\"text\"])\n",
    "                        returnList.append(subJsonString[\"text\"])\n",
    "                    \n",
    "                    elif(subJsonType == \"email\"):\n",
    "                        #text included\n",
    "                        processedEmails.append(subJsonString[\"text\"])\n",
    "                        \n",
    "                    elif(subJsonType == \"text_link\"):\n",
    "                        #text and href included\n",
    "                        processedURLs.append(subJsonString[\"href\"])\n",
    "                        #returnList.append(subJsonString[\"text\"])\n",
    "                        \n",
    "                    elif(subJsonType == \"link\"):\n",
    "                        #text included\n",
    "                        processedURLs.append(subJsonString[\"text\"])\n",
    "                        \n",
    "                    elif(subJsonType == \"hashtag\"):\n",
    "                        #text included\n",
    "                        #processedHashtags.append(subJsonString[\"text\"]) # TODO: Refactor: Dont add hashtags here!\n",
    "                        returnList.append(subJsonString[\"text\"])\n",
    "                        \n",
    "                    elif(subJsonType == \"mention\"):\n",
    "                        #text included\n",
    "                        returnList.append(subJsonString[\"text\"])\n",
    "                        \n",
    "                    elif(subJsonType == \"mention_name\"):\n",
    "                        #text and user_id included\n",
    "                        returnList.append(subJsonString[\"text\"])\n",
    "                        \n",
    "                    elif(subJsonType == \"bot_command\"):\n",
    "                        #text included\n",
    "                        returnList = returnList \n",
    "                        \n",
    "                    elif(subJsonType == \"code\"):\n",
    "                        #text included\n",
    "                        returnList = returnList\n",
    "                        \n",
    "                    elif(subJsonType == \"phone\"):\n",
    "                        #text included\n",
    "                        returnList = returnList\n",
    "                        \n",
    "                    elif(subJsonType == \"strikethrough\"):\n",
    "                        #text included\n",
    "                        returnList.append(subJsonString[\"text\"])\n",
    "                        \n",
    "                    elif(subJsonType == \"pre\"):\n",
    "                        #text and language included\n",
    "                        returnList.append(subJsonString[\"text\"])\n",
    "                        \n",
    "                    elif(subJsonType == \"bank_card\"):\n",
    "                        #text included\n",
    "                        returnList = returnList\n",
    "                        \n",
    "                    else:\n",
    "                        print(\"- Error: Unkown json type >>\" + str(subJsonType) + \"<< (ignore) >>\" + str(text) + \"<<\")\n",
    "\n",
    "                else:\n",
    "                    # Is no json formatted sub string (append text)\n",
    "                    returnList.append(messageString)\n",
    "\n",
    "            return (''.join(returnList), processedURLs, processedHashtags, processedBolds, processedItalics, processedUnderlines, processedEmails)\n",
    "        \n",
    "        except:\n",
    "            # Parser error (set inputText to returnText)\n",
    "            print(\"- Warn: Json parser error (set inputText to returnText) >>\" + str(text) + \"<<\")\n",
    "            return (text, processedURLs, processedHashtags, processedBolds, processedItalics, processedUnderlines, processedEmails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns dict (empty dict if disabled, dict with not entries if error)\n",
    "listUnknownTypes = list()\n",
    "def processNerPipeline(inputText, pipelineKey, configMinScore):\n",
    "    if(pipelineKey in pipelineKeys):\n",
    "\n",
    "        listPer     = list()\n",
    "        listMisc    = list()\n",
    "        listOrg     = list()\n",
    "        listLoc     = list()\n",
    "\n",
    "\n",
    "        try:\n",
    "\n",
    "            data = dictPipelines[pipelineKey](inputText)\n",
    "\n",
    "            for d in data:\n",
    "\n",
    "                jsonData = demjson.decode(str(d), encoding='utf8')\n",
    "                            \n",
    "                if(jsonData[\"score\"] >= configMinScore):\n",
    "                    # Is Valid\n",
    "                    if      (jsonData[\"entity\"] == \"I-PER\" or jsonData[\"entity\"] == \"B-PER\"):\n",
    "                        listPer.append(jsonData[\"word\"])\n",
    "                    elif    (jsonData[\"entity\"] == \"I-MISC\" or jsonData[\"entity\"] == \"B-MISC\"):\n",
    "                        listMisc.append(jsonData[\"word\"])\n",
    "                    elif    (jsonData[\"entity\"] == \"I-ORG\" or jsonData[\"entity\"] == \"B-ORG\"):\n",
    "                        listOrg.append(jsonData[\"word\"])\n",
    "                    elif    (jsonData[\"entity\"] == \"I-LOC\" or jsonData[\"entity\"] == \"B-LOC\"):\n",
    "                        listLoc.append(jsonData[\"word\"])\n",
    "                    else:\n",
    "                        uT = str(jsonData[\"entity\"])\n",
    "                        if(uT not in listUnknownTypes):\n",
    "                            print(\"- Warn - Got unknown type >>\" + uT + \"<<\")\n",
    "                            listUnknownTypes.append(uT)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "            #print(\"Error in processNerPipeline (ignore) >>\" + str(inputText) + \"<<\")\n",
    "        \n",
    "\n",
    "        return {\n",
    "            \"per\": listPer,\n",
    "            \"misc\": listMisc,\n",
    "            \"org\": listOrg,\n",
    "            \"loc\": listLoc\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        return dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns\n",
    "# 1 - 5 (1 = bad / 5 = good)\n",
    "# -1 disabled or error\n",
    "def processSenPipeline(inputText, pipelineKey, configMinScore):\n",
    "    if(pipelineKey in pipelineKeys):\n",
    "\n",
    "        sen = -1\n",
    "\n",
    "        try:\n",
    "\n",
    "            data = dictPipelines[pipelineKey](inputText)\n",
    "            \n",
    "            for d in data:\n",
    "\n",
    "\n",
    "                jsonData = demjson.decode(str(d), encoding='utf-8')\n",
    "\n",
    "                if(jsonData[\"score\"]) > configMinScore:\n",
    "                    # Is Valid\n",
    "                    labelData = str(jsonData[\"label\"])\n",
    "\n",
    "                    if(\"stars\" in labelData):\n",
    "                        labelData = re.sub(\" stars\", \"\", labelData)\n",
    "                    else:\n",
    "                        labelData = re.sub(\" star\", \"\", labelData)\n",
    "                    \n",
    "                    sen = int(labelData)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "            #print(\"Error in processSenPipeline (ignore) >>\" + str(inputText) + \"<<\")\n",
    "\n",
    "        return sen\n",
    "\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns\n",
    "# dict (polarity, subjectivity) or none (fail or disabled)\n",
    "def processSentimentAnalysisPython(inputText):\n",
    "\n",
    "    try:\n",
    "        t = TextBlob(inputText)\n",
    "        return {\n",
    "            \"polarity\": t.polarity,\n",
    "            \"subjectivity\": t.subjectivity\n",
    "        }\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalIsValidText(procTDTextLength):\n",
    "    if(procTDTextLength > 0):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalContainsSomething(att):\n",
    "    if(str(att) == \"nan\"):\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalNonEmptyList(att):\n",
    "    if(str(att) == \"[]\"):\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: https://github.com/wartaal/HanTa/blob/master/Demo.ipynb\n",
    "hanoverTagger = ht.HanoverTagger('morphmodel_ger.pgz')\n",
    "\n",
    "def getTokenFromText(inputText):\n",
    "    return nltk.word_tokenize(inputText, language=\"german\")\n",
    "\n",
    "def getLemmaAndTaggingFromText(inputText):\n",
    "    return hanoverTagger.tag_sent(getTokenFromText(inputText))\n",
    "\n",
    "# param outputFilename, if \"\" - no output\n",
    "def plotFreqNouns(inputText, outputFilename, mostCommon, flagRemoveStopwords):\n",
    "    # https://textmining.wp.hs-hannover.de/Preprocessing.html\n",
    "    nouns = []\n",
    "    sentences_tok = [nltk.tokenize.word_tokenize(sent) for sent in getTokenFromText(inputText)]\n",
    "\n",
    "    for sent in sentences_tok:\n",
    "        tags = hanoverTagger.tag_sent(sent) \n",
    "        nouns_from_sent = [lemma for (word,lemma,pos) in tags if pos == \"NN\" or pos == \"NE\"]\n",
    "        nouns.extend(nouns_from_sent)\n",
    "\n",
    "    pNouns = list()\n",
    "\n",
    "    if(flagRemoveStopwords):\n",
    "\n",
    "        print(\"- Warn: remove stopWords\")\n",
    "        stopWords = gloGetStopWordsList(filterList = list())\n",
    "        for n in nouns:\n",
    "            if n.lower() not in stopWords:\n",
    "                pNouns.append(n)\n",
    "\n",
    "    else:\n",
    "        pNouns = nouns\n",
    "\n",
    "    # Thank you https://stackoverflow.com/questions/52908305/how-to-save-a-nltk-freqdist-plot\n",
    "    fig = plt.figure(figsize = (16,9))\n",
    "    plt.gcf().subplots_adjust(bottom=0.15)\n",
    "\n",
    "    fdist = nltk.FreqDist(pNouns)    \n",
    "\n",
    "    fdist.plot(mostCommon,cumulative=False)\n",
    "\n",
    "    _ = plt.show()\n",
    "\n",
    "    if(outputFilename != \"\"):\n",
    "        fig.savefig(dir_var_output + outputFilename, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dictMeta          = {}   \n",
    "\n",
    "# Add Key = filePath / Value = DataFrame (Chat Meta)\n",
    "for fP in dfInputFiles.inputPath:\n",
    "    dictMeta[fP] = convertToDataFrameMeta(fP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return dictMessages and dfAllDataMessages\n",
    "def initProcessData():\n",
    "\n",
    "    dictMessages      = {}\n",
    "    dfAllDataMessages = pd.DataFrame()\n",
    "\n",
    "    gloStartStopwatch(\"Extract Text Data\")\n",
    "\n",
    "    # Add Key = filePath / Value = DataFrame (Chat Message)\n",
    "    for fP in dfInputFiles.inputPath:\n",
    "\n",
    "        gloStartStopwatch(\"TD-Extract \" + fP)\n",
    "        dfMessages                          = convertToDataFrameMessages(fP)\n",
    "        tmpMeta                             = convertToDataFrameMeta(fP)\n",
    "\n",
    "        # Short run\n",
    "        if(C_SHORT_RUN):\n",
    "            print(\"Short run active!\")\n",
    "            dfMessages = dfMessages.head(C_NUMBER_SAMPLES)\n",
    "\n",
    "        # Get chat attributes and check if message is json formatted\n",
    "        dfMessages[\"procChatFilePath\"]      = fP\n",
    "        dfMessages[\"procChatType\"]          = tmpMeta.type.iloc[0]\n",
    "        dfMessages[\"procIsJsonFormatted\"]   = dfMessages[\"text\"].apply(gloCheckIsTextJsonFormatted, singleMode = False)\n",
    "        \n",
    "        # Extract Text Data\n",
    "        dfMessages[\"tmpExtractedTD\"]        = dfMessages.apply(lambda x: extractTextData(x.procIsJsonFormatted, x.text), axis=1)\n",
    "\n",
    "        # Extract Text Data (params)\n",
    "        dfMessages[\"procTDText\"]            = dfMessages.apply(lambda x: getExtractedTextDataParam(0, x.tmpExtractedTD), axis=1)\n",
    "        dfMessages[\"procTDURLs\"]            = dfMessages.apply(lambda x: getExtractedTextDataParam(1, x.tmpExtractedTD), axis=1)\n",
    "        dfMessages[\"procTDHashtags\"]        = dfMessages.apply(lambda x: getExtractedTextDataParam(2, x.tmpExtractedTD), axis=1)\n",
    "        dfMessages[\"procTDBolds\"]           = dfMessages.apply(lambda x: getExtractedTextDataParam(3, x.tmpExtractedTD), axis=1)\n",
    "        dfMessages[\"procTDItalics\"]         = dfMessages.apply(lambda x: getExtractedTextDataParam(4, x.tmpExtractedTD), axis=1)\n",
    "        dfMessages[\"procTDUnderlines\"]      = dfMessages.apply(lambda x: getExtractedTextDataParam(5, x.tmpExtractedTD), axis=1)\n",
    "        dfMessages[\"procTDEmails\"]          = dfMessages.apply(lambda x: getExtractedTextDataParam(6, x.tmpExtractedTD), axis=1)\n",
    "\n",
    "        # Process text again\n",
    "        dfMessages['procTDCleanText']           = dfMessages['procTDText'].map(lambda x: re.sub('\\n', ' ', x)) # Replace \\n\n",
    "        dfMessages['procTDEmojis']              = dfMessages['procTDCleanText'].map(lambda x: demoji.findall_list(x, desc = False)) # Filter out emoji\n",
    "        dfMessages['procTDEmojisDesc']          = dfMessages['procTDCleanText'].map(lambda x: demoji.findall_list(x, desc = True)) # Filter out emoji with desc\n",
    "        dfMessages['procTDCleanText']           = dfMessages['procTDCleanText'].map(lambda x: demoji.replace(x, \" \")) # Rm emoji\n",
    "        dfMessages['procTDCleanText']           = dfMessages['procTDCleanText'].map(lambda x: gloReplaceGermanChars(x)) # Replace german chars\n",
    "        dfMessages['procTDSafeText']            = dfMessages['procTDCleanText'].map(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', ' ', x)) # Filter out . ! ? ... (get only safe chars)\n",
    "        dfMessages['procTDSafeLowercaseText']   = dfMessages['procTDSafeText'].map(lambda x: x.lower()) # To lower\n",
    "\n",
    "        # Calc text size\n",
    "        dfMessages[\"procTDTextLength\"]      = dfMessages[\"procTDCleanText\"].str.len()\n",
    "\n",
    "        # Add columns (if not exists)\n",
    "        if \"photo\" not in dfMessages:\n",
    "            print(\"- Debug: Add column >>photo<<\")\n",
    "            dfMessages[\"photo\"] = np.nan\n",
    "\n",
    "        if \"file\" not in dfMessages:\n",
    "            print(\"- Debug: Add column >>file<<\")\n",
    "            dfMessages[\"file\"] = np.nan\n",
    "\n",
    "        if \"edited\" not in dfMessages:\n",
    "            print(\"- Debug: Add column >>edited<<\")\n",
    "            dfMessages[\"edited\"] = np.nan\n",
    "\n",
    "        if \"forwarded_from\" not in dfMessages:\n",
    "            print(\"- Debug: Add column >>forwarded_from<<\")\n",
    "            dfMessages[\"forwarded_from\"] = np.nan\n",
    "\n",
    "        # Evaluate attributes\n",
    "        dfMessages[\"procEvalIsValidText\"]   = dfMessages.procTDTextLength.apply(evalIsValidText)\n",
    "\n",
    "        dfMessages[\"procEvalContainsPhoto\"] = dfMessages.photo.apply(evalContainsSomething)\n",
    "        dfMessages[\"procEvalContainsFile\"]  = dfMessages.file.apply(evalContainsSomething) \n",
    "        dfMessages[\"procEvalIsEdited\"]      = dfMessages.edited.apply(evalContainsSomething)\n",
    "        dfMessages[\"procEvalIsForwarded\"]   = dfMessages.forwarded_from.apply(evalContainsSomething)\n",
    "        \n",
    "        dfMessages[\"procEvalContainsUrl\"]              = dfMessages.procTDURLs.apply(evalNonEmptyList)\n",
    "        dfMessages[\"procEvalContainsHashtag\"]          = dfMessages.procTDHashtags.apply(evalNonEmptyList)\n",
    "        dfMessages[\"procEvalContainsBoldItem\"]         = dfMessages.procTDBolds.apply(evalNonEmptyList)\n",
    "        dfMessages[\"procEvalContainsItalicItem\"]       = dfMessages.procTDItalics.apply(evalNonEmptyList)\n",
    "        dfMessages[\"procEvalContainsUnderlineItem\"]    = dfMessages.procTDUnderlines.apply(evalNonEmptyList)\n",
    "        dfMessages[\"procEvalContainsEmailItem\"]        = dfMessages.procTDEmails.apply(evalNonEmptyList)\n",
    "        dfMessages['procEvalContainsEmojiItem']        = dfMessages.procTDEmojis.apply(evalNonEmptyList)\n",
    "\n",
    "        # Pipelines\n",
    "        if dfInputFiles[dfInputFiles.inputPath == fP].iloc[0].inputDesc in C_PIPELINE_DATASET:\n",
    "            gloStartStopwatch(\"Process pipeline ner-xlm-roberta\")\n",
    "            dfMessages['procPipeline-ner-xlm-roberta']    = dfMessages['procTDCleanText'].map(lambda x: processNerPipeline(x, \"ner-xlm-roberta\", configMinScore=0))\n",
    "            gloStopStopwatch(\"Process pipeline ner-xlm-roberta\")\n",
    "\n",
    "            gloStartStopwatch(\"Process pipeline ner-bert\")\n",
    "            dfMessages['procPipeline-ner-bert']           = dfMessages['procTDCleanText'].map(lambda x: processNerPipeline(x, \"ner-bert\", configMinScore=0))\n",
    "            gloStopStopwatch(\"Process pipeline ner-bert\")\n",
    "\n",
    "            gloStartStopwatch(\"Process pipeline sen-bert\")\n",
    "            dfMessages['procPipeline-sen-bert']           = dfMessages['procTDCleanText'].map(lambda x: processSenPipeline(x, \"sen-bert\", configMinScore=0))\n",
    "            gloStopStopwatch(\"Process pipeline sen-bert\")\n",
    "\n",
    "        # Sentiment Analysis\n",
    "        dfMessages['procPipeline-sentiment']           = dfMessages['procTDCleanText'].map(lambda x: processSentimentAnalysisPython(x))\n",
    "\n",
    "        # Add to dict    \n",
    "        dictMessages[fP] = dfMessages\n",
    "        gloStopStopwatch(\"TD-Extract \" + fP)\n",
    "\n",
    "    gloStopStopwatch(\"Extract Text Data\")\n",
    "\n",
    "    # All Messages to DataFrame\n",
    "    gloStartStopwatch(\"Generate global DataFrame\")\n",
    "    for fP in dfInputFiles.inputPath:\n",
    "        dfMessages        = dictMessages[fP].copy()\n",
    "        dfAllDataMessages = dfAllDataMessages.append(dfMessages)\n",
    "    gloStopStopwatch(\"Generate global DataFrame\")\n",
    "\n",
    "    return (dictMessages, dfAllDataMessages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return dictMessages and dfAllDataMessages\n",
    "def initCacheData(dfAllDataMessages):\n",
    "    dictMessages = {}\n",
    "    for fP in dfInputFiles.inputPath:\n",
    "        dictMessages[fP] = dfAllDataMessages[dfAllDataMessages.procChatFilePath == fP]\n",
    "    return (dictMessages, dfAllDataMessages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_USE_CACHE_FILE == \"\"):\n",
    "    print(\"Should not use cache (build new cache)\")\n",
    "    dictMessages, dfAllDataMessages = initProcessData()\n",
    "    if(C_NEW_CACHE_FILE != \"\"):\n",
    "        print(\"Write cache to file >>\" + str(C_NEW_CACHE_FILE) + \"<<\")\n",
    "        dfAllDataMessages.to_pickle(dir_var_pandas_cache + C_NEW_CACHE_FILE)\n",
    "else:\n",
    "    print(\"Should use cache (load cache)\")\n",
    "    dictMessages, dfAllDataMessages = initCacheData(pd.read_pickle(dir_var_pandas_cache + C_USE_CACHE_FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sText = \"Das ist ein Beispielstext. An diesem Text werde ich nun einige Verfahren anwenden. Ich komme aus dem Großraum München und ich mag Text.\"\n",
    "\n",
    "# Token Text\n",
    "print()\n",
    "print(\"- Token from text\")\n",
    "print(getTokenFromText(sText))\n",
    "\n",
    "# Tagging (english)\n",
    "print()\n",
    "print(\"- POS english\")\n",
    "print(nltk.pos_tag(getTokenFromText(sText)))\n",
    "\n",
    "# Lemma and tagging\n",
    "print()\n",
    "print(\"- Lemma and tagging\")\n",
    "print(getLemmaAndTaggingFromText(sText))\n",
    "print()\n",
    "\n",
    "# Freq Nouns\n",
    "print(\"- Freq nouns\")\n",
    "plotFreqNouns(sText, outputFilename = \"\", mostCommon = 10, flagRemoveStopwords = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta-Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type of channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfInputFiles.inputType.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryChatId(filePath):\n",
    "    dfMeta = dictMeta[filePath].copy()\n",
    "    return str(dfMeta[\"id\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryChatName(filePath):\n",
    "    dfMeta      = dictMeta[filePath].copy()\n",
    "    chatName    = str(dfMeta[\"name\"].iloc[0])\n",
    "    chatName    = gloConvertToSafeChatName(chatName)\n",
    "    return chatName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryChatType(filePath):\n",
    "    dfMeta = dictMeta[filePath].copy()\n",
    "    return str(dfMeta[\"type\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryNumberOfMessages(filePath):\n",
    "    dfMessages = dictMessages[filePath].copy()\n",
    "    return len(dfMessages.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryNumberOfMessagesByAttEqTrue(filePath, attKey):\n",
    "    dfMessages = dictMessages[filePath].copy()\n",
    "    dfMessages = dfMessages[dfMessages[attKey] == True]\n",
    "    return len(dfMessages.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfQueryMeta = pd.DataFrame(dfInputFiles.inputPath)\n",
    "\n",
    "dfQueryMeta[\"qryChatId\"]                        = dfQueryMeta.inputPath.apply(queryChatId)\n",
    "dfQueryMeta[\"qryChatName\"]                      = dfQueryMeta.inputPath.apply(queryChatName)\n",
    "dfQueryMeta[\"qryChatType\"]                      = dfQueryMeta.inputPath.apply(queryChatType)\n",
    "dfQueryMeta[\"qryNumberOfMessages\"]              = dfQueryMeta.inputPath.apply(queryNumberOfMessages)\n",
    "\n",
    "dfQueryMeta[\"qryNumberOfFormattedTextMessages\"] = dfQueryMeta.apply(lambda x: queryNumberOfMessagesByAttEqTrue(x.inputPath, \"procIsJsonFormatted\"), axis=1)\n",
    "\n",
    "dfQueryMeta[\"qryNumberOfValidTextMessages\"]     = dfQueryMeta.apply(lambda x: queryNumberOfMessagesByAttEqTrue(x.inputPath, \"procEvalIsValidText\"), axis=1)\n",
    "\n",
    "dfQueryMeta[\"qryNumberOfPhotos\"]                = dfQueryMeta.apply(lambda x: queryNumberOfMessagesByAttEqTrue(x.inputPath, \"procEvalContainsPhoto\"), axis=1)\n",
    "dfQueryMeta[\"qryNumberOfFiles\"]                 = dfQueryMeta.apply(lambda x: queryNumberOfMessagesByAttEqTrue(x.inputPath, \"procEvalContainsFile\"), axis=1)\n",
    "dfQueryMeta[\"qryNumberOfEditedMessages\"]        = dfQueryMeta.apply(lambda x: queryNumberOfMessagesByAttEqTrue(x.inputPath, \"procEvalIsEdited\"), axis=1)\n",
    "dfQueryMeta[\"qryNumberOfForwardedMessages\"]     = dfQueryMeta.apply(lambda x: queryNumberOfMessagesByAttEqTrue(x.inputPath, \"procEvalIsForwarded\"), axis=1)\n",
    "\n",
    "dfQueryMeta[\"qryNumberOfMessagesWithUrl\"]           = dfQueryMeta.apply(lambda x: queryNumberOfMessagesByAttEqTrue(x.inputPath, \"procEvalContainsUrl\"), axis=1)\n",
    "dfQueryMeta[\"qryNumberOfMessagesWithHashtag\"]       = dfQueryMeta.apply(lambda x: queryNumberOfMessagesByAttEqTrue(x.inputPath, \"procEvalContainsHashtag\"), axis=1)\n",
    "dfQueryMeta[\"qryNumberOfMessagesWithBold\"]          = dfQueryMeta.apply(lambda x: queryNumberOfMessagesByAttEqTrue(x.inputPath, \"procEvalContainsBoldItem\"), axis=1)\n",
    "dfQueryMeta[\"qryNumberOfMessagesWithItalic\"]        = dfQueryMeta.apply(lambda x: queryNumberOfMessagesByAttEqTrue(x.inputPath, \"procEvalContainsItalicItem\"), axis=1)\n",
    "dfQueryMeta[\"qryNumberOfMessagesWithUnderline\"]     = dfQueryMeta.apply(lambda x: queryNumberOfMessagesByAttEqTrue(x.inputPath, \"procEvalContainsUnderlineItem\"), axis=1)\n",
    "dfQueryMeta[\"qryNumberOfMessagesWithEmail\"]         = dfQueryMeta.apply(lambda x: queryNumberOfMessagesByAttEqTrue(x.inputPath, \"procEvalContainsEmailItem\"), axis=1)\n",
    "dfQueryMeta[\"qryNumberOfMessagesWithEmoji\"]         = dfQueryMeta.apply(lambda x: queryNumberOfMessagesByAttEqTrue(x.inputPath, \"procEvalContainsEmojiItem\"), axis=1)\n",
    "\n",
    "dfQueryMeta.sort_values(by=\"qryNumberOfMessages\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot meta queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto label query plot\n",
    "def autolabelAx(rects, ax):\n",
    "    \"\"\"\n",
    "    Attach a text label above each bar in *rects*, displaying its height.\n",
    "    Copied from https://matplotlib.org/3.1.1/gallery/lines_bars_and_markers/barchart.html (22.12.2020)\n",
    "    \"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param inputDescFilter set \"\" == no filter\n",
    "# param outputFilename set \"\" = no output\n",
    "def queryMetaPlotter(inputDescFilter, configPlotWidth, configPlotHeight, configBarWidth, outputFilename):\n",
    "    # Init data\n",
    "    dataLabels                          = list()\n",
    "    dataNumberOfMesssages               = list()\n",
    "    dataNumberOfFormattedTextMessages   = list()\n",
    "    dataNumberOfValidTextMessages       = list()\n",
    "    dataNumberOfEditedMessages          = list()\n",
    "    dataNumberOfForwardedMessages       = list()\n",
    "    dataNumberOfPhotos                  = list()\n",
    "    dataNumberOfFiles                   = list()\n",
    "    dataNumberOfMessagesWUrl            = list()\n",
    "    dataNumberOfMessagesWHashtag        = list()\n",
    "    dataNumberOfMessagesWBold           = list()\n",
    "    dataNumberOfMessagesWItalic         = list()\n",
    "    dataNumberOfMessagesWUnderline      = list()\n",
    "    dataNumberOfMessagesWEmail          = list()\n",
    "    dataNumberOfMessagesWEmoji          = list()\n",
    "\n",
    "    # Iterate over Meta DataFrame\n",
    "    for index, row in dfQueryMeta.sort_values(by=\"qryNumberOfMessages\", ascending=False).iterrows():\n",
    "\n",
    "        # Get attributes (check filter)\n",
    "        if(inputDescFilter == \"\" or dfInputFiles[dfInputFiles.inputPath == row.inputPath].inputDesc.iloc[0] == inputDescFilter):\n",
    "            dataLabels                          .append(row.qryChatName)\n",
    "            dataNumberOfMesssages               .append(row.qryNumberOfMessages)\n",
    "            dataNumberOfFormattedTextMessages   .append(row.qryNumberOfFormattedTextMessages)\n",
    "            dataNumberOfValidTextMessages       .append(row.qryNumberOfValidTextMessages)\n",
    "            dataNumberOfEditedMessages          .append(row.qryNumberOfEditedMessages)\n",
    "            dataNumberOfForwardedMessages       .append(row.qryNumberOfForwardedMessages)\n",
    "            dataNumberOfPhotos                  .append(row.qryNumberOfPhotos)\n",
    "            dataNumberOfFiles                   .append(row.qryNumberOfFiles)\n",
    "            dataNumberOfMessagesWUrl            .append(row.qryNumberOfMessagesWithUrl)\n",
    "            dataNumberOfMessagesWHashtag        .append(row.qryNumberOfMessagesWithHashtag)\n",
    "            dataNumberOfMessagesWBold           .append(row.qryNumberOfMessagesWithBold)\n",
    "            dataNumberOfMessagesWItalic         .append(row.qryNumberOfMessagesWithItalic)\n",
    "            dataNumberOfMessagesWUnderline      .append(row.qryNumberOfMessagesWithUnderline)\n",
    "            dataNumberOfMessagesWEmail          .append(row.qryNumberOfMessagesWithEmail)\n",
    "            dataNumberOfMessagesWEmoji          .append(row.qryNumberOfMessagesWithEmoji)\n",
    "\n",
    "    # Convert list to array\n",
    "    dataLabels                          = np.array(dataLabels)\n",
    "    dataNumberOfMesssages               = np.array(dataNumberOfMesssages)\n",
    "    dataNumberOfFormattedTextMessages   = np.array(dataNumberOfFormattedTextMessages)\n",
    "    dataNumberOfValidTextMessages       = np.array(dataNumberOfValidTextMessages)\n",
    "    dataNumberOfEditedMessages          = np.array(dataNumberOfEditedMessages)\n",
    "    dataNumberOfForwardedMessages       = np.array(dataNumberOfForwardedMessages)\n",
    "    dataNumberOfPhotos                  = np.array(dataNumberOfPhotos)\n",
    "    dataNumberOfFiles                   = np.array(dataNumberOfFiles)\n",
    "    dataNumberOfMessagesWUrl            = np.array(dataNumberOfMessagesWUrl)\n",
    "    dataNumberOfMessagesWHashtag        = np.array(dataNumberOfMessagesWHashtag)\n",
    "    dataNumberOfMessagesWBold           = np.array(dataNumberOfMessagesWBold)\n",
    "    dataNumberOfMessagesWItalic         = np.array(dataNumberOfMessagesWItalic)\n",
    "    dataNumberOfMessagesWUnderline      = np.array(dataNumberOfMessagesWUnderline)\n",
    "    dataNumberOfMessagesWEmail          = np.array(dataNumberOfMessagesWEmail)\n",
    "    dataNumberOfMessagesWEmoji          = np.array(dataNumberOfMessagesWEmoji)\n",
    "\n",
    "    # Draw\n",
    "    with sns.color_palette(\"tab10\", 11):\n",
    "        fig, ax = plt.subplots()\n",
    "    x = np.arange(len(dataLabels))\n",
    "\n",
    "    barWidth = configBarWidth\n",
    "\n",
    "    fig.set_figwidth(configPlotWidth)\n",
    "    fig.set_figheight(configPlotHeight)\n",
    "\n",
    "    r1 = x\n",
    "    r2 = [x + barWidth for x in r1]\n",
    "    r3 = [x + barWidth for x in r2]\n",
    "    r4 = [x + barWidth for x in r3]\n",
    "    r5 = [x + barWidth for x in r4]\n",
    "    r6 = [x + barWidth for x in r5]\n",
    "    r7 = [x + barWidth for x in r6]\n",
    "    r8 = [x + barWidth for x in r7]\n",
    "    r9 = [x + barWidth for x in r8]\n",
    "    r10 = [x + barWidth for x in r9]\n",
    "    r11 = [x + barWidth for x in r10]\n",
    "    r12 = [x + barWidth for x in r11]\n",
    "    r13 = [x + barWidth for x in r12]\n",
    "    r14 = [x + barWidth for x in r13]\n",
    "\n",
    "    rects1 = ax.bar(r1, dataNumberOfMesssages, barWidth, label='Messages')\n",
    "    rects2 = ax.bar(r2, dataNumberOfFormattedTextMessages, barWidth, label='Formatted Messsages')\n",
    "    rects3 = ax.bar(r3, dataNumberOfValidTextMessages, barWidth, label='Valid Text Messages')\n",
    "    rects4 = ax.bar(r4, dataNumberOfEditedMessages, barWidth, label='Edited Messages')\n",
    "    rects5 = ax.bar(r5, dataNumberOfForwardedMessages, barWidth, label='Forwarded Messages')\n",
    "    rects6 = ax.bar(r6, dataNumberOfPhotos, barWidth, label='with Photo')\n",
    "    rects7 = ax.bar(r7, dataNumberOfFiles, barWidth, label='with File')\n",
    "    rects8 = ax.bar(r8, dataNumberOfMessagesWUrl, barWidth, label='with Url')\n",
    "    rects9 = ax.bar(r9, dataNumberOfMessagesWHashtag, barWidth, label='with Hashtag')\n",
    "    rects10 = ax.bar(r10, dataNumberOfMessagesWBold, barWidth, label='with Bold Items')\n",
    "    rects11 = ax.bar(r11, dataNumberOfMessagesWItalic, barWidth, label='with Italic Items')\n",
    "    rects12 = ax.bar(r12, dataNumberOfMessagesWUnderline, barWidth, label='with Underlined Items')\n",
    "    rects13 = ax.bar(r13, dataNumberOfMessagesWEmail, barWidth, label='with E-Mails')\n",
    "    rects14 = ax.bar(r14, dataNumberOfMessagesWEmoji, barWidth, label='with Emojis')\n",
    "\n",
    "    chartTitle = \"\"\n",
    "    if(inputDescFilter != \"\"):\n",
    "        chartTitle = \" (\" + inputDescFilter + \")\"\n",
    "\n",
    "    ax.set_ylabel(\"Number of\")\n",
    "    ax.set_title(\"Meta Overview\" + chartTitle)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(dataLabels)\n",
    "    ax.legend()\n",
    "\n",
    "    rects = [rects1, rects2, rects3, rects4, rects5, rects6, rects7, rects8, rects9, rects10, rects11, rects12, rects13, rects14]\n",
    "\n",
    "    for rect in rects:\n",
    "        autolabelAx(rect, ax)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    #plt.xticks(rotation=30)\n",
    "    \n",
    "    if(outputFilename != \"\"):\n",
    "        plt.savefig(dir_var_output + outputFilename)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryMetaPlotter(\n",
    "    inputDescFilter = \"dataSet0\",\n",
    "    configPlotWidth = 32,\n",
    "    configPlotHeight = 9,\n",
    "    configBarWidth = 0.065,\n",
    "    outputFilename = \"meta-overview-dataSet0.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(\"dataSet1\" in C_LOAD_DATASET):\n",
    "    queryMetaPlotter(\n",
    "        inputDescFilter = \"dataSet1\",\n",
    "        configPlotWidth = 100,\n",
    "        configPlotHeight = 9,\n",
    "        configBarWidth = 0.065,\n",
    "        outputFilename = \"meta-overview-dataSet1.png\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(\"dataSet1a\" in C_LOAD_DATASET):\n",
    "    queryMetaPlotter(\n",
    "        inputDescFilter = \"dataSet1a\",\n",
    "        configPlotWidth = 16,\n",
    "        configPlotHeight = 9,\n",
    "        configBarWidth = 0.065,\n",
    "        outputFilename = \"meta-overview-dataSet1a.png\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(\"dataSet2\" in C_LOAD_DATASET):\n",
    "    queryMetaPlotter(\n",
    "        inputDescFilter = \"dataSet2\",\n",
    "        configPlotWidth = 34,\n",
    "        configPlotHeight = 9,\n",
    "        configBarWidth = 0.065,\n",
    "        outputFilename = \"meta-overview-dataSet2.png\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get text-length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeTextLengthOutliersFromDataFrame(df, interval, maxTextLength):\n",
    "    df = df.copy()\n",
    "    df = df[df.procTDTextLength < maxTextLength]\n",
    "    # https://stackoverflow.com/questions/23199796/detect-and-exclude-outliers-in-pandas-data-frame\n",
    "    # keep only the ones that are within <interval> to -<interval> standard deviations in the column 'Data'.\n",
    "    return df[np.abs(df.procTDTextLength-df.procTDTextLength.mean()) <= (interval*df.procTDTextLength.std())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param outputFilename set \"\" == no output file\n",
    "def textLengthHistPlotter(outputFilename):\n",
    "    dfMessages = dfAllDataMessages.copy()\n",
    "    print(\"Number of all messages:\\t\\t\\t\\t\\t\\t\" + str(len(dfMessages.index)))\n",
    "\n",
    "    dfMessages = dfMessages[dfMessages.procEvalIsValidText == True]\n",
    "    print(\"Number of valid text messages:\\t\\t\\t\\t\\t\" + str(len(dfMessages.index)))\n",
    "\n",
    "    dfMessages = removeTextLengthOutliersFromDataFrame(\n",
    "        dfMessages,\n",
    "        interval = 3,               #Default is 3\n",
    "        maxTextLength = 999999999   #TODO: Maybe enable max text length\n",
    "        )\n",
    "    print(\"Number of valid text messages (after outliers filtering):\\t\" + str(len(dfMessages.index)))\n",
    "\n",
    "    print()\n",
    "    print(\"Text Length Hist (after outliers filtering)\")\n",
    "    plt.figure(figsize=(8,4.5))\n",
    "    _ = dfMessages.procTDTextLength.hist(bins=40)\n",
    "    plt.title('Histogram Text Length')\n",
    "\n",
    "    if(outputFilename != \"\"):\n",
    "        plt.savefig(dir_var_output + outputFilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textLengthHistPlotter(outputFilename = \"meta-text-length-hist.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare ids and labels (has chat name changed?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareIdsAndLabels(df):\n",
    "\n",
    "    gloStartStopwatch(\"Compare ids and labels\")\n",
    "\n",
    "    dictFromTranslator  = {}\n",
    "    dictActorTranslator = {}\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    \n",
    "    df = df.set_index(\"date\")\n",
    "    df = df.sort_index()\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        n_from      = row[\"from\"]\n",
    "        n_from_id   = row[\"from_id\"]\n",
    "\n",
    "        n_from = str(n_from)\n",
    "        n_from_id = str(n_from_id)\n",
    "\n",
    "        n_actor      = row[\"actor\"]\n",
    "        n_actor_id   = row[\"actor_id\"]\n",
    "\n",
    "        n_actor = str(n_actor)\n",
    "        n_actor_id = str(n_actor_id)\n",
    "\n",
    "        if(str(n_from) != \"nan\"):\n",
    "            if(n_from_id not in dictFromTranslator):\n",
    "                # Add new key\n",
    "                dictFromTranslator[n_from_id] = [n_from]\n",
    "            else:\n",
    "                # Has changed?\n",
    "                oValueL = dictFromTranslator[n_from_id]\n",
    "                if(n_from not in oValueL):\n",
    "                    newList = oValueL.copy()\n",
    "                    newList.append(n_from)\n",
    "                    print(\"- Add changed attribute in from (prev=\" + str(oValueL) + \"/new=\" + str(newList) + \")\")\n",
    "                    dictFromTranslator[n_from_id] = newList\n",
    "\n",
    "        if(str(n_actor) != \"nan\"):\n",
    "            if(n_actor_id not in dictActorTranslator):\n",
    "                # Add new key\n",
    "                dictActorTranslator[n_actor_id] = [n_actor]\n",
    "            else:\n",
    "                # Has changed?\n",
    "                oValueL = dictActorTranslator[n_actor_id]\n",
    "                if(n_actor not in oValueL):\n",
    "                    newList = oValueL.copy()\n",
    "                    newList.append(n_actor)\n",
    "                    print(\"- Add changed attribute in actor (prev=\" + str(oValueL) + \"/new=\" + str(newList) + \")\")\n",
    "                    dictActorTranslator[n_actor_id] = newList\n",
    "\n",
    "    gloStopStopwatch(\"Compare ids and labels\")\n",
    "\n",
    "    return dictFromTranslator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    compareIdsAndLabels(dfAllDataMessages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Social Graph Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractImportantHashtags(df):\n",
    "    dfMessages = df.copy()\n",
    "    dfMessages = dfMessages[dfMessages.procEvalContainsHashtag == True]\n",
    "\n",
    "    hashTagList = list()\n",
    "    for index, row in dfMessages.iterrows():\n",
    "        for hashtagItem in row[\"procTDHashtags\"]:\n",
    "            hashTagList.append(hashtagItem)\n",
    "\n",
    "    return hashTagList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return combinations\n",
    "def extractImportantEmojis(df):\n",
    "\n",
    "    dfMessages = df.copy()\n",
    "    dfMessages = dfMessages[dfMessages.procEvalContainsEmojiItem == True]\n",
    "\n",
    "    li = dfMessages.procTDEmojisDesc.values.tolist()\n",
    "\n",
    "    retLi = list()\n",
    "\n",
    "    for l in li:\n",
    "        aString = \"\"\n",
    "        for e in l:\n",
    "            aString = aString + \":\" + e \n",
    "        retLi.append(aString)\n",
    "\n",
    "    return retLi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param flagResolveNewUrls  Flag (see config above)\n",
    "\n",
    "def resolveUrl(completeUrl, flagResolveNewUrls):\n",
    "    \n",
    "    if \"bit.ly\" in completeUrl:\n",
    "\n",
    "        if(gloCheckIsAlreadyCached(\"resolved-urls.csv\", completeUrl)):\n",
    "            return gloGetCached(\"resolved-urls.csv\", completeUrl)\n",
    "        else:\n",
    "\n",
    "            if(flagResolveNewUrls == False):\n",
    "                return completeUrl\n",
    "\n",
    "            print(\"(Resolve now >>\" + completeUrl + \"<<)\")\n",
    "            try:\n",
    "                r = requests.get(completeUrl, timeout = 5)\n",
    "                u = r.url\n",
    "                gloAddToCache(\"resolved-urls.csv\", completeUrl, u)\n",
    "                return u\n",
    "            except:\n",
    "                print(\"(- Warn: Can not resolve (return completeUrl))\")\n",
    "                return completeUrl\n",
    "\n",
    "    else:\n",
    "        return completeUrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return\n",
    "# a = urlList,\n",
    "# b = refList\n",
    "# c = hostList\n",
    "def extractImportantUrls(df):\n",
    "    dfMessages = df.copy()\n",
    "    dfMessages = dfMessages[dfMessages.procEvalContainsUrl == True]\n",
    "\n",
    "    hostList        = list()\n",
    "    urList          = list()\n",
    "    refList         = list()\n",
    "\n",
    "    counterSucHostname = 0\n",
    "    counterErrHostname = 0\n",
    "\n",
    "    for index, row in dfMessages.iterrows():\n",
    "        for urlItem in row[\"procTDURLs\"]:\n",
    "            \n",
    "            urlData = urlparse(str(urlItem))\n",
    "\n",
    "            completeUrl      = urlData.geturl()\n",
    "\n",
    "            rUrl     = resolveUrl(completeUrl, flagResolveNewUrls=C_RESOLVE_NEW_URLS)\n",
    "            rUrlData = urlparse(rUrl)\n",
    "            rCompleteUrl = rUrlData.geturl()\n",
    "            rCompleteHostname = rUrlData.hostname\n",
    "\n",
    "            if(str(rCompleteHostname) != \"None\"):\n",
    "                counterSucHostname = counterSucHostname + 1\n",
    "\n",
    "                hostList.append(str(rCompleteHostname))\n",
    "\n",
    "                urList.append(str(rCompleteUrl))\n",
    "\n",
    "                if \"t.me\" in str(rCompleteHostname):\n",
    "                    refList.append(str(rCompleteUrl))\n",
    "            else:\n",
    "                counterErrHostname = counterErrHostname + 1\n",
    "\n",
    "    print(\"Got Hostnames (suc=\" + str(counterSucHostname) + \"/err=\" + str(counterErrHostname) + \")\")\n",
    "\n",
    "    return (urList, refList, hostList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param flagResolveNewUrls  Flag (see config above)\n",
    "def resolveImportantYoutubeVideos(urlList, flagResolveNewUrls):\n",
    "\n",
    "    # Thanks https://gist.github.com/rodrigoborgesdeoliveira/987683cfbfcc8d800192da1e73adc486\n",
    "\n",
    "    ytList = list()\n",
    "\n",
    "    for url in urlList:\n",
    "\n",
    "        url = str(url)\n",
    "\n",
    "        if(\"youtube.com\" in url or \"youtu.be\" in url or \"youtube-nocookie.com\" in url):\n",
    "            if(gloCheckIsAlreadyCached(\"resolved-youtube.csv\", url)):\n",
    "                ytList.append(gloGetCached(\"resolved-youtube.csv\", url)) \n",
    "            else:\n",
    "\n",
    "                if(flagResolveNewUrls == False):\n",
    "                    print(\"(Disable resolve new youtube urls (return completeUrl) >>\" + url + \"<<)\")\n",
    "                    ytList.append(url)\n",
    "                else:\n",
    "                    print(\"Resolve now youtube >>\" + url + \"<<\")\n",
    "                    try:\n",
    "                        r = requests.get(url, timeout = 5)\n",
    "                        t = fromstring(r.content)\n",
    "                        a = str(t.findtext('.//title'))\n",
    "                        ytList.append(a)\n",
    "                        gloAddToCache(\"resolved-youtube.csv\", url, a)\n",
    "                    except:\n",
    "                        print(\"(- Warn: Can not resolve youtube url (return completeUrl))\")\n",
    "                        ytList.append(url)\n",
    "\n",
    "    return ytList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Bug: No Hostname detected if string startsWith ! \"http\" in urlparse\n",
    "# TODO: Check: Refs ins both directions\n",
    "\n",
    "# Returns\n",
    "# a = Counter forwardedFromList\n",
    "# b = Counter refList\n",
    "# c = Counter hashtagList\n",
    "# d = Counter hostList\n",
    "# e = Counter emojiList\n",
    "# f = Counter fromList\n",
    "def extractSocialGraph(filePath, debugPrint, debugPrintCount):\n",
    "\n",
    "    dfMessages = dictMessages[filePath].copy()\n",
    "\n",
    "    hashtagList = extractImportantHashtags(dfMessages)\n",
    "    emojiList = extractImportantEmojis(dfMessages)\n",
    "\n",
    "    urlList, refList, hostList = extractImportantUrls(dfMessages)\n",
    "\n",
    "    ytList = resolveImportantYoutubeVideos(urlList, flagResolveNewUrls = C_RESOLVE_NEW_URLS)\n",
    "            \n",
    "    forwardedFromList = list()\n",
    "    if(\"forwarded_from\" in dfMessages.columns):\n",
    "        df = dfMessages.copy()\n",
    "        df = df[df.procEvalIsForwarded == True]\n",
    "    \n",
    "        for index, row in df.iterrows():        \n",
    "            forwardedFromList.append(str(row[\"forwarded_from\"]))\n",
    "            \n",
    "    actorList = list()\n",
    "    if(\"actor\" in dfMessages.columns):\n",
    "        for index, row in dfMessages.iterrows():\n",
    "            actorList.append(str(row[\"actor\"]))\n",
    "    \n",
    "    memberList = list()\n",
    "    if(\"members\" in dfMessages.columns):\n",
    "        for index, row in dfMessages.iterrows():\n",
    "            if(str(row[\"members\"]) != \"nan\"):\n",
    "                for memberItem in row[\"members\"]:\n",
    "                    memberList.append(str(memberItem))\n",
    "                    \n",
    "    fromList = list()\n",
    "    if(\"from\" in dfMessages.columns):\n",
    "        for index, row in dfMessages.iterrows():\n",
    "            s = str(row[\"from\"])\n",
    "            s = gloConvertToSafeString(s)\n",
    "            if(s != \"None\"):\n",
    "                fromList.append(s)\n",
    "            \n",
    "    savedFromList = list()\n",
    "    if(\"saved_from\" in dfMessages.columns):\n",
    "        for index, row in dfMessages.iterrows():\n",
    "            savedFromList.append(str(row[\"saved_from\"]))\n",
    "\n",
    "    configTopN = debugPrintCount\n",
    "\n",
    "    if(debugPrint):\n",
    "\n",
    "        print()\n",
    "        print(\"Set top n to \" + str(debugPrintCount))\n",
    "        print()\n",
    "\n",
    "        print(\"- Top Hosts (resovled) -\")\n",
    "        print (\"\\n\".join(map(str, Counter(hostList).most_common(configTopN))))\n",
    "        print()\n",
    "        print(\"- Top URLs (resolved) -\")\n",
    "        print (\"\\n\".join(map(str, Counter(urlList).most_common(configTopN))))\n",
    "        print()\n",
    "        print(\"- Top Refs from text (resolved) -\")\n",
    "        print (\"\\n\".join(map(str, Counter(refList).most_common(configTopN))))\n",
    "        print()\n",
    "        print(\"- Top Refs (forwarded_from) -\")\n",
    "        print (\"\\n\".join(map(str, Counter(forwardedFromList).most_common(configTopN))))\n",
    "        print()\n",
    "        print(\"- Top Refs (actor) -\")\n",
    "        print (\"\\n\".join(map(str, Counter(actorList).most_common(configTopN))))\n",
    "        print()\n",
    "        print(\"- Top Refs (members) -\")\n",
    "        print (\"\\n\".join(map(str, Counter(memberList).most_common(configTopN))))\n",
    "        print()\n",
    "        print(\"- Top Refs (from) -\")\n",
    "        print (\"\\n\".join(map(str, Counter(fromList).most_common(configTopN))))\n",
    "        print()\n",
    "        print(\"- Top Refs (saved_from) -\")\n",
    "        print (\"\\n\".join(map(str, Counter(savedFromList).most_common(configTopN))))\n",
    "        print()\n",
    "        print(\"- Top hashtags -\")\n",
    "        print (\"\\n\".join(map(str, Counter(hashtagList).most_common(configTopN))))\n",
    "        print()\n",
    "        print(\"- Top emojis -\")\n",
    "        print (\"\\n\".join(map(str, Counter(emojiList).most_common(configTopN))))\n",
    "        print()\n",
    "        print(\"- Top yt (resolved) -\")\n",
    "        print (\"\\n\".join(map(str, Counter(ytList).most_common(configTopN))))\n",
    "        print()\n",
    "    \n",
    "    return (Counter(forwardedFromList), Counter(refList), Counter(hashtagList),  Counter(hostList), Counter(emojiList), Counter(fromList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printSocialGraphDebug(filePathList):\n",
    "    for fP in filePathList:\n",
    "        print(\"Analyse now >>\" + fP + \"<<\")\n",
    "        _ = extractSocialGraph(fP, debugPrint=True, debugPrintCount=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    printSocialGraphDebug(dfInputFiles[dfInputFiles.inputDesc == \"dataSet0\"].inputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False and False): # TODO: Enable - Disable (read)\n",
    "    printSocialGraphDebug(dfInputFiles[dfInputFiles.inputDesc == \"dataSet1a\"].inputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False) and False: # TODO: Enable - Disable (read)\n",
    "    printSocialGraphDebug(dfInputFiles[dfInputFiles.inputDesc == \"dataSet2\"].inputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dictSGD_ForwardedFrom = {}\n",
    "dictSGD_Ref           = {}\n",
    "dictSGD_Hashtag       = {}\n",
    "dictSGD_Host          = {}\n",
    "dictSGD_Emoji         = {}\n",
    "dictSGD_From          = {}\n",
    "\n",
    "gloStartStopwatch(\"Extract Social Graph Data\")\n",
    "\n",
    "for fP in dfInputFiles.inputPath:\n",
    "\n",
    "    gloStartStopwatch(\"Extract Social Graph Data >>\" + fP + \"<<\")\n",
    "\n",
    "    a, b, c, d, e, f = extractSocialGraph(fP, debugPrint=False, debugPrintCount = 0)\n",
    "\n",
    "    dictSGD_ForwardedFrom[fP]   = a\n",
    "    dictSGD_Ref[fP]             = b\n",
    "    dictSGD_Hashtag[fP]         = c\n",
    "    dictSGD_Host[fP]            = d\n",
    "    dictSGD_Emoji[fP]           = e\n",
    "    dictSGD_From[fP]            = f\n",
    "\n",
    "    gloStopStopwatch(\"Extract Social Graph Data >>\" + fP + \"<<\")\n",
    "\n",
    "gloStopStopwatch(\"Extract Social Graph Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Top Influencer (Downloaded?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Top Influencer\n",
    "# param fPList      filePath List\n",
    "# param configTopN  Get Top n influencer e.g. 10\n",
    "def getTopInfluencer(fPList, configTopN):\n",
    "\n",
    "    for fP in fPList:\n",
    "\n",
    "        chatName = queryChatName(fP)\n",
    "\n",
    "        print()\n",
    "        print(\"Analyse Chat (Forwarded From) >>\" + chatName + \"<<\")\n",
    "        \n",
    "        socialGraphData = dictSGD_ForwardedFrom[fP]\n",
    "        socialGraphData = socialGraphData.most_common(configTopN)\n",
    "\n",
    "        counter = 1\n",
    "\n",
    "        # Iterate over data\n",
    "        for oChatName, oChatRefs in socialGraphData:\n",
    "            \n",
    "            # Query other params\n",
    "            oChatName    = gloConvertToSafeChatName(str(oChatName))\n",
    "            oChatRefs    = oChatRefs\n",
    "\n",
    "            # Already downloaded?\n",
    "            flagDownloaded = False\n",
    "            if oChatName in dfQueryMeta.qryChatName.values:\n",
    "                flagDownloaded = True\n",
    "\n",
    "            if(oChatName != \"nan\"):\n",
    "\n",
    "                print(str(counter) + \": (downloaded=\" + str(flagDownloaded) + \") (refs=\" + str(oChatRefs) + \")\\t\\t>>\" + str(oChatName) + \"<<\")\n",
    "                counter = counter + 1\n",
    "\n",
    "\n",
    "        print()\n",
    "        print(\"Analyse Chat (Refs) >>\" + chatName + \"<<\")\n",
    "        \n",
    "        socialGraphData = dictSGD_Ref[fP]\n",
    "        socialGraphData = socialGraphData.most_common(configTopN)\n",
    "\n",
    "        counter = 1\n",
    "\n",
    "        # Iterate over data\n",
    "        for oChatName, oChatRefs in socialGraphData:\n",
    "            \n",
    "            # Query other params\n",
    "            oChatName    = str(oChatName)\n",
    "            oChatRefs    = oChatRefs\n",
    "\n",
    "            if(oChatName != \"nan\"):\n",
    "\n",
    "                print(str(counter) + \" (refs=\" + str(oChatRefs) + \")\\t\\t>>\" + str(oChatName) + \"<<\")\n",
    "                counter = counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Can not get all items in dataSet1\n",
    "\n",
    "\"\"\"\n",
    "# Attila Hildmann #\n",
    "- Anonymous Germany - not found\n",
    "- https://t.me/DEMOKRATENCHAT - no entries\n",
    "- https://t.me/ChatDerFreiheit - no entries\n",
    "- https://t.me/FREIHEITSCHAT2020 - not found\n",
    "\n",
    "# Oliver Janich #\n",
    "- Oliver Janich Premium - not found\n",
    "\n",
    "# Xavier Naidoo #\n",
    "- Xavier(Der VereiNiger)Naidoo😎 - not found\n",
    "- https://t.me/PostAppender_bot - bot chat\n",
    "\"\"\"\n",
    "getTopInfluencer(list(dfInputFiles[dfInputFiles.inputDesc == \"dataSet0\"].inputPath), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Social Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Social Graph Layout Selector\n",
    "\n",
    "param G Graph\n",
    "param layoutSelector:\n",
    "\n",
    "1 = Kamda Kawai Layout\n",
    "2 = Spring Layout\n",
    "3 = Graphviz Layout\n",
    "\"\"\"\n",
    "def getSocialGraphLayout(layoutSelector, G):\n",
    "    if(layoutSelector == 1):\n",
    "        return nx.kamada_kawai_layout(G.to_undirected())\n",
    "    elif(layoutSelector == 2):\n",
    "        return nx.spring_layout(G.to_undirected(), k = 0.15, iterations=200)\n",
    "    elif(layoutSelector == 3):\n",
    "        return nx.nx_pydot.graphviz_layout(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try different arrows (see below): https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.patches.ArrowStyle.html\n",
    "# TODO: Check distances between nodes\n",
    "\n",
    "\"\"\"\n",
    "Draw social grah\n",
    "\n",
    "param   G                           graph\n",
    "param   layoutSelector              see above\n",
    "param   configFactorEdge            e.g. 100 => weight / 100\n",
    "param   configFactorNode            e.g. 10  => weight / 10\n",
    "param   configArrowSize             e.g. 5\n",
    "param   configPlotWidth             e.g. 16\n",
    "param   configPlotHeight            e.g. 9\n",
    "param   outputFilename              e.g. test.png (set \"\" == no output file)\n",
    "param   outputTitle                 e.g. Graph (required)\n",
    "\"\"\"\n",
    "def drawSocialGraph(G, layoutSelector, configFactorEdge, configFactorNode, configArrowSize, configPlotWidth, configPlotHeight, outputFilename, outputTitle):\n",
    "    \n",
    "    gloStartStopwatch(\"Social Graph Plot\")\n",
    "    \n",
    "    plt.figure(figsize=(configPlotWidth,configPlotHeight))\n",
    "        \n",
    "    pos = getSocialGraphLayout(layoutSelector = layoutSelector, G = G)\n",
    "    \n",
    "    # Clean edges\n",
    "    edges       = nx.get_edge_attributes(G, \"weight\")\n",
    "    edgesTLabel = nx.get_edge_attributes(G, \"tLabel\")\n",
    "\n",
    "    clean_edges         = dict()\n",
    "    clean_edges_labels  = dict()\n",
    "    \n",
    "    for key in edges:\n",
    "        \n",
    "        #Set edge weight\n",
    "        clean_edges[key]        = (100 - edges[key]) / configFactorEdge\n",
    "\n",
    "        #set edge layout\n",
    "        clean_edges_labels[key] = edgesTLabel[key]\n",
    "    \n",
    "    # Clean nodes\n",
    "    nodes       = nx.get_node_attributes(G,'weight')\n",
    "    nodesTLabel = nx.get_node_attributes(G,'tLabel')\n",
    "    nodesTColor = nx.get_node_attributes(G,'tColor')\n",
    "\n",
    "    clean_nodes         = dict()\n",
    "    clean_nodes_labels  = dict()\n",
    "    clean_nodes_color   = dict()\n",
    "    \n",
    "    for key in nodes:\n",
    "        \n",
    "        #Set node weight        \n",
    "        clean_nodes[key]        = nodes[key] / configFactorNode\n",
    "\n",
    "        #Set node layout\n",
    "        clean_nodes_labels[key] = nodesTLabel[key]\n",
    "        clean_nodes_color[key]  = nodesTColor[key]\n",
    "    \n",
    "    # Revert DiGraph (arrows direction)\n",
    "    G_rev = nx.DiGraph.reverse(G)    \n",
    "\n",
    "    # Draw\n",
    "    nx.draw(G_rev,\n",
    "        pos,\n",
    "        with_labels=True,\n",
    "        width=list(clean_edges.values()),\n",
    "        node_size=list(clean_nodes.values()),\n",
    "        labels=clean_nodes_labels,\n",
    "        node_color=list(clean_nodes_color.values()),\n",
    "        arrowsize=configArrowSize,\n",
    "        arrowstyle=\"wedge\"\n",
    "        #connectionstyle=\"arc3, rad = 0.1\"\n",
    "    )\n",
    "    \n",
    "    # Set labels\n",
    "    _ = nx.draw_networkx_edge_labels(G_rev, pos, edge_labels=clean_edges_labels)\n",
    "\n",
    "    plt.title(outputTitle)\n",
    "\n",
    "    # Save and show fig\n",
    "    if(outputFilename != \"\"):\n",
    "        plt.savefig(dir_var_output + outputFilename)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    gloStopStopwatch(\"Social Graph Plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates Test Graph\n",
    "def generateTestGraph():\n",
    "\n",
    "    G_weighted = nx.DiGraph()\n",
    "\n",
    "    G_weighted.add_edge(\"N1\", \"N2\", weight=100-30,  tLabel = \"(≙\" + str(100-30) + \")\")\n",
    "    G_weighted.add_edge(\"N1\", \"N3\", weight=100-10,  tLabel = \"(≙\" + str(100-10) + \")\")\n",
    "    G_weighted.add_edge(\"N1\", \"N4\", weight=100-60,  tLabel = \"(≙\" + str(100-60) + \")\")\n",
    "\n",
    "    G_weighted.add_edge(\"N4\", \"N5\", weight=100-80,  tLabel = \"(≙\" + str(100-80) + \")\")\n",
    "    G_weighted.add_edge(\"N4\", \"N6\", weight=100-10,  tLabel = \"(≙\" + str(100-10) + \")\")\n",
    "\n",
    "    G_weighted.add_edge(\"N4\", \"N7\", weight=100-30,   tLabel = \"(≙\" + str(100-30) + \")\")\n",
    "    G_weighted.add_edge(\"N7\", \"N4\", weight=100-70,   tLabel = \"(≙\" + str(100-70) + \")\")\n",
    "\n",
    "    G_weighted.add_node(\"N1\", weight=500.0, tLabel = \"N1-T\", tColor=\"red\")\n",
    "    G_weighted.add_node(\"N2\", weight=500.0, tLabel = \"N2-T\", tColor=\"blue\")\n",
    "    G_weighted.add_node(\"N3\", weight=500.0, tLabel = \"N3-T\", tColor=\"blue\")\n",
    "    G_weighted.add_node(\"N4\", weight=500.0, tLabel = \"N4-T\", tColor=\"red\")\n",
    "    G_weighted.add_node(\"N5\", weight=500.0, tLabel = \"N5-T\", tColor=\"red\")\n",
    "    G_weighted.add_node(\"N6\", weight=500.0, tLabel = \"N6-T\", tColor=\"red\")\n",
    "    G_weighted.add_node(\"N7\", weight=500.0, tLabel = \"N7-T\", tColor=\"blue\")\n",
    "\n",
    "    return G_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add node weight to dict\n",
    "# Only adds new weight if newWeight > oldWeight\n",
    "def addSocialGraphNodeWeight(chatName, chatWeight, targetDict):\n",
    "    \n",
    "    if(chatName in targetDict):\n",
    "        oldWeight = targetDict[chatName]\n",
    "        if(chatWeight > oldWeight):\n",
    "            targetDict[chatName] = chatWeight\n",
    "    else:\n",
    "        targetDict[chatName] = chatWeight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate social graph\n",
    "\n",
    "param   configTopNInfluencer        e.g. For top 10 = 10\n",
    "param   configMinRefs               e.g. 1 must have > 1 % forwarded messages\n",
    "param   listFilePaths               List process filePaths\n",
    "param   socialGraphTargetDict       e.g. forwarded from dict or hashtag dict\n",
    "param   socialGraphTargetAttribute  e.g. procEvalIsForwarded (for calc percent)\n",
    "param   configFlagDebugLabel        e.g. show debug info on label\n",
    "\"\"\"\n",
    "def generateSocialGraph(configTopNInfluencer, configMinRefs, listFilePaths, socialGraphTargetDict, socialGraphTargetAttribute, configFlagDebugLabel):\n",
    "    \n",
    "    # Save node weights to dict\n",
    "    dictSocialNodeWeights   = dict()\n",
    "\n",
    "    # Flag downloaded nodes (exact node weight)\n",
    "    dictExactNodesLabels    = {}\n",
    "    \n",
    "    gloStartStopwatch(\"Social Graph\")\n",
    "    \n",
    "    # Generate directed graph\n",
    "    G_weighted = nx.DiGraph()\n",
    "    \n",
    "    print(\"- Add edges\")\n",
    "    for fP in listFilePaths:\n",
    "        \n",
    "        # Query own params\n",
    "        chatName                        = queryChatName(fP)\n",
    "        chatNumberOfMessages            = queryNumberOfMessages(fP)\n",
    "        chatNumberOfTargetMessages      = queryNumberOfMessagesByAttEqTrue(fP, socialGraphTargetAttribute)\n",
    "\n",
    "        gloStartStopwatch(\"SG-Extract \" + chatName + \"(\" + str(chatNumberOfTargetMessages) + \"/\" + str(chatNumberOfMessages) + \" messages)\")\n",
    "        \n",
    "        # Add exact node size (chat downloaded) and flag node\n",
    "        addSocialGraphNodeWeight(chatName, chatNumberOfMessages, dictSocialNodeWeights)\n",
    "        dictExactNodesLabels[chatName] = str(chatName) + \"\\n=[\" + str(chatNumberOfTargetMessages) + \"/\" + str(chatNumberOfMessages) + \"]\"\n",
    "\n",
    "        # Extract social graph data and get top influencer\n",
    "        socialGraphData = socialGraphTargetDict[fP]\n",
    "        socialGraphData = socialGraphData.most_common(configTopNInfluencer)\n",
    "        \n",
    "        # Iterate over forwarder\n",
    "        for oChatName, oChatRefs in socialGraphData:\n",
    "            \n",
    "            # Query other params\n",
    "            oChatName    = gloConvertToSafeChatName(str(oChatName))\n",
    "            oChatRefs    = oChatRefs\n",
    "\n",
    "            # If has forwarder\n",
    "            if(oChatName != \"nan\"):\n",
    "        \n",
    "                # Calc percent (forwarded_messages)\n",
    "                per = (oChatRefs/chatNumberOfTargetMessages) * 100\n",
    "\n",
    "                # Filter unimportant forwarders\n",
    "                if(per > configMinRefs):\n",
    "                \n",
    "                    # Add estimanted node size (chat not downloaded)\n",
    "                    addSocialGraphNodeWeight(oChatName, oChatRefs, dictSocialNodeWeights)\n",
    "\n",
    "                    # Invert percent (distance)\n",
    "                    wei = 100 - per\n",
    "\n",
    "                    # Label\n",
    "                    if(configFlagDebugLabel):\n",
    "                        lab = str(round(per, 3)) + \"% (\" + str(oChatRefs) + \"/\" + str(chatNumberOfTargetMessages) + \"≙\" + str(round(wei, 3)) + \")\"\n",
    "                    else:\n",
    "                        lab = str(round(per, 3)) + \"% (\" + str(oChatRefs) + \"/\" + str(chatNumberOfTargetMessages) + \")\"\n",
    "\n",
    "                    # Add edge\n",
    "                    G_weighted.add_edge(\n",
    "                        chatName,\n",
    "                        oChatName,\n",
    "                        weight=wei,\n",
    "                        tLabel = lab\n",
    "                    )\n",
    "\n",
    "        gloStopStopwatch(\"SG-Extract \" + chatName + \"(\" + str(chatNumberOfTargetMessages) + \"/\" + str(chatNumberOfMessages) + \" messages)\")\n",
    "        \n",
    "    print(\"- Add different nodes\")\n",
    "    for aNode in dictSocialNodeWeights:\n",
    "        \n",
    "        # Query node params\n",
    "        nodeName   = str(aNode)\n",
    "        nodeWeight = dictSocialNodeWeights[aNode]\n",
    "\n",
    "        # Set defaults\n",
    "        tValueColor = \"#ff8000\"\n",
    "        tLabel = str(nodeName) + \"\\n≈[\" + str(nodeWeight) + \"]\"\n",
    "\n",
    "        # Overwrite (if chat downloaded = exact weight)\n",
    "        if(nodeName in dictExactNodesLabels):\n",
    "            tValueColor = \"#0080ff\"\n",
    "            tLabel = dictExactNodesLabels[nodeName]\n",
    "        \n",
    "        G_weighted.add_node(\n",
    "            nodeName,\n",
    "            weight=nodeWeight,\n",
    "            tLabel = tLabel,\n",
    "            tColor=tValueColor\n",
    "        )\n",
    "        \n",
    "    gloStopStopwatch(\"Social Graph\")\n",
    "        \n",
    "    return G_weighted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generatedTestGraph = generateTestGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawSocialGraph(\n",
    "    G = generatedTestGraph,\n",
    "    layoutSelector=1,\n",
    "    configFactorEdge = 10,\n",
    "    configFactorNode = 1,\n",
    "    configArrowSize = 15,\n",
    "    configPlotWidth = 8,\n",
    "    configPlotHeight = 4.5,\n",
    "    outputFilename = \"\",\n",
    "    outputTitle = \"Test Graph Kamda Kawai Layout\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawSocialGraph(\n",
    "    G = generatedTestGraph,\n",
    "    layoutSelector=2,\n",
    "    configFactorEdge = 10,\n",
    "    configFactorNode = 1,\n",
    "    configArrowSize = 15,\n",
    "    configPlotWidth = 8,\n",
    "    configPlotHeight = 4.5,\n",
    "    outputFilename = \"\",\n",
    "    outputTitle = \"Test Graph Spring Layout\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawSocialGraph(\n",
    "    G = generatedTestGraph,\n",
    "    layoutSelector=3,\n",
    "    configFactorEdge = 10,\n",
    "    configFactorNode = 1,\n",
    "    configArrowSize = 15,\n",
    "    configPlotWidth = 8,\n",
    "    configPlotHeight = 4.5,\n",
    "    outputFilename = \"\",\n",
    "    outputTitle = \"Test Graph Graphviz Layout\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real Graph (ForwardedFrom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "drawSocialGraph(\n",
    "    generateSocialGraph(\n",
    "        configTopNInfluencer = 100,  \n",
    "        configMinRefs = 0.5,       \n",
    "        listFilePaths = list(dfInputFiles[dfInputFiles.inputDesc == \"dataSet0\"].inputPath),\n",
    "        socialGraphTargetDict = dictSGD_ForwardedFrom,\n",
    "        socialGraphTargetAttribute = \"procEvalIsForwarded\",\n",
    "        configFlagDebugLabel = False\n",
    "    ),\n",
    "    layoutSelector = 1,\n",
    "    configFactorEdge = 100,\n",
    "    configFactorNode = 10,\n",
    "    configArrowSize = 15,\n",
    "    configPlotWidth = 32,\n",
    "    configPlotHeight = 32,\n",
    "    outputFilename = \"social-graph-dataSet0-forwarded-from.png\",\n",
    "    outputTitle = \"Forwarded From (Top 100 by 0.5 percent) (dataSet0)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(\"dataSet2\" in C_LOAD_DATASET):\n",
    "    drawSocialGraph(\n",
    "        generateSocialGraph(\n",
    "            configTopNInfluencer = 100,  \n",
    "            configMinRefs = 1,       \n",
    "            listFilePaths = list(dfInputFiles[dfInputFiles.inputDesc == \"dataSet2\"].inputPath),\n",
    "            socialGraphTargetDict = dictSGD_ForwardedFrom,\n",
    "            socialGraphTargetAttribute = \"procEvalIsForwarded\",\n",
    "            configFlagDebugLabel = False\n",
    "        ),\n",
    "        layoutSelector = 1,\n",
    "        configFactorEdge = 100,\n",
    "        configFactorNode = 10,\n",
    "        configArrowSize = 15,\n",
    "        configPlotWidth = 32,\n",
    "        configPlotHeight = 32,\n",
    "        outputFilename = \"social-graph-dataSet2-forwarded-from.png\",\n",
    "        outputTitle = \"Forwarded From (Top 100 by 1 percent) (dataSet2)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enable\n",
    "\"\"\"\n",
    "drawSocialGraph(\n",
    "    generateSocialGraph(\n",
    "        configTopNInfluencer = 25,  \n",
    "        configMinRefs = 0,       \n",
    "        listFilePaths = list(dfInputFiles.inputPath),\n",
    "        socialGraphTargetDict = dictSGD_ForwardedFrom,\n",
    "        socialGraphTargetAttribute = \"procEvalIsForwarded\",\n",
    "        configFlagDebugLabel = False\n",
    "    ),\n",
    "    layoutSelector = 3,\n",
    "    configFactorEdge = 100,\n",
    "    configFactorNode = 10,\n",
    "    configArrowSize = 15,\n",
    "    configPlotWidth = 32,\n",
    "    configPlotHeight = 32,\n",
    "    outputFilename = \"social-graph-global-graphviz-forwarded-from.png\",\n",
    "    outputTitle = \"Forwarded From (Top 25) (global - graphviz)\"\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real Graph (Hashtag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawSocialGraph(\n",
    "    generateSocialGraph(\n",
    "        configTopNInfluencer = 100,  \n",
    "        configMinRefs = 0.5,        \n",
    "        listFilePaths = list(dfInputFiles[dfInputFiles.inputDesc == \"dataSet0\"].inputPath),\n",
    "        socialGraphTargetDict = dictSGD_Hashtag,\n",
    "        socialGraphTargetAttribute = \"procEvalContainsHashtag\",\n",
    "        configFlagDebugLabel = False\n",
    "    ),\n",
    "    layoutSelector = 1,\n",
    "    configFactorEdge = 100,\n",
    "    configFactorNode = 10,\n",
    "    configArrowSize = 15,\n",
    "    configPlotWidth = 32,\n",
    "    configPlotHeight = 32,\n",
    "    outputFilename = \"social-graph-dataSet0-hashtag.png\",\n",
    "    outputTitle = \"Hashtags (Top 100 by 0.5 percent) (dataSet0)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(\"dataSet2\" in C_LOAD_DATASET and False): #TODO: Fix Graph\n",
    "    drawSocialGraph(\n",
    "        generateSocialGraph(\n",
    "            configTopNInfluencer = 30,  \n",
    "            configMinRefs = 4,        \n",
    "            listFilePaths = list(dfInputFiles[dfInputFiles.inputDesc == \"dataSet2\"].inputPath),\n",
    "            socialGraphTargetDict = dictSGD_Hashtag,\n",
    "            socialGraphTargetAttribute = \"procEvalContainsHashtag\",\n",
    "            configFlagDebugLabel = False\n",
    "        ),\n",
    "        layoutSelector = 1,\n",
    "        configFactorEdge = 100,\n",
    "        configFactorNode = 10,\n",
    "        configArrowSize = 15,\n",
    "        configPlotWidth = 32,\n",
    "        configPlotHeight = 32,\n",
    "        outputFilename = \"social-graph-dataSet2-hashtag.png\",\n",
    "        outputTitle = \"Hashtags (Top 30) (dataSet2)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real Graph (Hosts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawSocialGraph(\n",
    "    generateSocialGraph(\n",
    "        configTopNInfluencer = 100,  \n",
    "        configMinRefs = 0.5,        \n",
    "        listFilePaths = list(dfInputFiles[dfInputFiles.inputDesc == \"dataSet0\"].inputPath),\n",
    "        socialGraphTargetDict = dictSGD_Host,\n",
    "        socialGraphTargetAttribute = \"procEvalContainsUrl\",\n",
    "        configFlagDebugLabel = False\n",
    "    ),\n",
    "    layoutSelector = 1,\n",
    "    configFactorEdge = 100,\n",
    "    configFactorNode = 10,\n",
    "    configArrowSize = 15,\n",
    "    configPlotWidth = 32,\n",
    "    configPlotHeight = 32,\n",
    "    outputFilename = \"social-graph-dataSet0-host.png\",\n",
    "    outputTitle = \"Hosts (Top 100 by 0.5 percent) (dataSet0)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(\"dataSet2\" in C_LOAD_DATASET):\n",
    "    drawSocialGraph(\n",
    "        generateSocialGraph(\n",
    "            configTopNInfluencer = 100,  \n",
    "            configMinRefs = 1,        \n",
    "            listFilePaths = list(dfInputFiles[dfInputFiles.inputDesc == \"dataSet2\"].inputPath),\n",
    "            socialGraphTargetDict = dictSGD_Host,\n",
    "            socialGraphTargetAttribute = \"procEvalContainsUrl\",\n",
    "            configFlagDebugLabel = False\n",
    "        ),\n",
    "        layoutSelector = 1,\n",
    "        configFactorEdge = 100,\n",
    "        configFactorNode = 10,\n",
    "        configArrowSize = 15,\n",
    "        configPlotWidth = 32,\n",
    "        configPlotHeight = 32,\n",
    "        outputFilename = \"social-graph-dataSet2-host.png\",\n",
    "        outputTitle = \"Hosts (Top 100 by 1 percent) (dataSet2)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real Graph (Emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawSocialGraph(\n",
    "    generateSocialGraph(\n",
    "        configTopNInfluencer = 100,  \n",
    "        configMinRefs = 0.5,        \n",
    "        listFilePaths = list(dfInputFiles[dfInputFiles.inputDesc == \"dataSet0\"].inputPath),\n",
    "        socialGraphTargetDict = dictSGD_Emoji,\n",
    "        socialGraphTargetAttribute = \"procEvalContainsEmojiItem\",\n",
    "        configFlagDebugLabel = False\n",
    "    ),\n",
    "    layoutSelector = 1,\n",
    "    configFactorEdge = 100,\n",
    "    configFactorNode = 10,\n",
    "    configArrowSize = 15,\n",
    "    configPlotWidth = 32,\n",
    "    configPlotHeight = 32,\n",
    "    outputFilename = \"social-graph-dataSet0-emoji.png\",\n",
    "    outputTitle = \"Emojis (Top 100 by 0.5 percent) (dataSet0)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(\"dataSet2\" in C_LOAD_DATASET):\n",
    "    drawSocialGraph(\n",
    "        generateSocialGraph(\n",
    "            configTopNInfluencer = 100,  \n",
    "            configMinRefs = 1,        \n",
    "            listFilePaths = list(dfInputFiles[dfInputFiles.inputDesc == \"dataSet2\"].inputPath),\n",
    "            socialGraphTargetDict = dictSGD_Emoji,\n",
    "            socialGraphTargetAttribute = \"procEvalContainsEmojiItem\",\n",
    "            configFlagDebugLabel = False\n",
    "        ),\n",
    "        layoutSelector = 1,\n",
    "        configFactorEdge = 100,\n",
    "        configFactorNode = 10,\n",
    "        configArrowSize = 15,\n",
    "        configPlotWidth = 32,\n",
    "        configPlotHeight = 32,\n",
    "        outputFilename = \"social-graph-dataSet2-emoji.png\",\n",
    "        outputTitle = \"Emojis (Top 100 by 1 percent) (dataSet2)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real Graph (From)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(\"dataSet1a\" in C_LOAD_DATASET):\n",
    "    drawSocialGraph(\n",
    "        generateSocialGraph(\n",
    "            configTopNInfluencer = 50,  \n",
    "            configMinRefs = 0,        \n",
    "            listFilePaths = list(dfInputFiles[dfInputFiles.inputDesc == \"dataSet1a\"].inputPath),\n",
    "            socialGraphTargetDict = dictSGD_From,\n",
    "            socialGraphTargetAttribute = \"procEvalIsValidText\", #TODO: Improve\n",
    "            configFlagDebugLabel = False\n",
    "        ),\n",
    "        layoutSelector = 1,\n",
    "        configFactorEdge = 100,\n",
    "        configFactorNode = 1,\n",
    "        configArrowSize = 15,\n",
    "        configPlotWidth = 32,\n",
    "        configPlotHeight = 32,\n",
    "        outputFilename = \"social-graph-dataSet1a-from.png\",\n",
    "        outputTitle = \"From (Top 50) (dataSet1a)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(\"dataSet2\" in C_LOAD_DATASET):\n",
    "    drawSocialGraph(\n",
    "        generateSocialGraph(\n",
    "            configTopNInfluencer = 50,  \n",
    "            configMinRefs = 0,        \n",
    "            listFilePaths = [\n",
    "                \"DS-13-01-2021/ChatExport_2021-01-13-querdenken089\",\n",
    "                \"DS-13-01-2021/ChatExport_2021-01-13-querdenken69\",\n",
    "                \"DS-13-01-2021/ChatExport_2021-01-13-querdenken773\",\n",
    "                \"DS-13-01-2021/ChatExport_2021-01-13-querdenken711\"\n",
    "                ],\n",
    "            socialGraphTargetDict = dictSGD_From,\n",
    "            socialGraphTargetAttribute = \"procEvalIsValidText\", #TODO: Improve\n",
    "            configFlagDebugLabel = False\n",
    "        ),\n",
    "        layoutSelector = 1,\n",
    "        configFactorEdge = 100,\n",
    "        configFactorNode = 1,\n",
    "        configArrowSize = 15,\n",
    "        configPlotWidth = 32,\n",
    "        configPlotHeight = 32,\n",
    "        outputFilename = \"social-graph-groups-dataSet2-from.png\",\n",
    "        outputTitle = \"From (Top 50) (dataSet2-groups)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "param   targetDate      e.g. 1970-01-01\n",
    "param   fP              filePath\n",
    "param   highlightWord   set \"\" = no filter\n",
    "\"\"\"\n",
    "def queryNumberOfMessagesByDate(targetDate, fP, highlightWord):\n",
    "\n",
    "    df = dictMessages[fP].copy()\n",
    "\n",
    "    df = df[df.procEvalIsValidText == True]\n",
    "\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    \n",
    "    df = df[df.date <= targetDate]\n",
    "\n",
    "    if(highlightWord != \"\"):\n",
    "        df = df[df.procTDSafeLowercaseText.str.contains(highlightWord)]\n",
    "\n",
    "    l = len(df.index)\n",
    "\n",
    "    if(l > 0):\n",
    "        return l\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "param   filePathList\n",
    "param   outputFilename  set \"\" = no output file\n",
    "param   highlightWords  list of highlight words (leave empty if not used)\n",
    "param   configFrequency e.g. 1D or 1M\n",
    "\"\"\"\n",
    "# TODO: Add percent to label\n",
    "def drawTimePlot(filePathList, outputFilename, highlightWords, configFrequency):\n",
    "\n",
    "    gloStartStopwatch(\"Time Plot\")\n",
    "\n",
    "    plt.figure(figsize=(16, 9))\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        index=pd.date_range( #m/d/y\n",
    "            start='9/1/2018',\n",
    "            end='2/1/2021',\n",
    "            freq=configFrequency\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Add date to process\n",
    "    df[\"date\"] = df.index\n",
    "\n",
    "    vLineHeight = -1\n",
    "\n",
    "    for fP in filePathList:\n",
    "        gloStartStopwatch(\"Time Plot >>\" + fP + \"<<\")\n",
    "\n",
    "        # Plot Graph Var 1\n",
    "        if not highlightWords:\n",
    "            # Plot\n",
    "            plt.plot(\n",
    "                df.index, #x\n",
    "                df.apply(lambda x: queryNumberOfMessagesByDate(x.date, fP, highlightWord = \"\"), axis=1), #y\n",
    "                label = queryChatName(fP) #label\n",
    "            )\n",
    "            # Set vline height\n",
    "            currentHeight = queryNumberOfMessagesByAttEqTrue(fP, \"procEvalIsValidText\")\n",
    "            if(currentHeight > vLineHeight):\n",
    "                vLineHeight = currentHeight\n",
    "\n",
    "        # Plot High Light Word Graph / Var 2\n",
    "        for hWord in highlightWords:\n",
    "            y = df.apply(lambda x: queryNumberOfMessagesByDate(x.date, fP, highlightWord = hWord), axis=1)\n",
    "            # Plot\n",
    "            plt.plot(\n",
    "                df.index, #x\n",
    "                y, #y\n",
    "                label = queryChatName(fP) + \" usage of '\" + hWord + \"'\" #label\n",
    "            )\n",
    "            # Set vline height\n",
    "            currentHeight = y.max()\n",
    "            if(currentHeight > vLineHeight):\n",
    "                vLineHeight = currentHeight\n",
    "\n",
    "        gloStopStopwatch(\"Time Plot >>\" + fP + \"<<\")\n",
    "\n",
    "    # yy - mm - dd\n",
    "    # TODO: Double check https://www.bundesgesundheitsministerium.de/coronavirus/chronik-coronavirus.html?stand=20210104\n",
    "    plt.vlines(x = [\"2018-12-10\"], ymin=0, ymax=vLineHeight, color=\"orange\", ls='--', label=\"Global Compact for Migration (2018-12-10)\")\n",
    "    plt.vlines(x = [\"2020-01-27\"], ymin=0, ymax=vLineHeight, color=\"grey\", ls='--', label=\"Corona Patient Zero Germany\")\n",
    "    plt.vlines(x = [\"2020-03-23\"], ymin=0, ymax=vLineHeight, color=\"purple\", ls='--', label=\"1. Lockdown Germany (2020-03-23)\")\n",
    "    plt.vlines(x = [\"2020-11-02\"], ymin=0, ymax=vLineHeight, color=\"purple\", ls='--', label=\"2. Lockdown light Germany (2020-11-02)\")\n",
    "    plt.vlines(x = [\"2020-12-16\"], ymin=0, ymax=vLineHeight, color=\"purple\", ls='--', label=\"3. Lockdown Germany (2020-12-16)\")\n",
    "\n",
    "    plt.gcf().autofmt_xdate()\n",
    "    _ = plt.legend()\n",
    "\n",
    "    if(outputFilename != \"\"):\n",
    "        plt.savefig(dir_var_output + outputFilename)\n",
    "\n",
    "    gloStopStopwatch(\"Time Plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    drawTimePlot(\n",
    "        filePathList = list(dfInputFiles[dfInputFiles.inputDesc == \"dataSet0\"].inputPath),\n",
    "        outputFilename = \"time-plot-dataSet0.png\",\n",
    "        highlightWords = [],\n",
    "        configFrequency=C_TIME_PLOT_FREQ\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    if(\"dataSet1\" in C_LOAD_DATASET):\n",
    "        drawTimePlot(\n",
    "            filePathList = list(dfInputFiles[dfInputFiles.inputDesc == \"dataSet1\"].inputPath),\n",
    "            outputFilename = \"time-plot-dataSet1.png\",\n",
    "            highlightWords = [],\n",
    "            configFrequency=C_TIME_PLOT_FREQ\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    if(\"dataSet1a\" in C_LOAD_DATASET):\n",
    "        drawTimePlot(\n",
    "            filePathList = list(dfInputFiles[dfInputFiles.inputDesc == \"dataSet1a\"].inputPath),\n",
    "            outputFilename = \"time-plot-dataSet1a.png\",\n",
    "            highlightWords = [],\n",
    "            configFrequency=C_TIME_PLOT_FREQ\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    if(\"dataSet2\" in C_LOAD_DATASET):\n",
    "        drawTimePlot(\n",
    "            filePathList = list(dfInputFiles[dfInputFiles.inputDesc == \"dataSet2\"].inputPath),\n",
    "            outputFilename = \"time-plot-dataSet2.png\",\n",
    "            highlightWords = [],\n",
    "            configFrequency=C_TIME_PLOT_FREQ\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Context?\n",
    "# TODO: Improve stop words\n",
    "\n",
    "\"\"\"\n",
    "WordCloud\n",
    "\n",
    "param   targetDataFrame     DataFrame\n",
    "param   outputFilename      filename in outputdir (set \"\" == no output file)\n",
    "param   filterList          Exclude list\n",
    "param   flagShow            Set true == show wordcloud\n",
    "param   configPlotWidth     e.g. 1920\n",
    "param   configPlotHeight    e.g. 1080\n",
    "\"\"\"\n",
    "def generateWordCloud(targetDataFrame, outputFilename, filterList, flagShow, configPlotWidth, configPlotHeight):\n",
    "    \n",
    "\n",
    "    dfMessages = targetDataFrame.copy()\n",
    "    \n",
    "    textString = gloGenerateTextFromChat(dfMessages, rowID=\"procTDSafeText\")\n",
    "    \n",
    "    stopWordsList = gloGetStopWordsList(filterList)\n",
    "    \n",
    "    # Generate word cloud and save it to file\n",
    "    wordcloud = WordCloud(\n",
    "                background_color=\"black\",\n",
    "                width=configPlotWidth,\n",
    "                height=configPlotHeight,\n",
    "                stopwords=stopWordsList\n",
    "            ).generate(textString)\n",
    "\n",
    "    if(outputFilename != \"\"):\n",
    "        wordcloud.to_file(dir_var_output + outputFilename)\n",
    "    \n",
    "    if(flagShow):\n",
    "        # Show top 20\n",
    "        print()\n",
    "        print(\"Top 20 occ:\\n\" + str(pd.Series(wordcloud.words_).head(20)))\n",
    "        print()\n",
    "        \n",
    "        # Show word cloud\n",
    "        print(\"- Start generate figure\")\n",
    "        plt.figure(figsize=(14, 14))\n",
    "        plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oliver Janich öffentlich (public_channel - dataSet0)\n",
    "generateWordCloud(\n",
    "    dictMessages[\"DS-05-01-2021/ChatExport_2021-01-05-janich\"],\n",
    "    \"wordcloud-oliver-janich.png\",\n",
    "    [],\n",
    "    flagShow = True,\n",
    "    configPlotWidth = 1920,\n",
    "    configPlotHeight = 1080\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATTILA HILDMANN OFFICIAL (public_channel - dataSet0)\n",
    "if(C_SHORT_RUN == False):\n",
    "    generateWordCloud(\n",
    "        dictMessages[\"DS-05-01-2021/ChatExport_2021-01-05-hildmann\"],\n",
    "        \"wordcloud-attila-hildmann.png\",\n",
    "        [],\n",
    "        flagShow = True,\n",
    "        configPlotWidth = 1920,\n",
    "        configPlotHeight = 1080\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eva Herman Offiziell (public_channel - dataSet0)\n",
    "if(C_SHORT_RUN == False):\n",
    "    generateWordCloud(\n",
    "        dictMessages[\"DS-05-01-2021/ChatExport_2021-01-05-evaherman\"],\n",
    "        \"wordcloud-eva-herman.png\",\n",
    "        [],\n",
    "        flagShow = True,\n",
    "        configPlotWidth = 1920,\n",
    "        configPlotHeight = 1080\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Xavier Naidoo (public_channel - dataSet0)\n",
    "if(C_SHORT_RUN == False):\n",
    "    generateWordCloud(\n",
    "        dictMessages[\"DS-05-01-2021/ChatExport_2021-01-05-xavier\"],\n",
    "        \"wordcloud-xavier-naidoo.png\",\n",
    "        [],\n",
    "        flagShow = True,\n",
    "        configPlotWidth = 1920,\n",
    "        configPlotHeight = 1080\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    if(\"dataSet2\" in C_LOAD_DATASET):\n",
    "        generateWordCloud(\n",
    "            dictMessages[\"DS-13-01-2021/ChatExport_2021-01-13-querdenken089\"],\n",
    "            \"wordcloud-querdenken-089-group.png\",\n",
    "            [],\n",
    "            flagShow = True,\n",
    "            configPlotWidth = 1920,\n",
    "            configPlotHeight = 1080\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    if(\"dataSet2\" in C_LOAD_DATASET):\n",
    "        generateWordCloud(\n",
    "            dictMessages[\"DS-13-01-2021/ChatExport_2021-01-13-querdenken591Info\"],\n",
    "            \"wordcloud-querdenken-591-info.png\",\n",
    "            [],\n",
    "            flagShow = True,\n",
    "            configPlotWidth = 1920,\n",
    "            configPlotHeight = 1080\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    if(\"dataSet2\" in C_LOAD_DATASET):\n",
    "        generateWordCloud(\n",
    "            dictMessages[\"DS-13-01-2021/ChatExport_2021-01-13-querdenken773\"],\n",
    "            \"wordcloud-querdenken-773-group.png\",\n",
    "            [],\n",
    "            flagShow = True,\n",
    "            configPlotWidth = 1920,\n",
    "            configPlotHeight = 1080\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    if(\"dataSet2\" in C_LOAD_DATASET):\n",
    "        generateWordCloud(\n",
    "            dictMessages[\"DS-13-01-2021/ChatExport_2021-01-13-querdenken773Info\"],\n",
    "            \"wordcloud-querdenken-773-info.png\",\n",
    "            [],\n",
    "            flagShow = True,\n",
    "            configPlotWidth = 1920,\n",
    "            configPlotHeight = 1080\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    if(\"dataSet2\" in C_LOAD_DATASET):\n",
    "        generateWordCloud(\n",
    "            dictMessages[\"DS-13-01-2021/ChatExport_2021-01-13-querdenken711\"],\n",
    "            \"wordcloud-querdenken-711-group.png\",\n",
    "            [],\n",
    "            flagShow = True,\n",
    "            configPlotWidth = 1920,\n",
    "            configPlotHeight = 1080\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    if(\"dataSet2\" in C_LOAD_DATASET):\n",
    "        generateWordCloud(\n",
    "            dictMessages[\"DS-13-01-2021/ChatExport_2021-01-13-querdenken711Info\"],\n",
    "            \"wordcloud-querdenken-711-info.png\",\n",
    "            [],\n",
    "            flagShow = True,\n",
    "            configPlotWidth = 1920,\n",
    "            configPlotHeight = 1080\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    if(\"dataSet2\" in C_LOAD_DATASET):\n",
    "        generateWordCloud(\n",
    "            dictMessages[\"DS-13-01-2021/ChatExport_2021-01-13-querdenken69\"],\n",
    "            \"wordcloud-querdenken-69-group.png\",\n",
    "            [],\n",
    "            flagShow = True,\n",
    "            configPlotWidth = 1920,\n",
    "            configPlotHeight = 1080\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    if(\"dataSet2\" in C_LOAD_DATASET):\n",
    "        generateWordCloud(\n",
    "            dictMessages[\"DS-13-01-2021/ChatExport_2021-01-13-querdenken69Info\"],\n",
    "            \"wordcloud-querdenken-69-info.png\",\n",
    "            [],\n",
    "            flagShow = True,\n",
    "            configPlotWidth = 1920,\n",
    "            configPlotHeight = 1080\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for test purposes\n",
    "if(C_SHORT_RUN == False and False): #TODO: Enable\n",
    "    generateWordCloud(\n",
    "        dfAllDataMessages,\n",
    "        \"\",\n",
    "        [],\n",
    "        flagShow = True,\n",
    "        configPlotWidth = 1920,\n",
    "        configPlotHeight = 1080\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto Mode (Batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractTimePeriodDataFrame(df, timeStart, timeStop):\n",
    "\n",
    "    #print(\"- Got Start \" + str(timeStart) + \" and Stop \" + str(timeStop))\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    \n",
    "    dfNew = df[df.date <= timeStop]\n",
    "    dfNew = dfNew[dfNew.date >= timeStart]\n",
    "\n",
    "    dfNew = dfNew.set_index(\"date\")\n",
    "    dfNew = dfNew.sort_index()\n",
    "\n",
    "    return dfNew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateWCPeriod():\n",
    "    return list(pd.date_range( #m/d/y\n",
    "            start='1/1/2018',\n",
    "            end='2/1/2021',\n",
    "            #freq=\"W-MON\",\n",
    "            freq=\"1M\"\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Wrapper WordCloud\n",
    "\n",
    "param   fP  filePath\n",
    "param   label e.g. chatName\n",
    "param   filterList additional stopWords\n",
    "\"\"\"\n",
    "def generateWordCloudAuto(fP, label, filterList):\n",
    "\n",
    "    gloStartStopwatch(\"Generate World Cloud Auto >>\" + fP + \"<<\")\n",
    "\n",
    "    periods = generateWCPeriod()\n",
    "\n",
    "    dictSaved = {}\n",
    "\n",
    "    prevStart = periods[0]\n",
    "\n",
    "    for period in periods:\n",
    "\n",
    "        stop = period\n",
    "\n",
    "        e = extractTimePeriodDataFrame(dictMessages[fP], timeStart = prevStart, timeStop = stop)\n",
    "\n",
    "        if(prevStart != stop and len(e.index) > 0):\n",
    "            fileName = \"autoWordCloud/\" + queryChatName(fP) + \"-\" + str(prevStart) + \"-\" + str(stop) + \".png\"\n",
    "            generateWordCloud(\n",
    "                e,\n",
    "                fileName,\n",
    "                filterList,\n",
    "                flagShow = False,\n",
    "                configPlotWidth = 1280,\n",
    "                configPlotHeight = 720\n",
    "            )\n",
    "            #print(\"- Save file \" + fileName)\n",
    "            dictSaved[fileName] = str(prevStart) + \" - \" + str(stop)\n",
    "\n",
    "        \"\"\"\n",
    "        else:\n",
    "            print(\"- Start and Stop equal or no message found\")\n",
    "        \"\"\"\n",
    "\n",
    "        prevStart = stop\n",
    "\n",
    "    gloWriteDictToFile(\"auto-wordcloud-\" + label + \".csv\", dictSaved)\n",
    "\n",
    "    gloStopStopwatch(\"Generate World Cloud Auto >>\" + fP + \"<<\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    generateWordCloudAuto(\n",
    "        fP = \"DS-05-01-2021/ChatExport_2021-01-05-janich\",\n",
    "        label = \"oliver-janich\",\n",
    "        filterList = []\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    generateWordCloudAuto(\n",
    "        fP = \"DS-05-01-2021/ChatExport_2021-01-05-hildmann\",\n",
    "        label = \"attila-hildmann\",\n",
    "        filterList = []\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    generateWordCloudAuto(\n",
    "        fP = \"DS-05-01-2021/ChatExport_2021-01-05-evaherman\",\n",
    "        label = \"eva-herman\",\n",
    "        filterList = []\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    generateWordCloudAuto(\n",
    "        fP = \"DS-05-01-2021/ChatExport_2021-01-05-xavier\",\n",
    "        label = \"xavier-naidoo\",\n",
    "        filterList = []\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateNGram(text, n):\n",
    "    # https://albertauyeung.github.io/2018/06/03/generating-ngrams.html\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "    tokens = [token for token in text.split(\" \") if token != \"\"]\n",
    "    \n",
    "    return list(ngrams(tokens, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateNGramChat(fP, n, mostCommon):\n",
    "    return Counter(\n",
    "        generateNGram(\n",
    "            gloGenerateTextFromChat(dictMessages[fP], rowID=\"procTDSafeText\"),\n",
    "            n = n\n",
    "        )\n",
    "    ).most_common(mostCommon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateNGramAuto(filePathList, n, mostCommon):\n",
    "    for fP in filePathList:\n",
    "\n",
    "        print()\n",
    "        print(\"Analyse now >>\" + fP + \"<<\")\n",
    "\n",
    "        c = generateNGramChat(\n",
    "            fP,\n",
    "            n = n,\n",
    "            mostCommon = mostCommon\n",
    "        )\n",
    "\n",
    "        print (\"\\n\".join(map(str, c)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataSet0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generateNGramAuto(\n",
    "    dfInputFiles[dfInputFiles.inputDesc == \"dataSet0\"].inputPath,\n",
    "    n = 2,\n",
    "    mostCommon = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generateNGramAuto(\n",
    "    dfInputFiles[dfInputFiles.inputDesc == \"dataSet0\"].inputPath,\n",
    "    n = 3,\n",
    "    mostCommon = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generateNGramAuto(\n",
    "    dfInputFiles[dfInputFiles.inputDesc == \"dataSet0\"].inputPath,\n",
    "    n = 4,\n",
    "    mostCommon = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generateNGramAuto(\n",
    "    dfInputFiles[dfInputFiles.inputDesc == \"dataSet0\"].inputPath,\n",
    "    n = 5,\n",
    "    mostCommon = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generateNGramAuto(\n",
    "    dfInputFiles[dfInputFiles.inputDesc == \"dataSet0\"].inputPath,\n",
    "    n = 6,\n",
    "    mostCommon = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excerpt DataSet2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(\"dataSet2\" in C_LOAD_DATASET):\n",
    "    generateNGramAuto(\n",
    "        dfInputFiles[dfInputFiles.inputDesc == \"dataSet2\"].inputPath,\n",
    "        n = 2,\n",
    "        mostCommon = 10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(\"dataSet2\" in C_LOAD_DATASET):\n",
    "    generateNGramAuto(\n",
    "        dfInputFiles[dfInputFiles.inputDesc == \"dataSet2\"].inputPath,\n",
    "        n = 3,\n",
    "        mostCommon = 10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(\"dataSet2\" in C_LOAD_DATASET):\n",
    "    generateNGramAuto(\n",
    "        dfInputFiles[dfInputFiles.inputDesc == \"dataSet2\"].inputPath,\n",
    "        n = 5,\n",
    "        mostCommon = 10\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freq Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateFreqNounsPlot(fP, mostCommon, outputFilename):\n",
    "\n",
    "    gloStartStopwatch(\"Generate text\")\n",
    "    df = dictMessages[fP].copy()\n",
    "    inputText = gloGenerateTextFromChat(df, \"procTDCleanText\")\n",
    "    gloStopStopwatch(\"Generate text\")\n",
    "\n",
    "    gloStartStopwatch(\"Process data\")\n",
    "    plotFreqNouns(inputText, outputFilename=outputFilename, mostCommon=mostCommon, flagRemoveStopwords=True)\n",
    "    gloStopStopwatch(\"Process data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    generateFreqNounsPlot(\"DS-05-01-2021/ChatExport_2021-01-05-janich\", mostCommon=25, outputFilename = \"freq-nouns-oliver-janich.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    generateFreqNounsPlot(\"DS-05-01-2021/ChatExport_2021-01-05-hildmann\", mostCommon=25, outputFilename = \"freq-nouns-attila-hildmann.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    generateFreqNounsPlot(\"DS-05-01-2021/ChatExport_2021-01-05-evaherman\", mostCommon=25, outputFilename = \"freq-nouns-eva-herman.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    generateFreqNounsPlot(\"DS-05-01-2021/ChatExport_2021-01-05-xavier\", mostCommon=25, outputFilename = \"freq-nouns-xavier-naidoo.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP with Bert and co."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval Ner Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalNerPipeline(pipelineKey, inputSelector, configTopN):\n",
    "\n",
    "    if(inputSelector in C_PIPELINE_DATASET):\n",
    "        \n",
    "        filePaths = dfInputFiles[dfInputFiles.inputDesc == inputSelector].inputPath\n",
    "\n",
    "        for fP in filePaths:\n",
    "            \n",
    "            gloStartStopwatch(\"Process now >>\" + str(fP) + \"<<\")\n",
    "\n",
    "            if(pipelineKey == \"ner-xlm-roberta\" or pipelineKey == \"ner-bert\"):\n",
    "                \n",
    "                df = dictMessages[fP].copy()\n",
    "                df = df[df.procEvalIsValidText == True]\n",
    "                \n",
    "                listPer     = list()\n",
    "                listMisc    = list()\n",
    "                listOrg     = list()\n",
    "                listLoc     = list()\n",
    "\n",
    "                for index, row in df.iterrows():\n",
    "\n",
    "                    d = row[\"procPipeline-\" + pipelineKey]\n",
    "                    \n",
    "                    listPer.extend(d[\"per\"])\n",
    "                    listMisc.extend(d[\"misc\"])\n",
    "                    listOrg.extend(d[\"org\"])\n",
    "                    listLoc.extend(d[\"loc\"])\n",
    "\n",
    "                print(\"- Top per -\")\n",
    "                print (\"\\n\".join(map(str, Counter(listPer).most_common(configTopN))))\n",
    "                print()\n",
    "\n",
    "                print(\"- Top misc -\")\n",
    "                print (\"\\n\".join(map(str, Counter(listMisc).most_common(configTopN))))\n",
    "                print()\n",
    "\n",
    "                print(\"- Top org -\")\n",
    "                print (\"\\n\".join(map(str, Counter(listOrg).most_common(configTopN))))\n",
    "                print()\n",
    "\n",
    "                print(\"- Top loc -\")\n",
    "                print (\"\\n\".join(map(str, Counter(listLoc).most_common(configTopN))))\n",
    "                print()\n",
    "\n",
    "            else:\n",
    "                print(\"Error pipeline not found >>\" + str(pipelineKey) + \"<<\")\n",
    "\n",
    "            gloStopStopwatch(\"Process now >>\" + str(fP) + \"<<\")\n",
    "\n",
    "    else:\n",
    "        print(\"Error data not found >>\" + inputSelector + \"<<\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalNerPipeline(\"ner-xlm-roberta\", \"dataSet0\", configTopN = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalNerPipeline(\"ner-bert\", \"dataSet0\", configTopN = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval Sen Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalSenPipeline(pipelineKey, inputSelector, outputFilename, configRolling, configShowScatter):\n",
    "\n",
    "    if(inputSelector in C_PIPELINE_DATASET):\n",
    "        \n",
    "        filePaths = dfInputFiles[dfInputFiles.inputDesc == inputSelector].inputPath\n",
    "\n",
    "        plt.figure(figsize=(32, 18))\n",
    "\n",
    "        for fP in filePaths:\n",
    "            \n",
    "            gloStartStopwatch(\"Process now >>\" + str(fP) + \"<<\")\n",
    "\n",
    "            if(pipelineKey == \"sen-bert\"):\n",
    "                \n",
    "                df = dictMessages[fP].copy()\n",
    "                df = df[df.procEvalIsValidText == True]\n",
    "\n",
    "                df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "                df = df.set_index(\"date\")\n",
    "                df = df.sort_index()\n",
    "\n",
    "                # key = x = time / value = y = score\n",
    "                dictData = {}\n",
    "\n",
    "                for index, row in df.iterrows():\n",
    "                    \n",
    "                    date = index\n",
    "                    score = row[\"procPipeline-sen-bert\"]\n",
    "\n",
    "                    if(score != -1):\n",
    "                        dictData[date] = score\n",
    "\n",
    "                # Plot\n",
    "                x,y = zip(*sorted(dictData.items()))\n",
    "                \n",
    "                df = pd.DataFrame(list(zip(x, y)), columns =['x', 'y'])\n",
    "\n",
    "                df['rolling'] = df.y.rolling(configRolling).mean()\n",
    "\n",
    "                sns.lineplot(data=df, x=\"x\", y=\"rolling\", label = queryChatName(fP))\n",
    "\n",
    "                if(configShowScatter):\n",
    "                    sns.scatterplot(data=df, x=\"x\", y=\"y\", label = queryChatName(fP), marker=\"+\")\n",
    "\n",
    "                plt.gcf().autofmt_xdate()\n",
    "\n",
    "                # Add vlines\n",
    "                vLineMin = 2\n",
    "                vLineMax = 4\n",
    "\n",
    "            elif(pipelineKey==\"sentiment\"):\n",
    "\n",
    "                df = dictMessages[fP].copy()\n",
    "                df = df[df.procEvalIsValidText == True]\n",
    "\n",
    "                df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "                df = df.set_index(\"date\")\n",
    "                df = df.sort_index()\n",
    "\n",
    "                # key = x = time / value = y = score\n",
    "                dictData = {}\n",
    "\n",
    "                for index, row in df.iterrows():\n",
    "                    \n",
    "                    date = index\n",
    "                    retDict = row[\"procPipeline-sentiment\"]\n",
    "\n",
    "                    if(retDict != None):\n",
    "                        polarity = retDict[\"polarity\"]\n",
    "                        dictData[date] = polarity\n",
    "\n",
    "                # Plot\n",
    "                x,y = zip(*sorted(dictData.items()))\n",
    "\n",
    "                df = pd.DataFrame(list(zip(x, y)), columns =['x', 'y'])\n",
    "\n",
    "                df['rolling'] = df.y.rolling(configRolling).mean()\n",
    "\n",
    "                sns.lineplot(data=df, x=\"x\", y=\"rolling\", label = queryChatName(fP))\n",
    "\n",
    "                if(configShowScatter):\n",
    "                    sns.scatterplot(data=df, x=\"x\", y=\"y\", label = queryChatName(fP), marker=\"+\")\n",
    "\n",
    "                plt.gcf().autofmt_xdate()\n",
    "\n",
    "                # Add vlines\n",
    "                vLineMin = -0.05\n",
    "                vLineMax = 0.175\n",
    "\n",
    "            elif(pipelineKey==\"subjectivity\"):\n",
    "\n",
    "                df = dictMessages[fP].copy()\n",
    "                df = df[df.procEvalIsValidText == True]\n",
    "\n",
    "                df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "                df = df.set_index(\"date\")\n",
    "                df = df.sort_index()\n",
    "\n",
    "                # key = x = time / value = y = score\n",
    "                dictData = {}\n",
    "\n",
    "                for index, row in df.iterrows():\n",
    "                    \n",
    "                    date = index\n",
    "                    retDict = row[\"procPipeline-sentiment\"]\n",
    "\n",
    "                    if(retDict != None):\n",
    "\n",
    "                        subjectivity = retDict[\"subjectivity\"]\n",
    "                        dictData[date] = subjectivity\n",
    "\n",
    "                # Plot\n",
    "                x,y = zip(*sorted(dictData.items()))\n",
    "\n",
    "                df = pd.DataFrame(list(zip(x, y)), columns =['x', 'y'])\n",
    "\n",
    "                df['rolling'] = df.y.rolling(configRolling).mean()\n",
    "\n",
    "                sns.lineplot(data=df, x=\"x\", y=\"rolling\", label = queryChatName(fP))\n",
    "\n",
    "                if(configShowScatter):\n",
    "                    sns.scatterplot(data=df, x=\"x\", y=\"y\", label = queryChatName(fP), marker=\"+\")\n",
    "\n",
    "                plt.gcf().autofmt_xdate()\n",
    "\n",
    "                # Add vlines\n",
    "                vLineMin = 0\n",
    "                vLineMax = 0.10\n",
    "                \n",
    "            else:\n",
    "                print(\"Error pipeline not found >>\" + str(pipelineKey) + \"<<\")\n",
    "\n",
    "            gloStopStopwatch(\"Process now >>\" + str(fP) + \"<<\")\n",
    "\n",
    "        # yy - mm - dd\n",
    "        # TODO: Double check https://www.bundesgesundheitsministerium.de/coronavirus/chronik-coronavirus.html?stand=20210104\n",
    "        plt.vlines(x = [\"2018-12-10\"], ymin=vLineMin, ymax=vLineMax, color=\"orange\", ls='--', label=\"Global Compact for Migration (2018-12-10)\")\n",
    "        plt.vlines(x = [\"2020-01-27\"], ymin=vLineMin, ymax=vLineMax, color=\"grey\", ls='--', label=\"Corona Patient Zero Germany\")\n",
    "        plt.vlines(x = [\"2020-03-23\"], ymin=vLineMin, ymax=vLineMax, color=\"purple\", ls='--', label=\"1. Lockdown Germany (2020-03-23)\")\n",
    "        plt.vlines(x = [\"2020-11-02\"], ymin=vLineMin, ymax=vLineMax, color=\"purple\", ls='--', label=\"2. Lockdown light Germany (2020-11-02)\")\n",
    "        plt.vlines(x = [\"2020-12-16\"], ymin=vLineMin, ymax=vLineMax, color=\"purple\", ls='--', label=\"3. Lockdown Germany (2020-12-16)\")\n",
    "\n",
    "        _ = plt.legend()\n",
    "\n",
    "        if(outputFilename != \"\"):\n",
    "            plt.savefig(dir_var_output + outputFilename)\n",
    "\n",
    "    else:\n",
    "        print(\"Error data not found >>\" + inputSelector + \"<<\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalSenPipeline(\"sen-bert\", \"dataSet0\", outputFilename = \"\", configRolling = 600, configShowScatter = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalSenPipeline(\"sen-bert\", \"dataSet0\", outputFilename = \"eval-pipeline-sen-dataSet0.png\", configRolling = 600, configShowScatter = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalSenPipeline(\"sentiment\", \"dataSet0\", outputFilename = \"\", configRolling = 600, configShowScatter = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalSenPipeline(\"sentiment\", \"dataSet0\", outputFilename = \"eval-pipeline-sen-textblob-dataSet0.png\", configRolling = 600, configShowScatter = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalSenPipeline(\"subjectivity\", \"dataSet0\", outputFilename = \"\", configRolling = 600, configShowScatter = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalSenPipeline(\"subjectivity\", \"dataSet0\", outputFilename = \"eval-pipeline-subjectivity-dataSet0.png\", configRolling = 600, configShowScatter = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(processSentimentAnalysisPython(\"Heute ist ein toller Tag. Ich freue mich hier zu sein!\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(processSentimentAnalysisPython(\"Heute war ein furchtbarer Tag. Ich hasse alles.\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processSenPipeline(\"Das ist toll. Ich würde es mir wieder kaufen!\", \"sen-bert\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processSenPipeline(\"Das ist toll. Ich würde es aber nicht mehr kaufen!\", \"sen-bert\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processSenPipeline(\"Das funktioniert nicht.\", \"sen-bert\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processTextGenPipeline(inputText, pipelineKey, cMaxLength):\n",
    "    if(pipelineKey in pipelineKeys):\n",
    "        return dictPipelines[pipelineKey](inputText, max_length=cMaxLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sText = \"Hallo, mein Name ist Max und ich esse gerne Eis. Ich schreibe gerade an meiner Masterarbeit und teste neue Verfahren. Ich komme aus dem Großraum München und bin Informatiker.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processNerPipeline(sText, \"ner-xlm-roberta\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processNerPipeline(sText, \"ner-bert\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processTextGenPipeline(sText, \"text-gen-gpt2\", cMaxLength = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processTextGenPipeline(sText, \"text-gen-gpt2-faust\", cMaxLength = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling\n",
    "\n",
    "Overview Topic Modeling\n",
    "https://nlpforhackers.io/topic-modeling/\n",
    "\n",
    "- LDA (Probabilistic Graphical Models)\n",
    "- LSA or LSI (Linear Algebra Singular Value Decomposition)\n",
    "- NMF (Linear Algebra Non-Negative Matrix Factorization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation (LDA) with Gensim\n",
    "\n",
    "https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Get clean words from it (arrays)\n",
    "def sendToWords(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc= removes punctuations\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "        \n",
    "def removeStopwords(texts, stop_words):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "return  (lda_model, corpus, id2word)\n",
    "\"\"\"\n",
    "def processLda(df, num_topics, debugPrint, filterList):\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    df = df[df.procEvalIsValidText == True]\n",
    "\n",
    "    df = df[[\"date\", \"procTDSafeLowercaseText\"]]\n",
    "\n",
    "    df = df.set_index(\"date\")\n",
    "    df = df.sort_index()\n",
    "\n",
    "    # ###############################\n",
    "    # ### Transform Data ############\n",
    "    # ###############################\n",
    "    stops_words = gloGetStopWordsList(filterList)\n",
    "\n",
    "    msgList     = df.procTDSafeLowercaseText.values.tolist()\n",
    "\n",
    "    # Create words\n",
    "    msg_words   = list(sendToWords(msgList))\n",
    "    msg_word    = removeStopwords(texts = msg_words, stop_words = stops_words)\n",
    "\n",
    "    # Create Dictionary (id to word)\n",
    "    id2word = corpora.Dictionary(msg_word)\n",
    "\n",
    "    # Create Corpus\n",
    "    texts = msg_word\n",
    "\n",
    "    # Term Document Frequency (dict to bag of words)\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "    # Build LDA model\n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics)\n",
    "\n",
    "    if(debugPrint):\n",
    "        pprint(lda_model.print_topics())\n",
    "        #doc_lda = lda_model[corpus] # TODO: ?\n",
    "\n",
    "    return (lda_model, corpus, id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "param   outputLabel required\n",
    "\"\"\"\n",
    "def ldaToHtml(lda_model, corpus, id2word, outputLabel):\n",
    "\n",
    "    # pyLDAvis.enable_notebook()\n",
    "\n",
    "    LDAvis_data_filepath = os.path.join(dir_var_output + 'pyLDAvis/' + outputLabel + '-data')\n",
    "\n",
    "    # # this is a bit time consuming - make the if statement True\n",
    "    # # if you want to execute visualization prep yourself\n",
    "    \"\"\"\n",
    "    if 1 == 1:\n",
    "        LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "        with open(LDAvis_data_filepath, 'wb') as f:\n",
    "            pickle.dump(LDAvis_prepared, f)\n",
    "\n",
    "    # load the pre-prepared pyLDAvis data from disk\n",
    "    with open(LDAvis_data_filepath, 'rb') as f:\n",
    "        LDAvis_prepared = pickle.load(f)\n",
    "    \"\"\"\n",
    "\n",
    "    LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "\n",
    "    pyLDAvis.save_html(LDAvis_prepared, dir_var_output + 'pyLDAvis/' + outputLabel + '-report.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param outputLabel required\n",
    "def autoLda(df, debugPrint, outputLabel, filterList, listNumberTopics):\n",
    "\n",
    "    for iTopics in listNumberTopics:\n",
    "\n",
    "        iLabel = outputLabel + \"-t-\" + str(iTopics)\n",
    "\n",
    "        gloStartStopwatch(\"Process LDA (\" + str(iTopics) + \" topics) >> \"+ iLabel + \"<<\")\n",
    "\n",
    "        try:\n",
    "            \n",
    "            lda_model, corpus, id2word = processLda(\n",
    "                df = df,\n",
    "                num_topics = iTopics,\n",
    "                debugPrint = debugPrint,\n",
    "                filterList = filterList\n",
    "            )\n",
    "\n",
    "            ldaToHtml(\n",
    "                lda_model = lda_model,\n",
    "                corpus = corpus,\n",
    "                id2word = id2word,\n",
    "                outputLabel = iLabel\n",
    "            )\n",
    "\n",
    "        except:\n",
    "            print(\"Error in process lda\")\n",
    "\n",
    "        gloStopStopwatch(\"Process LDA (\" + str(iTopics) + \" topics) >> \"+ iLabel + \"<<\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    autoLda(\n",
    "        df = dictMessages[\"DS-05-01-2021/ChatExport_2021-01-05-janich\"],\n",
    "        debugPrint = False,\n",
    "        outputLabel = \"oliver-janich\",\n",
    "        filterList = [],\n",
    "        listNumberTopics = [2,4,8,16]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    autoLda(\n",
    "        df = dictMessages[\"DS-05-01-2021/ChatExport_2021-01-05-hildmann\"],\n",
    "        debugPrint = False,\n",
    "        outputLabel = \"attila-hildmann\",\n",
    "        filterList = [],\n",
    "        listNumberTopics = [2,4,8,16]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    autoLda(\n",
    "        df = dictMessages[\"DS-05-01-2021/ChatExport_2021-01-05-evaherman\"],\n",
    "        debugPrint = False,\n",
    "        outputLabel = \"eva-herman\",\n",
    "        filterList = [],\n",
    "        listNumberTopics = [2,4,8,16]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    autoLda(\n",
    "        df = dictMessages[\"DS-05-01-2021/ChatExport_2021-01-05-xavier\"],\n",
    "        debugPrint = False,\n",
    "        outputLabel = \"xavier-naidoo\",\n",
    "        filterList = [],\n",
    "        listNumberTopics = [2,4,8,16]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    if(\"dataSet1a\" in C_LOAD_DATASET):\n",
    "        autoLda(\n",
    "            df = dictMessages[\"DS-05-01-2021a/ChatExport_2021-01-05-freiheitsChat\"],\n",
    "            debugPrint = False,\n",
    "            outputLabel = \"group-freiheitsChat\",\n",
    "            filterList = [],\n",
    "            listNumberTopics = [2,4,8,16,32]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    if(\"dataSet1a\" in C_LOAD_DATASET):\n",
    "        autoLda(\n",
    "            df = dictMessages[\"DS-05-01-2021a/ChatExport_2021-01-05-freiheitsChatBlitz\"],\n",
    "            debugPrint = False,\n",
    "            outputLabel = \"group-freiheitsChatBlitz\",\n",
    "            filterList = [],\n",
    "            listNumberTopics = [2,4,8,16,32]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    if(\"dataSet1a\" in C_LOAD_DATASET):\n",
    "        autoLda(\n",
    "            df = dictMessages[\"DS-05-01-2021a/ChatExport_2021-01-05-liveFuerDeOsSc\"],\n",
    "            debugPrint = False,\n",
    "            outputLabel = \"group-liveFuerDeOsSc\",\n",
    "            filterList = [],\n",
    "            listNumberTopics = [2,4,8,16,32]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tracer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.bundestag.de/parlament/plenum/sitzverteilung_19wp\n",
    "highlightwords = [\"cdu\", \"spd\", \"afd\", \"fdp\", \"linke\", \"gruenen\", \"merkel\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    drawTimePlot(\n",
    "        filePathList = list([\"DS-05-01-2021/ChatExport_2021-01-05-janich\"]),\n",
    "        outputFilename = \"word-tracer-oliver-janich.png\",\n",
    "        highlightWords = highlightwords,\n",
    "        configFrequency=C_TIME_PLOT_FREQ\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    drawTimePlot(\n",
    "        filePathList = list([\"DS-05-01-2021/ChatExport_2021-01-05-hildmann\"]),\n",
    "        outputFilename = \"word-tracer-attila-hildmann.png\",\n",
    "        highlightWords = highlightwords,\n",
    "        configFrequency=C_TIME_PLOT_FREQ\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    drawTimePlot(\n",
    "        filePathList = list([\"DS-05-01-2021/ChatExport_2021-01-05-evaherman\"]),\n",
    "        outputFilename = \"word-tracer-eva-herman.png\",\n",
    "        highlightWords = highlightwords,\n",
    "        configFrequency=C_TIME_PLOT_FREQ\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    drawTimePlot(\n",
    "        filePathList = list([\"DS-05-01-2021/ChatExport_2021-01-05-xavier\"]),\n",
    "        outputFilename = \"word-tracer-xavier-naidoo.png\",\n",
    "        highlightWords = highlightwords,\n",
    "        configFrequency=C_TIME_PLOT_FREQ\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gloStopStopwatch(\"Global notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next Steps\n",
    "\"\"\"\n",
    "- Add freq words to word tracer\n",
    "- Python NatrualLanguage Toolkit tools?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsai.net/p/data-mining/text-mining-in-python-steps-and-examples-78b3f8fd913b\n",
    "\"\"\"\n",
    "- Concordance (and Kookkurrenz and Correl?)\n",
    "- Tokenization\n",
    "- Finding frequency distinct in the text\n",
    "- Stemming\n",
    "- Lemmatization\n",
    "- Stop words?\n",
    "- Part of speech tagging (POS)\n",
    "- Named entity recognition\n",
    "- Chunking\n",
    "\"\"\"\n",
    "\n",
    "# https://towardsdatascience.com/text-mining-for-dummies-text-classification-with-python-98e47c3a9deb\n",
    "\"\"\"\n",
    "- Sentiment Analysis\n",
    "\"\"\"\n",
    "\n",
    "# https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-deal-text-data-predictive-python/\n",
    "\"\"\"\n",
    "- Features: (Number of words, Number of characters, Average word length, Number of stopwords, Number of special characters, Number of nummberics, Number of uppercase words)\n",
    "- Pre-processing (Lower casing, Punctuation removal, Stopwrods removal, Frequent word removal, Rare words removal, Spelling correction, Tokenization, Stemming, Lemmatization)\n",
    "- Adv-processing (N-grams, Term Frequency, Inverse Document Frequency, Term Frequency-Inverse Document Frequency 'TF-IDF', Bag of words, Sentiment Analysis, Word Embedding)\n",
    "\"\"\"\n",
    "\n",
    "# https://realpython.com/python-keras-text-classification/\n",
    "\"\"\"\n",
    "- Text Analysis with Keras\n",
    "\"\"\"\n",
    "\n",
    "# https://www.tidytextmining.com/ngrams.html\n",
    "\"\"\"\n",
    "- Relationships between words: n-grams and correlations\n",
    "\"\"\"\n",
    "\n",
    "# http://seaborn.pydata.org/tutorial/categorical.html?highlight=bar%20plot\n",
    "\"\"\"\n",
    "- Plotting with categorical data (Box Plot)\n",
    "\"\"\"\n",
    "\n",
    "# https://towardsdatascience.com/visualizing-data-with-pair-plots-in-python-f228cf529166\n",
    "\"\"\"\n",
    "- Visualizing Data with Pairs Plots in Python\n",
    "\"\"\"\n",
    "\n",
    "# https://www.kirenz.com/post/2019-08-13-network_analysis/\n",
    "\"\"\"\n",
    "- Social Network Analysis with Python\n",
    "\"\"\"\n",
    "\n",
    "# https://tgstat.com\n",
    "\"\"\"\n",
    "- Compare website with my analyse\n",
    "\"\"\"\n",
    "\n",
    "# https://huggingface.co/bert-base-german-cased\n",
    "\"\"\"\n",
    "- Language Model Bert German\n",
    "\"\"\"\n",
    "\n",
    "# https://github.com/sekhansen/text-mining-tutorial/tree/master\n",
    "\"\"\"\n",
    "- An Introduction to Topic Modelling via Gibbs sampling\n",
    "\"\"\"\n",
    "\n",
    "# https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
    "\"\"\"\n",
    "- Topic Modeling with Gensim (Python)\n",
    "\"\"\"\n",
    "\n",
    "# https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0\n",
    "\"\"\"\n",
    "- Topic Modeling in Python: Latent Dirichlet Allocation (LDA)\n",
    "\"\"\"\n",
    "\n",
    "# https://github.com/sekhansen/text-mining-tutorial/blob/master/tutorial_notebook.ipynb\n",
    "\"\"\"\n",
    "- Text Mining Python Tutorial\n",
    "\"\"\"\n",
    "\n",
    "# https://textmining.wp.hs-hannover.de/Preprocessing.html\n",
    "\"\"\"\n",
    "- Preprocessing samples\n",
    "\"\"\"\n",
    "\n",
    "# https://likegeeks.com/nlp-tutorial-using-python-nltk/\n",
    "\"\"\"\n",
    "- NLTK and NLP Tutorial\n",
    "\"\"\"\n",
    "\n",
    "# https://www.datacamp.com/community/tutorials/text-analytics-beginners-nltk\n",
    "\"\"\"\n",
    "- Another NLTK Tutorial\n",
    "\"\"\"\n",
    "\n",
    "# https://data-flair.training/blogs/nltk-python-tutorial/\n",
    "\"\"\"\n",
    "- NLTK Overview\n",
    "\"\"\"\n",
    "\n",
    "# https://github.com/expectocode/telegram-analysis\n",
    "\"\"\"\n",
    "- Telegram Python Sample\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}