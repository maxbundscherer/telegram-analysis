{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telegram Analyse (Master-Thesis)\n",
    "\n",
    "Informatik Master\n",
    "\n",
    "Maximilian Bundscherer\n",
    "\n",
    "Beschreibung tbd.\n",
    "\n",
    "## Umgebung starten und Konfigurationen anwenden\n",
    "\n",
    "### Parameter mit Konfigurationen\n",
    "\n",
    "Die Läufe lassen sich mit diesen Parametern beinflussen:\n",
    "\n",
    "| Bezeichner | Datentyp | Beispiel | Beschreibung |\n",
    "|---|---|---|---|\n",
    "|``C_LOCAL``|``bool``|``True``|Setzte auf ``True``, falls eine externe Verbindung zum Kernel besteht. Setzte auf ``False``, falls im Browser gearbeitet wird. Beeinflusst Pfade Arbeitsverzeichnisse.|\n",
    "|``C_SHORT_RUN``|``bool``|``False``|Setzte auf ``True``, falls ein reduzierter Lauf durchgeführt werden soll. Ver- kürzt Entwicklungszeiten lokal.|\n",
    "|``C_NUMBER_SAMPLES``|``int``|``1000``|Falls ``C_SHORT_RUN`` auf ``True`` gesetzt ist gültig. Um die Entwicklungszeiten weiter zu verkürzen, kann nur auf ei- nem Teil der Datenmenge operiert wer- den.|\n",
    "|``C_RESOLVE_NEW_URLS``|``bool``|``True``|Sollen YouTube-Titel und Webseiten- Titel während dieses Laufs aufgelöst werden?|\n",
    "|``C_LOAD_DATASETS``|``string[]``|``[\"dataSet0\"]``|Welche DateSets sollen geladen werden?|\n",
    "|``C_LOAD_PIPELINES``|``bool``|``True``|Definiert ob die Transformers geladen werden sollen. Die Läufe berücksichtigen das, da es lange Laden kann.|\n",
    "|``C_PIPELINE_DATASETS``|``string[]``|``[\"dataSet0\"]``|Falls ``C_LOAD_PIPELINES`` auf ``True`` gesetzt ist gültig. Definiert welche DateSets Trans- formers angewendet werden.|\n",
    "|``C_TIME_PLOT_FREQ``|``string``|``1D``|Definiert Zeitspanne weiter unten|\n",
    "|``C_USE_CACHE_FILE``|``string``|``file.pkl``|Setzten falls neuer DataFrame-Cache erzeugt werden soll. Definiert Dateinamen|\n",
    "|``C_NEW_CACHE_FILE``|``string``|``file.pkl``|Setzten falls besteher DataFrame- Cache verwendet werden soll. Defi- niert Dateinamen.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_LOCAL                 = False\n",
    "\n",
    "C_SHORT_RUN             = False\n",
    "C_NUMBER_SAMPLES        = 1000\n",
    "\n",
    "C_RESOLVE_NEW_URLS      = False\n",
    "\n",
    "\"\"\"\n",
    "Ava:    [\"dataSet0\", \"dataSet1\", \"dataSet1a\", \"dataSet2\"]\n",
    "Htdocs: [\"dataSet0\", \"dataSet1a\", \"dataSet2\"]\n",
    "Req:    [\"dataSet0\"]\n",
    "\"\"\"\n",
    "C_LOAD_DATASETS         = [\"dataSet0\"]\n",
    "\n",
    "C_LOAD_PIPELINES        = True\n",
    "C_PIPELINE_DATASETS     = [\"dataSet0\"]\n",
    "\n",
    "C_TIME_PLOT_FREQ        = \"1D\"\n",
    "\n",
    "\"\"\"\n",
    "Please set only one value!\n",
    "e.g.\n",
    "# - long-run-server-28-01.pkl   (Long run, with hf, with htdocs-datasets, updated with sen-pipe-2)\n",
    "# - long-run-server-07-02.pkl   (Long run, with hf, with all datasets, updated with sen-pipe-2)\n",
    "# - local-run-28-01.pkl         (Short run, with hf, with htdocs-datasets, updated with sen-pipe-2)\n",
    "# - test.pkl                    (Test file)\n",
    "\"\"\"\n",
    "C_USE_CACHE_FILE        = \"long-run-server-07-02.pkl\"\n",
    "C_NEW_CACHE_FILE        = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliotheken und Abhängigkeiten laden\n",
    "\n",
    "#### Laden von Abhängigkeiten\n",
    "\n",
    "##### Abhänigkeiten vom Docker-Image und IO-Libs und weitere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import default libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import demjson\n",
    "import requests\n",
    "import networkx as nx\n",
    "import warnings\n",
    "from pprint import pprint\n",
    "from urllib.parse import urlparse\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from lxml.html import fromstring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Weitere Abhänigkeiten installieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install demoji\n",
    "!{sys.executable} -m pip install HanTa\n",
    "!{sys.executable} -m pip install textblob-de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Weitere Abhängigkeiten "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import demoji\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "from HanTa import HanoverTagger as ht\n",
    "\n",
    "from textblob_de import TextBlobDE as TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stopuhr bereitstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictGloStopwatches = dict()\n",
    "\n",
    "# Start timer (for reporting)\n",
    "def gloStartStopwatch(key):\n",
    "    print(\"[Stopwatch started >>\" + str(key) + \"<<]\")\n",
    "    dictGloStopwatches[key] = time.time()\n",
    "\n",
    "# Stop timer (for reporting)\n",
    "def gloStopStopwatch(key):\n",
    "    endTime     = time.time()\n",
    "    startTime   = dictGloStopwatches[key]\n",
    "    print(\"[Stopwatch stopped >>\" + str(key) + \"<< (\" + '{:5.3f}s'.format(endTime-startTime) + \")]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download von Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transfomers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictPipelines = {}\n",
    "\n",
    "def loadPipelines():\n",
    "\n",
    "    if(C_LOAD_PIPELINES == False):\n",
    "        print(\"Skip loading pipelines\")\n",
    "        return list()\n",
    "\n",
    "    gloStartStopwatch(\"Load Pipelines\")\n",
    "    \n",
    "\n",
    "    gloStartStopwatch(\"Load ner-xlm-Roberta\")\n",
    "    dictPipelines[\"ner-xlm-roberta\"] = pipeline(\n",
    "        'ner', \n",
    "        model='xlm-roberta-large-finetuned-conll03-german',\n",
    "        tokenizer='xlm-roberta-large-finetuned-conll03-german'\n",
    "    )\n",
    "    gloStopStopwatch(\"Load ner-xlm-Roberta\")\n",
    "\n",
    "    gloStartStopwatch(\"Load ner-Bert\")\n",
    "    dictPipelines[\"ner-bert\"] = pipeline(\n",
    "        'ner', \n",
    "        model='fhswf/bert_de_ner',\n",
    "        tokenizer='fhswf/bert_de_ner'\n",
    "    )\n",
    "    gloStopStopwatch(\"Load ner-Bert\")\n",
    "\n",
    "    gloStartStopwatch(\"Load sen-Bert\")\n",
    "    dictPipelines[\"sen-bert\"] = pipeline(\n",
    "        'sentiment-analysis', \n",
    "        model='nlptown/bert-base-multilingual-uncased-sentiment',\n",
    "        tokenizer='nlptown/bert-base-multilingual-uncased-sentiment'\n",
    "    )\n",
    "    gloStopStopwatch(\"Load sen-Bert\")\n",
    "\n",
    "    gloStartStopwatch(\"Load text-gen-gpt2\")\n",
    "    dictPipelines[\"text-gen-gpt2\"] = pipeline(\n",
    "        'text-generation', \n",
    "        model='dbmdz/german-gpt2',\n",
    "        tokenizer='dbmdz/german-gpt2'\n",
    "    )\n",
    "    gloStopStopwatch(\"Load text-gen-gpt2\")\n",
    "\n",
    "    gloStartStopwatch(\"Load text-gen-gpt2-faust\")\n",
    "    dictPipelines[\"text-gen-gpt2-faust\"] = pipeline(\n",
    "        'text-generation', \n",
    "        model='dbmdz/german-gpt2-faust',\n",
    "        tokenizer='dbmdz/german-gpt2-faust'\n",
    "    )\n",
    "    gloStopStopwatch(\"Load text-gen-gpt2-faust\")\n",
    "\n",
    "\n",
    "    gloStopStopwatch(\"Load Pipelines\")\n",
    "\n",
    "    return dictPipelines.keys()\n",
    "\n",
    "pipelineKeys = loadPipelines()\n",
    "\n",
    "print()\n",
    "print(str(pipelineKeys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk download\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Demoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demoji.download_codes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Umgebung konfigurieren und vorbereiten\n",
    "\n",
    "#### Konfigurationen Umgebung anwenden\n",
    "\n",
    "##### IO Einstellungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hide DeprecationWarning\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.simplefilter(\"once\")\n",
    "\n",
    "# Set tokenizer parallelism \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# matplotlib output\n",
    "%matplotlib inline\n",
    "\n",
    "# Show all columns (pandas hides columns by default)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Arbeitsverzeichnis definieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set env vars\n",
    "if(C_LOCAL == True):\n",
    "    dir_var = \"./work/notebooks/\"\n",
    "else:\n",
    "    dir_var = \"./\"\n",
    "\n",
    "dir_var_output = dir_var + \"output/\"\n",
    "\n",
    "dir_var_cache= dir_var + \"cache/\"\n",
    "\n",
    "dir_var_pandas_cache = dir_var + \"cache/pandas/\"\n",
    "\n",
    "# Debug output\n",
    "! echo \"- Workdir -\"\n",
    "! ls -al $dir_var\n",
    "\n",
    "! echo\n",
    "! echo \"- Outputdir -\"\n",
    "! ls -al $dir_var_output\n",
    "\n",
    "! echo\n",
    "! echo \"- Cachedir -\"\n",
    "! ls -al $dir_var_cache\n",
    "\n",
    "! echo\n",
    "! echo \"- Pandas -\"\n",
    "! ls -al $dir_var_pandas_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Globale Text-Normalisierungsfunktionen definieren\n",
    "\n",
    "##### Handelt es sich um einen JSON formatierten String?\n",
    "\n",
    "Unterstützt singleMode und multiMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gloCheckIsTextJsonFormatted(text, singleMode):\n",
    "    textString = str(text)\n",
    "    if      (singleMode == False and textString.startswith(\"[\") == True and textString.endswith(\"]\") == True):\n",
    "        return True\n",
    "    elif    (singleMode == True and textString.startswith(\"{\") == True and textString.endswith(\"}\") == True):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deutsche Umlaute ersetzten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gloReplaceGermanChars(inputText):\n",
    "\n",
    "    inputText = inputText.replace(\"ö\", \"oe\")\n",
    "    inputText = inputText.replace(\"ü\", \"ue\")\n",
    "    inputText = inputText.replace(\"ä\", \"ae\")\n",
    "\n",
    "    inputText = inputText.replace(\"Ö\", \"Oe\")\n",
    "    inputText = inputText.replace(\"Ü\", \"Ue\")\n",
    "    inputText = inputText.replace(\"Ä\", \"Ae\")\n",
    "\n",
    "    inputText = inputText.replace(\"ß\", \"ss\")\n",
    "    \n",
    "    return inputText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sonderzeichen entfernen und Chat Titel normalisieren\n",
    "\n",
    "von Text und Chat-Titel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rm unsafe chars\n",
    "def gloConvertToSafeString(text):\n",
    "    text = demoji.replace(text, \"\")\n",
    "    text = gloReplaceGermanChars(text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# Generate unique chat name\n",
    "def gloConvertToSafeChatName(chatName):\n",
    "    chatName = gloConvertToSafeString(chatName)\n",
    "    return chatName[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bereitstellen von Stop Words Datenbanken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gloGetStopWordsList(filterList):\n",
    "\n",
    "    stopwWorldsList = []\n",
    "\n",
    "    deWordsList = nltk.corpus.stopwords.words('german')\n",
    "\n",
    "    enWordsList = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    aStopwords = []\n",
    "    with open(dir_var + \"additionalStopwords.txt\") as file:\n",
    "        for line in file: \n",
    "            line = line.strip()\n",
    "            if(line != \"\"):\n",
    "                aStopwords.append(line)\n",
    "\n",
    "    for s in filterList:\n",
    "        s = gloReplaceGermanChars(s)\n",
    "        stopwWorldsList.append(s)\n",
    "\n",
    "    for s in deWordsList:\n",
    "        s = gloReplaceGermanChars(s)\n",
    "        stopwWorldsList.append(s)\n",
    "\n",
    "    for s in enWordsList:\n",
    "        stopwWorldsList.append(s)\n",
    "\n",
    "    for s in aStopwords:\n",
    "        s = gloReplaceGermanChars(s)\n",
    "        stopwWorldsList.append(s)\n",
    "\n",
    "    return stopwWorldsList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gesamten Text vom einem Chat extrahieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param rowID e.g. procTDSafeText\n",
    "def gloGenerateTextFromChat(df, rowID):\n",
    "    df = df.copy()\n",
    "    df = df[df.procEvalIsValidText == True]\n",
    "    \n",
    "    # Iterate over text (global text from group)\n",
    "    textList = []\n",
    "    for index, row in df.iterrows():\n",
    "        textList.append(\" \" + row[rowID])\n",
    "        \n",
    "    textString = ''.join(textList)\n",
    "\n",
    "    return textString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Token / Lemma / POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: https://github.com/wartaal/HanTa/blob/master/Demo.ipynb\n",
    "hanoverTagger = ht.HanoverTagger('morphmodel_ger.pgz')\n",
    "\n",
    "def getTokenFromText(inputText):\n",
    "    return nltk.word_tokenize(inputText, language=\"german\")\n",
    "\n",
    "def getLemmaAndTaggingFromText(inputText):\n",
    "    return hanoverTagger.tag_sent(getTokenFromText(inputText))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Freq Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param outputFilename, if \"\" - no output\n",
    "def plotFreqNouns(inputText, outputFilename, mostCommon, flagRemoveStopwords):\n",
    "    # https://textmining.wp.hs-hannover.de/Preprocessing.html\n",
    "    nouns = []\n",
    "    sentences_tok = [nltk.tokenize.word_tokenize(sent) for sent in getTokenFromText(inputText)]\n",
    "\n",
    "    for sent in sentences_tok:\n",
    "        tags = hanoverTagger.tag_sent(sent) \n",
    "        nouns_from_sent = [lemma for (word,lemma,pos) in tags if pos == \"NN\" or pos == \"NE\"]\n",
    "        nouns.extend(nouns_from_sent)\n",
    "\n",
    "    pNouns = list()\n",
    "\n",
    "    if(flagRemoveStopwords):\n",
    "\n",
    "        print(\"- Warn: remove stopWords\")\n",
    "        stopWords = gloGetStopWordsList(filterList = list())\n",
    "        for n in nouns:\n",
    "            if n.lower() not in stopWords:\n",
    "                pNouns.append(n)\n",
    "\n",
    "    else:\n",
    "        pNouns = nouns\n",
    "\n",
    "    # Thank you https://stackoverflow.com/questions/52908305/how-to-save-a-nltk-freqdist-plot\n",
    "    fig = plt.figure(figsize = (16,9))\n",
    "    plt.gcf().subplots_adjust(bottom=0.15)\n",
    "\n",
    "    fdist = nltk.FreqDist(pNouns)    \n",
    "\n",
    "    fdist.plot(mostCommon,cumulative=False)\n",
    "\n",
    "    _ = plt.show()\n",
    "\n",
    "    if(outputFilename != \"\"):\n",
    "        fig.savefig(dir_var_output + outputFilename, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datei Cache vorbereiten\n",
    "\n",
    "##### Cache-IO Funktionen\n",
    "\n",
    "- toFile\n",
    "- fromFile\n",
    "- initFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict File Cache\n",
    "dictFileCache = {}\n",
    "\n",
    "# Write dict to file (CSV)\n",
    "def gloWriteDictToFile(filename, targetDict):\n",
    "    dictFileCache = {} #Clear cache\n",
    "    d = pd.DataFrame.from_dict(targetDict, orient=\"index\")\n",
    "    d.to_csv(dir_var_cache + filename, header=False)\n",
    "\n",
    "# Read dict from file (CSV)\n",
    "def gloReadDictFromFile(filename):\n",
    "    # Cache?\n",
    "    if(filename in dictFileCache):\n",
    "        return dictFileCache[filename]\n",
    "\n",
    "    d = pd.read_csv(dir_var_cache + filename, header=None, index_col=0, squeeze=True)\n",
    "    retDict = d.to_dict()\n",
    "\n",
    "    dictFileCache[filename] = retDict #Add to cache\n",
    "\n",
    "    return retDict\n",
    "\n",
    "# Init csv file if not exists\n",
    "def gloInitFileDict(filename):\n",
    "    f = Path(dir_var_cache + filename)\n",
    "    if(f.exists() == False):\n",
    "        print(\"Init cache file >>\" + filename + \"<<\")\n",
    "        f.touch()\n",
    "        gloWriteDictToFile(filename, {\"initKey\": \"initValue\"})\n",
    "    else:\n",
    "        print(\"Cache already exists >>\" + filename + \"<<\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cache Funktionen\n",
    "\n",
    "- checkIsCached\n",
    "- addToCache\n",
    "- getFromCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if is already cached\n",
    "def gloCheckIsAlreadyCached(filename, targetKey):\n",
    "    targetDict = gloReadDictFromFile(filename)\n",
    "    if(targetKey in targetDict.keys()):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Add key to cache\n",
    "def gloAddToCache(filename, targetKey, targetValue):\n",
    "    targetDict = gloReadDictFromFile(filename)\n",
    "    targetDict[targetKey] = targetValue\n",
    "    gloWriteDictToFile(filename, targetDict)\n",
    "\n",
    "# Get key from cache\n",
    "def gloGetCached(filename, targetKey):\n",
    "    targetDict = gloReadDictFromFile(filename)\n",
    "    return targetDict[targetKey]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cache-IO init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gloInitFileDict(\"resolved-urls.csv\")\n",
    "gloInitFileDict(\"resolved-youtube.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einführende Beispiele\n",
    "\n",
    "Beispielstext definieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ich denke an Eis in München. Das ist ein guter Beispielstext aus München. An diesem tollen Text werde ich nun einige Verfahren anwenden! Ich wohne in der Nähe von München und esse gerne Eis.'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampleText = \"Ich denke an Eis in München. Das ist ein guter Beispielstext aus München. An diesem tollen Text werde ich nun einige Verfahren anwenden! Ich wohne in der Nähe von München und esse gerne Eis.\"\n",
    "sampleText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token\n",
    "NLTK German Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ich', 'denke', 'an', 'Eis', 'in', 'München', '.', 'Das', 'ist', 'ein']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(getTokenFromText(sampleText))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Versuch mit NLTK Englisch\n",
    "\n",
    "1. NLTK German Token\n",
    "2. Englische Sprache (NLTK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ich', 'NNP'),\n",
       " ('denke', 'VBD'),\n",
       " ('an', 'DT'),\n",
       " ('Eis', 'NNP'),\n",
       " ('in', 'IN'),\n",
       " ('München', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('Das', 'NNP'),\n",
       " ('ist', 'JJ'),\n",
       " ('ein', 'NN'),\n",
       " ('guter', 'NN'),\n",
       " ('Beispielstext', 'NNP'),\n",
       " ('aus', 'NN'),\n",
       " ('München', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('An', 'DT'),\n",
       " ('diesem', 'JJ'),\n",
       " ('tollen', 'NN'),\n",
       " ('Text', 'NNP'),\n",
       " ('werde', 'NN'),\n",
       " ('ich', 'NN'),\n",
       " ('nun', 'FW'),\n",
       " ('einige', 'FW'),\n",
       " ('Verfahren', 'NNP'),\n",
       " ('anwenden', 'NN'),\n",
       " ('!', '.'),\n",
       " ('Ich', 'NNP'),\n",
       " ('wohne', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('der', 'NN'),\n",
       " ('Nähe', 'NNP'),\n",
       " ('von', 'NNP'),\n",
       " ('München', 'NNP'),\n",
       " ('und', 'IN'),\n",
       " ('esse', 'FW'),\n",
       " ('gerne', 'JJ'),\n",
       " ('Eis', 'NNP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(getTokenFromText(sampleText))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemma and POS HanTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ich', 'ich', 'PPER'),\n",
       " ('denke', 'denken', 'VVFIN'),\n",
       " ('an', 'an', 'APPR'),\n",
       " ('Eis', 'Eis', 'NN'),\n",
       " ('in', 'in', 'APPR'),\n",
       " ('München', 'München', 'NE'),\n",
       " ('.', '--', '$.'),\n",
       " ('Das', 'das', 'PDS'),\n",
       " ('ist', 'sein', 'VAFIN'),\n",
       " ('ein', 'ein', 'ART'),\n",
       " ('guter', 'gut', 'ADJA'),\n",
       " ('Beispielstext', 'Beispielstext', 'NN'),\n",
       " ('aus', 'aus', 'APPR'),\n",
       " ('München', 'München', 'NE'),\n",
       " ('.', '--', '$.'),\n",
       " ('An', 'an', 'APPR'),\n",
       " ('diesem', 'diesem', 'PDAT'),\n",
       " ('tollen', 'toll', 'ADJA'),\n",
       " ('Text', 'Text', 'NN'),\n",
       " ('werde', 'werden', 'VAFIN'),\n",
       " ('ich', 'ich', 'PPER'),\n",
       " ('nun', 'nun', 'ADV'),\n",
       " ('einige', 'einige', 'PIAT'),\n",
       " ('Verfahren', 'Verfahren', 'NN'),\n",
       " ('anwenden', 'anwenden', 'VVINF'),\n",
       " ('!', '--', '$.'),\n",
       " ('Ich', 'ich', 'PPER'),\n",
       " ('wohne', 'wohnen', 'VVFIN'),\n",
       " ('in', 'in', 'APPR'),\n",
       " ('der', 'der', 'ART'),\n",
       " ('Nähe', 'Nähe', 'NN'),\n",
       " ('von', 'von', 'APPR'),\n",
       " ('München', 'München', 'NE'),\n",
       " ('und', 'und', 'KON'),\n",
       " ('esse', 'essen', 'VVFIN'),\n",
       " ('gerne', 'gerne', 'ADV'),\n",
       " ('Eis', 'Eis', 'NN'),\n",
       " ('.', '--', '$.')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getLemmaAndTaggingFromText(sampleText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freq Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Warn: remove stopWords\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x648 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotFreqNouns(sampleText, outputFilename = \"\", mostCommon = 10, flagRemoveStopwords = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chats laden und aufbereiten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stufe 1 Parsen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JSON Helfer\n",
    "\n",
    "##### zu Data Frame Meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame Meta (Chat Meta)\n",
    "def convertToDataFrameMeta(filePath):\n",
    "    dF = pd.read_json(dir_var + \"data/\" + filePath + \"/result.json\", encoding='utf-8')\n",
    "    return dF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### zu Data Frame Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame Messages (Chat Messages)\n",
    "def convertToDataFrameMessages(filePath):\n",
    "    dF = pd.json_normalize(dictMeta[filePath].messages)\n",
    "    return dF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stufe 2 Text und Text Meta Daten Extrahieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract text data (see cell above key)\n",
    "See cell above (key)\n",
    "\n",
    "param   procIsJsonFormatted Boolean (is text json formatted?)\n",
    "param   text                String  (text from message) \n",
    "\n",
    "return\n",
    "a   procText            Plain Text\n",
    "b   processedURLs       Array of URLs in Text\n",
    "c   processedHashtags   Array of Hashtags in Text #TODO: RM\n",
    "d   processedBolds      Array of Bold Items in Text\n",
    "e   processedItalics    Array of Italic Items in Text\n",
    "f   processedUnderlines Array of Underlined Items in Text\n",
    "g   processedEmails     Array of E-Mails in Text\n",
    "\"\"\"\n",
    "def extractTextData(procIsJsonFormatted, text):\n",
    "    \n",
    "    # 3 returns in this function...\n",
    "    \n",
    "    processedURLs       = list()\n",
    "    processedHashtags   = list() # TODO: RM\n",
    "    processedBolds      = list()\n",
    "    processedItalics    = list()\n",
    "    processedUnderlines = list()\n",
    "    processedEmails     = list()\n",
    "    \n",
    "    if(procIsJsonFormatted != True):\n",
    "        #Is not JSON formatted (return normal text)\n",
    "        return (text, processedURLs, processedHashtags, processedBolds, processedItalics, processedUnderlines, processedEmails)\n",
    "    else:\n",
    "        #Is is JSON formatted (try to parse)\n",
    "        try:\n",
    "            returnList = []\n",
    "            jsonList = demjson.decode(str(text), encoding='utf8')\n",
    "\n",
    "            # Do for each item in list\n",
    "            for lItem in jsonList:\n",
    "\n",
    "                messageString = str(lItem)\n",
    "\n",
    "                isJsonSubString = gloCheckIsTextJsonFormatted(messageString, singleMode = True)\n",
    "\n",
    "                if(isJsonSubString):\n",
    "                    # Is Json Sub String\n",
    "                    subJsonString = demjson.decode(str(messageString), encoding='utf8')\n",
    "                    subJsonType = subJsonString[\"type\"]\n",
    "\n",
    "                    if(subJsonType == \"bold\"):\n",
    "                        #text included\n",
    "                        processedBolds.append(subJsonString[\"text\"])\n",
    "                        returnList.append(subJsonString[\"text\"])\n",
    "                        \n",
    "                    elif(subJsonType == \"italic\"):\n",
    "                        #text included\n",
    "                        processedItalics.append(subJsonString[\"text\"])\n",
    "                        returnList.append(subJsonString[\"text\"])\n",
    "                        \n",
    "                    elif(subJsonType == \"underline\"):\n",
    "                        #text included\n",
    "                        processedUnderlines.append(subJsonString[\"text\"])\n",
    "                        returnList.append(subJsonString[\"text\"])\n",
    "                    \n",
    "                    elif(subJsonType == \"email\"):\n",
    "                        #text included\n",
    "                        processedEmails.append(subJsonString[\"text\"])\n",
    "                        \n",
    "                    elif(subJsonType == \"text_link\"):\n",
    "                        #text and href included\n",
    "                        processedURLs.append(subJsonString[\"href\"])\n",
    "                        #returnList.append(subJsonString[\"text\"])\n",
    "                        \n",
    "                    elif(subJsonType == \"link\"):\n",
    "                        #text included\n",
    "                        processedURLs.append(subJsonString[\"text\"])\n",
    "                        \n",
    "                    elif(subJsonType == \"hashtag\"):\n",
    "                        #text included\n",
    "                        #processedHashtags.append(subJsonString[\"text\"]) # TODO: Refactor: Dont add hashtags here!\n",
    "                        returnList.append(subJsonString[\"text\"])\n",
    "                        \n",
    "                    elif(subJsonType == \"mention\"):\n",
    "                        #text included\n",
    "                        returnList.append(subJsonString[\"text\"])\n",
    "                        \n",
    "                    elif(subJsonType == \"mention_name\"):\n",
    "                        #text and user_id included\n",
    "                        returnList.append(subJsonString[\"text\"])\n",
    "                        \n",
    "                    elif(subJsonType == \"bot_command\"):\n",
    "                        #text included\n",
    "                        returnList = returnList \n",
    "                        \n",
    "                    elif(subJsonType == \"code\"):\n",
    "                        #text included\n",
    "                        returnList = returnList\n",
    "                        \n",
    "                    elif(subJsonType == \"phone\"):\n",
    "                        #text included\n",
    "                        returnList = returnList\n",
    "                        \n",
    "                    elif(subJsonType == \"strikethrough\"):\n",
    "                        #text included\n",
    "                        returnList.append(subJsonString[\"text\"])\n",
    "                        \n",
    "                    elif(subJsonType == \"pre\"):\n",
    "                        #text and language included\n",
    "                        returnList.append(subJsonString[\"text\"])\n",
    "                        \n",
    "                    elif(subJsonType == \"bank_card\"):\n",
    "                        #text included\n",
    "                        returnList = returnList\n",
    "                        \n",
    "                    else:\n",
    "                        print(\"- Error: Unkown json type >>\" + str(subJsonType) + \"<< (ignore) >>\" + str(text) + \"<<\")\n",
    "\n",
    "                else:\n",
    "                    # Is no json formatted sub string (append text)\n",
    "                    returnList.append(messageString)\n",
    "\n",
    "            return (''.join(returnList), processedURLs, processedHashtags, processedBolds, processedItalics, processedUnderlines, processedEmails)\n",
    "        \n",
    "        except:\n",
    "            # Parser error (set inputText to returnText)\n",
    "            print(\"- Warn: Json parser error (set inputText to returnText) >>\" + str(text) + \"<<\")\n",
    "            return (text, processedURLs, processedHashtags, processedBolds, processedItalics, processedUnderlines, processedEmails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text und Text Meta Daten Extrahieren (Stufe 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Werkzeuge definieren\n",
    "\n",
    "##### Url\n",
    "\n",
    "- getUrlRegex\n",
    "- extractUrls\n",
    "- removeUrls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/6718633/python-regular-expression-again-match-url\n",
    "def getUrlRegex():\n",
    "    return \"((?:https?://)?(?:(?:www\\.)?(?:[\\da-z\\.-]+)\\.(?:[a-z]{2,6})|(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)|(?:(?:[0-9a-fA-F]{1,4}:){7,7}[0-9a-fA-F]{1,4}|(?:[0-9a-fA-F]{1,4}:){1,7}:|(?:[0-9a-fA-F]{1,4}:){1,6}:[0-9a-fA-F]{1,4}|(?:[0-9a-fA-F]{1,4}:){1,5}(?::[0-9a-fA-F]{1,4}){1,2}|(?:[0-9a-fA-F]{1,4}:){1,4}(?::[0-9a-fA-F]{1,4}){1,3}|(?:[0-9a-fA-F]{1,4}:){1,3}(?::[0-9a-fA-F]{1,4}){1,4}|(?:[0-9a-fA-F]{1,4}:){1,2}(?::[0-9a-fA-F]{1,4}){1,5}|[0-9a-fA-F]{1,4}:(?:(?::[0-9a-fA-F]{1,4}){1,6})|:(?:(?::[0-9a-fA-F]{1,4}){1,7}|:)|fe80:(?::[0-9a-fA-F]{0,4}){0,4}%[0-9a-zA-Z]{1,}|::(?:ffff(?::0{1,4}){0,1}:){0,1}(?:(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])|(?:[0-9a-fA-F]{1,4}:){1,4}:(?:(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])))(?::[0-9]{1,4}|[1-5][0-9]{4}|6[0-4][0-9]{3}|65[0-4][0-9]{2}|655[0-2][0-9]|6553[0-5])?(?:/[\\w\\.-]*)*/?)\"\n",
    "\n",
    "def urlExtractUrls(inputText):\n",
    "    return re.findall(getUrlRegex(), str(inputText))\n",
    "\n",
    "def urlRemoveUrls(inputText):\n",
    "    return re.sub(getUrlRegex(), \" \", str(inputText))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hashtags\n",
    "\n",
    "- getHashtagRegex\n",
    "- extractHashTags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHashtagRegex():\n",
    "    return \"#(\\w+)\"\n",
    "\n",
    "def hashTagExtractHashTags(inputText):\n",
    "\n",
    "    inputText = str(inputText)\n",
    "\n",
    "    inputText = re.sub('\\n', ' ', inputText) # Replace \\n\n",
    "    inputText = demoji.replace(inputText, \" \") # Rm emoji\n",
    "    inputText = gloReplaceGermanChars(inputText) # Replace german chars\n",
    "\n",
    "    return re.findall(getHashtagRegex(), inputText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get params from extractedTextData\n",
    "See cell below (key)\n",
    "\"\"\"\n",
    "def getExtractedTextDataParam(key, extractedTextData):\n",
    "\n",
    "    a,b,c,d,e,f,g = extractedTextData\n",
    "\n",
    "    if(key == 0):\n",
    "\n",
    "        return urlRemoveUrls(a)\n",
    "\n",
    "    elif(key == 1):\n",
    "\n",
    "        before = b\n",
    "        extracted = urlExtractUrls(a)\n",
    "\n",
    "        after = before\n",
    "        after.extend(extracted)\n",
    "\n",
    "        \"\"\"\n",
    "        if(str(extracted) != \"[]\"):\n",
    "            # TODO: Fix return bug\n",
    "            print(\"Debug >>\" + str(before) + \"/\" + str(extracted) + \">>\" + str(after) + \"<<\")\n",
    "        \"\"\"\n",
    "\n",
    "        return after\n",
    "\n",
    "    elif(key == 2):\n",
    "\n",
    "        # TODO: Refactor dont take it from extractedTextData\n",
    "        return hashTagExtractHashTags(a)\n",
    "\n",
    "    else:\n",
    "        switcher = {\n",
    "            3: d,\n",
    "            4: e,\n",
    "            5: f,\n",
    "            6: g\n",
    "        }\n",
    "        return switcher.get(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text und Text Meta Daten Extrahieren (Stufe 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns dict (empty dict if disabled, dict with not entries if error)\n",
    "listUnknownTypes = list()\n",
    "def processNerPipeline(inputText, pipelineKey, configMinScore):\n",
    "    if(pipelineKey in pipelineKeys):\n",
    "\n",
    "        listPer     = list()\n",
    "        listMisc    = list()\n",
    "        listOrg     = list()\n",
    "        listLoc     = list()\n",
    "\n",
    "\n",
    "        try:\n",
    "\n",
    "            data = dictPipelines[pipelineKey](inputText)\n",
    "\n",
    "            for d in data:\n",
    "\n",
    "                jsonData = demjson.decode(str(d), encoding='utf8')\n",
    "                            \n",
    "                if(jsonData[\"score\"] >= configMinScore):\n",
    "                    # Is Valid\n",
    "                    if      (jsonData[\"entity\"] == \"I-PER\" or jsonData[\"entity\"] == \"B-PER\"):\n",
    "                        listPer.append(jsonData[\"word\"])\n",
    "                    elif    (jsonData[\"entity\"] == \"I-MISC\" or jsonData[\"entity\"] == \"B-MISC\"):\n",
    "                        listMisc.append(jsonData[\"word\"])\n",
    "                    elif    (jsonData[\"entity\"] == \"I-ORG\" or jsonData[\"entity\"] == \"B-ORG\"):\n",
    "                        listOrg.append(jsonData[\"word\"])\n",
    "                    elif    (jsonData[\"entity\"] == \"I-LOC\" or jsonData[\"entity\"] == \"B-LOC\"):\n",
    "                        listLoc.append(jsonData[\"word\"])\n",
    "                    else:\n",
    "                        uT = str(jsonData[\"entity\"])\n",
    "                        if(uT not in listUnknownTypes):\n",
    "                            print(\"- Warn - Got unknown type >>\" + uT + \"<<\")\n",
    "                            listUnknownTypes.append(uT)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "            #print(\"Error in processNerPipeline (ignore) >>\" + str(inputText) + \"<<\")\n",
    "        \n",
    "\n",
    "        return {\n",
    "            \"per\": listPer,\n",
    "            \"misc\": listMisc,\n",
    "            \"org\": listOrg,\n",
    "            \"loc\": listLoc\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        return dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns\n",
    "# 1 - 5 (1 = bad / 5 = good)\n",
    "# -1 disabled or error\n",
    "def processSenPipeline(inputText, pipelineKey, configMinScore):\n",
    "    if(pipelineKey in pipelineKeys):\n",
    "\n",
    "        sen = -1\n",
    "\n",
    "        try:\n",
    "\n",
    "            data = dictPipelines[pipelineKey](inputText)\n",
    "            \n",
    "            for d in data:\n",
    "\n",
    "\n",
    "                jsonData = demjson.decode(str(d), encoding='utf-8')\n",
    "\n",
    "                if(jsonData[\"score\"]) > configMinScore:\n",
    "                    # Is Valid\n",
    "                    labelData = str(jsonData[\"label\"])\n",
    "\n",
    "                    if(\"stars\" in labelData):\n",
    "                        labelData = re.sub(\" stars\", \"\", labelData)\n",
    "                    else:\n",
    "                        labelData = re.sub(\" star\", \"\", labelData)\n",
    "                    \n",
    "                    sen = int(labelData)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "            #print(\"Error in processSenPipeline (ignore) >>\" + str(inputText) + \"<<\")\n",
    "\n",
    "        return sen\n",
    "\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns\n",
    "# dict (polarity, subjectivity) or none (fail or disabled)\n",
    "def processSentimentAnalysisPython(inputText):\n",
    "\n",
    "    try:\n",
    "        t = TextBlob(inputText)\n",
    "        return {\n",
    "            \"polarity\": t.polarity,\n",
    "            \"subjectivity\": t.subjectivity\n",
    "        }\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalIsValidText(procTDTextLength):\n",
    "    if(procTDTextLength > 0):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalContainsSomething(att):\n",
    "    if(str(att) == \"nan\"):\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalNonEmptyList(att):\n",
    "    if(str(att) == \"[]\"):\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chats einlesen, filtern und in DataFrame laden\n",
    "Ab hier Stopuhr global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stopwatch started >>Global notebook<<]\n"
     ]
    }
   ],
   "source": [
    "gloStartStopwatch(\"Global notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aus CSV Datei in DataFrame laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read jobs from file\n",
    "dfInputFiles = pd.read_csv(dir_var + \"inputFiles.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Daten filtern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputName</th>\n",
       "      <th>inputPath</th>\n",
       "      <th>inputType</th>\n",
       "      <th>inputId</th>\n",
       "      <th>inputDesc</th>\n",
       "      <th>inputDownloadType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>ATTILA HILDMANN</td>\n",
       "      <td>DS-05-01-2021/ChatExport_2021-01-05-hildmann</td>\n",
       "      <td>public_channel</td>\n",
       "      <td>10034163583</td>\n",
       "      <td>dataSet0</td>\n",
       "      <td>onlyText</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Oliver Janich oeffentlich</td>\n",
       "      <td>DS-05-01-2021/ChatExport_2021-01-05-janich</td>\n",
       "      <td>public_channel</td>\n",
       "      <td>9808932799</td>\n",
       "      <td>dataSet0</td>\n",
       "      <td>onlyText</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Eva Herman Offiziell</td>\n",
       "      <td>DS-05-01-2021/ChatExport_2021-01-05-evaherman</td>\n",
       "      <td>public_channel</td>\n",
       "      <td>9915108907</td>\n",
       "      <td>dataSet0</td>\n",
       "      <td>onlyText</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Xavier Naidoo (inoffiziell)</td>\n",
       "      <td>DS-05-01-2021/ChatExport_2021-01-05-xavier</td>\n",
       "      <td>public_channel</td>\n",
       "      <td>9874390332</td>\n",
       "      <td>dataSet0</td>\n",
       "      <td>onlyText</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      inputName  \\\n",
       "47              ATTILA HILDMANN   \n",
       "48    Oliver Janich oeffentlich   \n",
       "49         Eva Herman Offiziell   \n",
       "50  Xavier Naidoo (inoffiziell)   \n",
       "\n",
       "                                        inputPath       inputType  \\\n",
       "47   DS-05-01-2021/ChatExport_2021-01-05-hildmann  public_channel   \n",
       "48     DS-05-01-2021/ChatExport_2021-01-05-janich  public_channel   \n",
       "49  DS-05-01-2021/ChatExport_2021-01-05-evaherman  public_channel   \n",
       "50     DS-05-01-2021/ChatExport_2021-01-05-xavier  public_channel   \n",
       "\n",
       "        inputId inputDesc inputDownloadType  \n",
       "47  10034163583  dataSet0          onlyText  \n",
       "48   9808932799  dataSet0          onlyText  \n",
       "49   9915108907  dataSet0          onlyText  \n",
       "50   9874390332  dataSet0          onlyText  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfFilter = pd.DataFrame()\n",
    "\n",
    "for dS in C_LOAD_DATASETS:\n",
    "    dfFilter = dfFilter.append(dfInputFiles[dfInputFiles.inputDesc == dS])\n",
    "\n",
    "dfInputFiles = dfFilter\n",
    "\n",
    "dfInputFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dictMeta          = {}   \n",
    "\n",
    "# Add Key = filePath / Value = DataFrame (Chat Meta)\n",
    "for fP in dfInputFiles.inputPath:\n",
    "    dictMeta[fP] = convertToDataFrameMeta(fP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return dictMessages and dfAllDataMessages\n",
    "def initProcessData():\n",
    "\n",
    "    dictMessages      = {}\n",
    "    dfAllDataMessages = pd.DataFrame()\n",
    "\n",
    "    gloStartStopwatch(\"Extract Text Data\")\n",
    "\n",
    "    # Add Key = filePath / Value = DataFrame (Chat Message)\n",
    "    for fP in dfInputFiles.inputPath:\n",
    "\n",
    "        gloStartStopwatch(\"TD-Extract \" + fP)\n",
    "        dfMessages                          = convertToDataFrameMessages(fP)\n",
    "        tmpMeta                             = convertToDataFrameMeta(fP)\n",
    "\n",
    "        # Short run\n",
    "        if(C_SHORT_RUN):\n",
    "            print(\"Short run active!\")\n",
    "            dfMessages = dfMessages.head(C_NUMBER_SAMPLES)\n",
    "\n",
    "        # Get chat attributes and check if message is json formatted\n",
    "        dfMessages[\"procChatFilePath\"]      = fP\n",
    "        dfMessages[\"procChatType\"]          = tmpMeta.type.iloc[0]\n",
    "        dfMessages[\"procIsJsonFormatted\"]   = dfMessages[\"text\"].apply(gloCheckIsTextJsonFormatted, singleMode = False)\n",
    "        \n",
    "        # Extract Text Data\n",
    "        dfMessages[\"tmpExtractedTD\"]        = dfMessages.apply(lambda x: extractTextData(x.procIsJsonFormatted, x.text), axis=1)\n",
    "\n",
    "        # Extract Text Data (params)\n",
    "        dfMessages[\"procTDText\"]            = dfMessages.apply(lambda x: getExtractedTextDataParam(0, x.tmpExtractedTD), axis=1)\n",
    "        dfMessages[\"procTDURLs\"]            = dfMessages.apply(lambda x: getExtractedTextDataParam(1, x.tmpExtractedTD), axis=1)\n",
    "        dfMessages[\"procTDHashtags\"]        = dfMessages.apply(lambda x: getExtractedTextDataParam(2, x.tmpExtractedTD), axis=1)\n",
    "        dfMessages[\"procTDBolds\"]           = dfMessages.apply(lambda x: getExtractedTextDataParam(3, x.tmpExtractedTD), axis=1)\n",
    "        dfMessages[\"procTDItalics\"]         = dfMessages.apply(lambda x: getExtractedTextDataParam(4, x.tmpExtractedTD), axis=1)\n",
    "        dfMessages[\"procTDUnderlines\"]      = dfMessages.apply(lambda x: getExtractedTextDataParam(5, x.tmpExtractedTD), axis=1)\n",
    "        dfMessages[\"procTDEmails\"]          = dfMessages.apply(lambda x: getExtractedTextDataParam(6, x.tmpExtractedTD), axis=1)\n",
    "\n",
    "        # Process text again\n",
    "        dfMessages['procTDCleanText']           = dfMessages['procTDText'].map(lambda x: re.sub('\\n', ' ', x)) # Replace \\n\n",
    "        dfMessages['procTDEmojis']              = dfMessages['procTDCleanText'].map(lambda x: demoji.findall_list(x, desc = False)) # Filter out emoji\n",
    "        dfMessages['procTDEmojisDesc']          = dfMessages['procTDCleanText'].map(lambda x: demoji.findall_list(x, desc = True)) # Filter out emoji with desc\n",
    "        dfMessages['procTDCleanText']           = dfMessages['procTDCleanText'].map(lambda x: demoji.replace(x, \" \")) # Rm emoji\n",
    "        dfMessages['procTDCleanText']           = dfMessages['procTDCleanText'].map(lambda x: gloReplaceGermanChars(x)) # Replace german chars\n",
    "        dfMessages['procTDSafeText']            = dfMessages['procTDCleanText'].map(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', ' ', x)) # Filter out . ! ? ... (get only safe chars)\n",
    "        dfMessages['procTDSafeLowercaseText']   = dfMessages['procTDSafeText'].map(lambda x: x.lower()) # To lower\n",
    "\n",
    "        # Calc text size\n",
    "        dfMessages[\"procTDTextLength\"]      = dfMessages[\"procTDCleanText\"].str.len()\n",
    "\n",
    "        # Add columns (if not exists)\n",
    "        if \"photo\" not in dfMessages:\n",
    "            print(\"- Debug: Add column >>photo<<\")\n",
    "            dfMessages[\"photo\"] = np.nan\n",
    "\n",
    "        if \"file\" not in dfMessages:\n",
    "            print(\"- Debug: Add column >>file<<\")\n",
    "            dfMessages[\"file\"] = np.nan\n",
    "\n",
    "        if \"edited\" not in dfMessages:\n",
    "            print(\"- Debug: Add column >>edited<<\")\n",
    "            dfMessages[\"edited\"] = np.nan\n",
    "\n",
    "        if \"forwarded_from\" not in dfMessages:\n",
    "            print(\"- Debug: Add column >>forwarded_from<<\")\n",
    "            dfMessages[\"forwarded_from\"] = np.nan\n",
    "\n",
    "        # Evaluate attributes\n",
    "        dfMessages[\"procEvalIsValidText\"]   = dfMessages.procTDTextLength.apply(evalIsValidText)\n",
    "\n",
    "        dfMessages[\"procEvalContainsPhoto\"] = dfMessages.photo.apply(evalContainsSomething)\n",
    "        dfMessages[\"procEvalContainsFile\"]  = dfMessages.file.apply(evalContainsSomething) \n",
    "        dfMessages[\"procEvalIsEdited\"]      = dfMessages.edited.apply(evalContainsSomething)\n",
    "        dfMessages[\"procEvalIsForwarded\"]   = dfMessages.forwarded_from.apply(evalContainsSomething)\n",
    "        \n",
    "        dfMessages[\"procEvalContainsUrl\"]              = dfMessages.procTDURLs.apply(evalNonEmptyList)\n",
    "        dfMessages[\"procEvalContainsHashtag\"]          = dfMessages.procTDHashtags.apply(evalNonEmptyList)\n",
    "        dfMessages[\"procEvalContainsBoldItem\"]         = dfMessages.procTDBolds.apply(evalNonEmptyList)\n",
    "        dfMessages[\"procEvalContainsItalicItem\"]       = dfMessages.procTDItalics.apply(evalNonEmptyList)\n",
    "        dfMessages[\"procEvalContainsUnderlineItem\"]    = dfMessages.procTDUnderlines.apply(evalNonEmptyList)\n",
    "        dfMessages[\"procEvalContainsEmailItem\"]        = dfMessages.procTDEmails.apply(evalNonEmptyList)\n",
    "        dfMessages['procEvalContainsEmojiItem']        = dfMessages.procTDEmojis.apply(evalNonEmptyList)\n",
    "\n",
    "        # Pipelines\n",
    "        if dfInputFiles[dfInputFiles.inputPath == fP].iloc[0].inputDesc in C_PIPELINE_DATASETS:\n",
    "            gloStartStopwatch(\"Process pipeline ner-xlm-roberta\")\n",
    "            dfMessages['procPipeline-ner-xlm-roberta']    = dfMessages['procTDCleanText'].map(lambda x: processNerPipeline(x, \"ner-xlm-roberta\", configMinScore=0))\n",
    "            gloStopStopwatch(\"Process pipeline ner-xlm-roberta\")\n",
    "\n",
    "            gloStartStopwatch(\"Process pipeline ner-bert\")\n",
    "            dfMessages['procPipeline-ner-bert']           = dfMessages['procTDCleanText'].map(lambda x: processNerPipeline(x, \"ner-bert\", configMinScore=0))\n",
    "            gloStopStopwatch(\"Process pipeline ner-bert\")\n",
    "\n",
    "            gloStartStopwatch(\"Process pipeline sen-bert\")\n",
    "            dfMessages['procPipeline-sen-bert']           = dfMessages['procTDCleanText'].map(lambda x: processSenPipeline(x, \"sen-bert\", configMinScore=0))\n",
    "            gloStopStopwatch(\"Process pipeline sen-bert\")\n",
    "\n",
    "        # Sentiment Analysis\n",
    "        dfMessages['procPipeline-sentiment']           = dfMessages['procTDCleanText'].map(lambda x: processSentimentAnalysisPython(x))\n",
    "\n",
    "        # Add to dict    \n",
    "        dictMessages[fP] = dfMessages\n",
    "        gloStopStopwatch(\"TD-Extract \" + fP)\n",
    "\n",
    "    gloStopStopwatch(\"Extract Text Data\")\n",
    "\n",
    "    # All Messages to DataFrame\n",
    "    gloStartStopwatch(\"Generate global DataFrame\")\n",
    "    for fP in dfInputFiles.inputPath:\n",
    "        dfMessages        = dictMessages[fP].copy()\n",
    "        dfAllDataMessages = dfAllDataMessages.append(dfMessages)\n",
    "    gloStopStopwatch(\"Generate global DataFrame\")\n",
    "\n",
    "    return (dictMessages, dfAllDataMessages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return dictMessages and dfAllDataMessages\n",
    "def initCacheData(dfAllDataMessages):\n",
    "    dictMessages = {}\n",
    "    for fP in dfInputFiles.inputPath:\n",
    "        dictMessages[fP] = dfAllDataMessages[dfAllDataMessages.procChatFilePath == fP]\n",
    "    return (dictMessages, dfAllDataMessages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_USE_CACHE_FILE == \"\"):\n",
    "    print(\"Should not use cache (build new cache)\")\n",
    "    dictMessages, dfAllDataMessages = initProcessData()\n",
    "    if(C_NEW_CACHE_FILE != \"\"):\n",
    "        print(\"Write cache to file >>\" + str(C_NEW_CACHE_FILE) + \"<<\")\n",
    "        dfAllDataMessages.to_pickle(dir_var_pandas_cache + C_NEW_CACHE_FILE)\n",
    "else:\n",
    "    print(\"Should use cache (load cache)\")\n",
    "    dictMessages, dfAllDataMessages = initCacheData(pd.read_pickle(dir_var_pandas_cache + C_USE_CACHE_FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta-Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type of channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfInputFiles.inputType.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryChatId(filePath):\n",
    "    dfMeta = dictMeta[filePath].copy()\n",
    "    return str(dfMeta[\"id\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryChatName(filePath):\n",
    "    dfMeta      = dictMeta[filePath].copy()\n",
    "    chatName    = str(dfMeta[\"name\"].iloc[0])\n",
    "    chatName    = gloConvertToSafeChatName(chatName)\n",
    "    return chatName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryChatType(filePath):\n",
    "    dfMeta = dictMeta[filePath].copy()\n",
    "    return str(dfMeta[\"type\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryNumberOfMessages(filePath):\n",
    "    dfMessages = dictMessages[filePath].copy()\n",
    "    return len(dfMessages.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryNumberOfMessagesByAttEqTrue(filePath, attKey):\n",
    "    dfMessages = dictMessages[filePath].copy()\n",
    "    dfMessages = dfMessages[dfMessages[attKey] == True]\n",
    "    return len(dfMessages.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfQueryMeta = pd.DataFrame(dfInputFiles.inputPath)\n",
    "\n",
    "dfQueryMeta[\"qryChatId\"]                        = dfQueryMeta.inputPath.apply(queryChatId)\n",
    "dfQueryMeta[\"qryChatName\"]                      = dfQueryMeta.inputPath.apply(queryChatName)\n",
    "dfQueryMeta[\"qryChatType\"]                      = dfQueryMeta.inputPath.apply(queryChatType)\n",
    "dfQueryMeta[\"qryNumberOfMessages\"]              = dfQueryMeta.inputPath.apply(queryNumberOfMessages)\n",
    "\n",
    "dfQueryMeta[\"qryNumberOfFormattedTextMessages\"] = dfQueryMeta.apply(lambda x: queryNumberOfMessagesByAttEqTrue(x.inputPath, \"procIsJsonFormatted\"), axis=1)\n",
    "\n",
    "dfQueryMeta[\"qryNumberOfValidTextMessages\"]     = dfQueryMeta.apply(lambda x: queryNumberOfMessagesByAttEqTrue(x.inputPath, \"procEvalIsValidText\"), axis=1)\n",
    "\n",
    "dfQueryMeta[\"qryNumberOfPhotos\"]                = dfQueryMeta.apply(lambda x: queryNumberOfMessagesByAttEqTrue(x.inputPath, \"procEvalContainsPhoto\"), axis=1)\n",
    "dfQueryMeta[\"qryNumberOfFiles\"]                 = dfQueryMeta.apply(lambda x: queryNumberOfMessagesByAttEqTrue(x.inputPath, \"procEvalContainsFile\"), axis=1)\n",
    "dfQueryMeta[\"qryNumberOfEditedMessages\"]        = dfQueryMeta.apply(lambda x: queryNumberOfMessagesByAttEqTrue(x.inputPath, \"procEvalIsEdited\"), axis=1)\n",
    "dfQueryMeta[\"qryNumberOfForwardedMessages\"]     = dfQueryMeta.apply(lambda x: queryNumberOfMessagesByAttEqTrue(x.inputPath, \"procEvalIsForwarded\"), axis=1)\n",
    "\n",
    "dfQueryMeta[\"qryNumberOfMessagesWithUrl\"]           = dfQueryMeta.apply(lambda x: queryNumberOfMessagesByAttEqTrue(x.inputPath, \"procEvalContainsUrl\"), axis=1)\n",
    "dfQueryMeta[\"qryNumberOfMessagesWithHashtag\"]       = dfQueryMeta.apply(lambda x: queryNumberOfMessagesByAttEqTrue(x.inputPath, \"procEvalContainsHashtag\"), axis=1)\n",
    "dfQueryMeta[\"qryNumberOfMessagesWithBold\"]          = dfQueryMeta.apply(lambda x: queryNumberOfMessagesByAttEqTrue(x.inputPath, \"procEvalContainsBoldItem\"), axis=1)\n",
    "dfQueryMeta[\"qryNumberOfMessagesWithItalic\"]        = dfQueryMeta.apply(lambda x: queryNumberOfMessagesByAttEqTrue(x.inputPath, \"procEvalContainsItalicItem\"), axis=1)\n",
    "dfQueryMeta[\"qryNumberOfMessagesWithUnderline\"]     = dfQueryMeta.apply(lambda x: queryNumberOfMessagesByAttEqTrue(x.inputPath, \"procEvalContainsUnderlineItem\"), axis=1)\n",
    "dfQueryMeta[\"qryNumberOfMessagesWithEmail\"]         = dfQueryMeta.apply(lambda x: queryNumberOfMessagesByAttEqTrue(x.inputPath, \"procEvalContainsEmailItem\"), axis=1)\n",
    "dfQueryMeta[\"qryNumberOfMessagesWithEmoji\"]         = dfQueryMeta.apply(lambda x: queryNumberOfMessagesByAttEqTrue(x.inputPath, \"procEvalContainsEmojiItem\"), axis=1)\n",
    "\n",
    "dfQueryMeta.sort_values(by=\"qryNumberOfMessages\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot meta queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto label query plot\n",
    "def autolabelAx(rects, ax):\n",
    "    \"\"\"\n",
    "    Attach a text label above each bar in *rects*, displaying its height.\n",
    "    Copied from https://matplotlib.org/3.1.1/gallery/lines_bars_and_markers/barchart.html (22.12.2020)\n",
    "    \"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param inputDescFilter set \"\" == no filter\n",
    "# param outputFilename set \"\" = no output\n",
    "def queryMetaPlotter(inputDescFilter, configPlotWidth, configPlotHeight, configBarWidth, outputFilename):\n",
    "    # Init data\n",
    "    dataLabels                          = list()\n",
    "    dataNumberOfMesssages               = list()\n",
    "    dataNumberOfFormattedTextMessages   = list()\n",
    "    dataNumberOfValidTextMessages       = list()\n",
    "    dataNumberOfEditedMessages          = list()\n",
    "    dataNumberOfForwardedMessages       = list()\n",
    "    dataNumberOfPhotos                  = list()\n",
    "    dataNumberOfFiles                   = list()\n",
    "    dataNumberOfMessagesWUrl            = list()\n",
    "    dataNumberOfMessagesWHashtag        = list()\n",
    "    dataNumberOfMessagesWBold           = list()\n",
    "    dataNumberOfMessagesWItalic         = list()\n",
    "    dataNumberOfMessagesWUnderline      = list()\n",
    "    dataNumberOfMessagesWEmail          = list()\n",
    "    dataNumberOfMessagesWEmoji          = list()\n",
    "\n",
    "    # Iterate over Meta DataFrame\n",
    "    for index, row in dfQueryMeta.sort_values(by=\"qryNumberOfMessages\", ascending=False).iterrows():\n",
    "\n",
    "        # Get attributes (check filter)\n",
    "        if(inputDescFilter == \"\" or dfInputFiles[dfInputFiles.inputPath == row.inputPath].inputDesc.iloc[0] == inputDescFilter):\n",
    "            dataLabels                          .append(row.qryChatName)\n",
    "            dataNumberOfMesssages               .append(row.qryNumberOfMessages)\n",
    "            dataNumberOfFormattedTextMessages   .append(row.qryNumberOfFormattedTextMessages)\n",
    "            dataNumberOfValidTextMessages       .append(row.qryNumberOfValidTextMessages)\n",
    "            dataNumberOfEditedMessages          .append(row.qryNumberOfEditedMessages)\n",
    "            dataNumberOfForwardedMessages       .append(row.qryNumberOfForwardedMessages)\n",
    "            dataNumberOfPhotos                  .append(row.qryNumberOfPhotos)\n",
    "            dataNumberOfFiles                   .append(row.qryNumberOfFiles)\n",
    "            dataNumberOfMessagesWUrl            .append(row.qryNumberOfMessagesWithUrl)\n",
    "            dataNumberOfMessagesWHashtag        .append(row.qryNumberOfMessagesWithHashtag)\n",
    "            dataNumberOfMessagesWBold           .append(row.qryNumberOfMessagesWithBold)\n",
    "            dataNumberOfMessagesWItalic         .append(row.qryNumberOfMessagesWithItalic)\n",
    "            dataNumberOfMessagesWUnderline      .append(row.qryNumberOfMessagesWithUnderline)\n",
    "            dataNumberOfMessagesWEmail          .append(row.qryNumberOfMessagesWithEmail)\n",
    "            dataNumberOfMessagesWEmoji          .append(row.qryNumberOfMessagesWithEmoji)\n",
    "\n",
    "    # Convert list to array\n",
    "    dataLabels                          = np.array(dataLabels)\n",
    "    dataNumberOfMesssages               = np.array(dataNumberOfMesssages)\n",
    "    dataNumberOfFormattedTextMessages   = np.array(dataNumberOfFormattedTextMessages)\n",
    "    dataNumberOfValidTextMessages       = np.array(dataNumberOfValidTextMessages)\n",
    "    dataNumberOfEditedMessages          = np.array(dataNumberOfEditedMessages)\n",
    "    dataNumberOfForwardedMessages       = np.array(dataNumberOfForwardedMessages)\n",
    "    dataNumberOfPhotos                  = np.array(dataNumberOfPhotos)\n",
    "    dataNumberOfFiles                   = np.array(dataNumberOfFiles)\n",
    "    dataNumberOfMessagesWUrl            = np.array(dataNumberOfMessagesWUrl)\n",
    "    dataNumberOfMessagesWHashtag        = np.array(dataNumberOfMessagesWHashtag)\n",
    "    dataNumberOfMessagesWBold           = np.array(dataNumberOfMessagesWBold)\n",
    "    dataNumberOfMessagesWItalic         = np.array(dataNumberOfMessagesWItalic)\n",
    "    dataNumberOfMessagesWUnderline      = np.array(dataNumberOfMessagesWUnderline)\n",
    "    dataNumberOfMessagesWEmail          = np.array(dataNumberOfMessagesWEmail)\n",
    "    dataNumberOfMessagesWEmoji          = np.array(dataNumberOfMessagesWEmoji)\n",
    "\n",
    "    # Draw\n",
    "    with sns.color_palette(\"tab10\", 11):\n",
    "        fig, ax = plt.subplots()\n",
    "    x = np.arange(len(dataLabels))\n",
    "\n",
    "    barWidth = configBarWidth\n",
    "\n",
    "    fig.set_figwidth(configPlotWidth)\n",
    "    fig.set_figheight(configPlotHeight)\n",
    "\n",
    "    r1 = x\n",
    "    r2 = [x + barWidth for x in r1]\n",
    "    r3 = [x + barWidth for x in r2]\n",
    "    r4 = [x + barWidth for x in r3]\n",
    "    r5 = [x + barWidth for x in r4]\n",
    "    r6 = [x + barWidth for x in r5]\n",
    "    r7 = [x + barWidth for x in r6]\n",
    "    r8 = [x + barWidth for x in r7]\n",
    "    r9 = [x + barWidth for x in r8]\n",
    "    r10 = [x + barWidth for x in r9]\n",
    "    r11 = [x + barWidth for x in r10]\n",
    "    r12 = [x + barWidth for x in r11]\n",
    "    r13 = [x + barWidth for x in r12]\n",
    "    r14 = [x + barWidth for x in r13]\n",
    "\n",
    "    rects1 = ax.bar(r1, dataNumberOfMesssages, barWidth, label='Messages')\n",
    "    rects2 = ax.bar(r2, dataNumberOfFormattedTextMessages, barWidth, label='Formatted Messsages')\n",
    "    rects3 = ax.bar(r3, dataNumberOfValidTextMessages, barWidth, label='Valid Text Messages')\n",
    "    rects4 = ax.bar(r4, dataNumberOfEditedMessages, barWidth, label='Edited Messages')\n",
    "    rects5 = ax.bar(r5, dataNumberOfForwardedMessages, barWidth, label='Forwarded Messages')\n",
    "    rects6 = ax.bar(r6, dataNumberOfPhotos, barWidth, label='with Photo')\n",
    "    rects7 = ax.bar(r7, dataNumberOfFiles, barWidth, label='with File')\n",
    "    rects8 = ax.bar(r8, dataNumberOfMessagesWUrl, barWidth, label='with Url')\n",
    "    rects9 = ax.bar(r9, dataNumberOfMessagesWHashtag, barWidth, label='with Hashtag')\n",
    "    rects10 = ax.bar(r10, dataNumberOfMessagesWBold, barWidth, label='with Bold Items')\n",
    "    rects11 = ax.bar(r11, dataNumberOfMessagesWItalic, barWidth, label='with Italic Items')\n",
    "    rects12 = ax.bar(r12, dataNumberOfMessagesWUnderline, barWidth, label='with Underlined Items')\n",
    "    rects13 = ax.bar(r13, dataNumberOfMessagesWEmail, barWidth, label='with E-Mails')\n",
    "    rects14 = ax.bar(r14, dataNumberOfMessagesWEmoji, barWidth, label='with Emojis')\n",
    "\n",
    "    chartTitle = \"\"\n",
    "    if(inputDescFilter != \"\"):\n",
    "        chartTitle = \" (\" + inputDescFilter + \")\"\n",
    "\n",
    "    ax.set_ylabel(\"Number of\")\n",
    "    ax.set_title(\"Meta Overview\" + chartTitle)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(dataLabels)\n",
    "    ax.legend()\n",
    "\n",
    "    rects = [rects1, rects2, rects3, rects4, rects5, rects6, rects7, rects8, rects9, rects10, rects11, rects12, rects13, rects14]\n",
    "\n",
    "    for rect in rects:\n",
    "        autolabelAx(rect, ax)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    #plt.xticks(rotation=30)\n",
    "    \n",
    "    if(outputFilename != \"\"):\n",
    "        plt.savefig(dir_var_output + outputFilename)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryMetaPlotter(\n",
    "    inputDescFilter = \"dataSet0\",\n",
    "    configPlotWidth = 32,\n",
    "    configPlotHeight = 9,\n",
    "    configBarWidth = 0.065,\n",
    "    outputFilename = \"meta-overview-dataSet0.svg\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(\"dataSet1\" in C_LOAD_DATASETS):\n",
    "    queryMetaPlotter(\n",
    "        inputDescFilter = \"dataSet1\",\n",
    "        configPlotWidth = 100,\n",
    "        configPlotHeight = 9,\n",
    "        configBarWidth = 0.065,\n",
    "        outputFilename = \"meta-overview-dataSet1.svg\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(\"dataSet1a\" in C_LOAD_DATASETS):\n",
    "    queryMetaPlotter(\n",
    "        inputDescFilter = \"dataSet1a\",\n",
    "        configPlotWidth = 16,\n",
    "        configPlotHeight = 9,\n",
    "        configBarWidth = 0.065,\n",
    "        outputFilename = \"meta-overview-dataSet1a.svg\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(\"dataSet2\" in C_LOAD_DATASETS):\n",
    "    queryMetaPlotter(\n",
    "        inputDescFilter = \"dataSet2\",\n",
    "        configPlotWidth = 34,\n",
    "        configPlotHeight = 9,\n",
    "        configBarWidth = 0.065,\n",
    "        outputFilename = \"meta-overview-dataSet2.svg\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get text-length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeTextLengthOutliersFromDataFrame(df, interval, maxTextLength):\n",
    "    df = df.copy()\n",
    "    df = df[df.procTDTextLength < maxTextLength]\n",
    "    # https://stackoverflow.com/questions/23199796/detect-and-exclude-outliers-in-pandas-data-frame\n",
    "    # keep only the ones that are within <interval> to -<interval> standard deviations in the column 'Data'.\n",
    "    return df[np.abs(df.procTDTextLength-df.procTDTextLength.mean()) <= (interval*df.procTDTextLength.std())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param outputFilename set \"\" == no output file\n",
    "def textLengthHistPlotter(outputFilename):\n",
    "    dfMessages = dfAllDataMessages.copy()\n",
    "    print(\"Number of all messages:\\t\\t\\t\\t\\t\\t\" + str(len(dfMessages.index)))\n",
    "\n",
    "    dfMessages = dfMessages[dfMessages.procEvalIsValidText == True]\n",
    "    print(\"Number of valid text messages:\\t\\t\\t\\t\\t\" + str(len(dfMessages.index)))\n",
    "\n",
    "    dfMessages = removeTextLengthOutliersFromDataFrame(\n",
    "        dfMessages,\n",
    "        interval = 3,               #Default is 3\n",
    "        maxTextLength = 999999999   #TODO: Maybe enable max text length\n",
    "        )\n",
    "    print(\"Number of valid text messages (after outliers filtering):\\t\" + str(len(dfMessages.index)))\n",
    "\n",
    "    print()\n",
    "    print(\"Text Length Hist (after outliers filtering)\")\n",
    "    plt.figure(figsize=(8,4.5))\n",
    "    _ = dfMessages.procTDTextLength.hist(bins=40)\n",
    "    plt.title('Histogram Text Length')\n",
    "\n",
    "    if(outputFilename != \"\"):\n",
    "        plt.savefig(dir_var_output + outputFilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textLengthHistPlotter(outputFilename = \"meta-text-length-hist.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare ids and labels (has chat name changed?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareIdsAndLabels(df):\n",
    "\n",
    "    gloStartStopwatch(\"Compare ids and labels\")\n",
    "\n",
    "    dictFromTranslator  = {}\n",
    "    dictActorTranslator = {}\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    \n",
    "    df = df.set_index(\"date\")\n",
    "    df = df.sort_index()\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        n_from      = row[\"from\"]\n",
    "        n_from_id   = row[\"from_id\"]\n",
    "\n",
    "        n_from = str(n_from)\n",
    "        n_from_id = str(n_from_id)\n",
    "\n",
    "        n_actor      = row[\"actor\"]\n",
    "        n_actor_id   = row[\"actor_id\"]\n",
    "\n",
    "        n_actor = str(n_actor)\n",
    "        n_actor_id = str(n_actor_id)\n",
    "\n",
    "        if(str(n_from) != \"nan\"):\n",
    "            if(n_from_id not in dictFromTranslator):\n",
    "                # Add new key\n",
    "                dictFromTranslator[n_from_id] = [n_from]\n",
    "            else:\n",
    "                # Has changed?\n",
    "                oValueL = dictFromTranslator[n_from_id]\n",
    "                if(n_from not in oValueL):\n",
    "                    newList = oValueL.copy()\n",
    "                    newList.append(n_from)\n",
    "                    print(\"- Add changed attribute in from (prev=\" + str(oValueL) + \"/new=\" + str(newList) + \")\")\n",
    "                    dictFromTranslator[n_from_id] = newList\n",
    "\n",
    "        if(str(n_actor) != \"nan\"):\n",
    "            if(n_actor_id not in dictActorTranslator):\n",
    "                # Add new key\n",
    "                dictActorTranslator[n_actor_id] = [n_actor]\n",
    "            else:\n",
    "                # Has changed?\n",
    "                oValueL = dictActorTranslator[n_actor_id]\n",
    "                if(n_actor not in oValueL):\n",
    "                    newList = oValueL.copy()\n",
    "                    newList.append(n_actor)\n",
    "                    print(\"- Add changed attribute in actor (prev=\" + str(oValueL) + \"/new=\" + str(newList) + \")\")\n",
    "                    dictActorTranslator[n_actor_id] = newList\n",
    "\n",
    "    gloStopStopwatch(\"Compare ids and labels\")\n",
    "\n",
    "    return dictFromTranslator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    compareIdsAndLabels(dfAllDataMessages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Social Graph Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractImportantHashtags(df):\n",
    "    dfMessages = df.copy()\n",
    "    dfMessages = dfMessages[dfMessages.procEvalContainsHashtag == True]\n",
    "\n",
    "    hashTagList = list()\n",
    "    for index, row in dfMessages.iterrows():\n",
    "        for hashtagItem in row[\"procTDHashtags\"]:\n",
    "            hashTagList.append(hashtagItem)\n",
    "\n",
    "    return hashTagList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return combinations\n",
    "def extractImportantEmojis(df):\n",
    "\n",
    "    dfMessages = df.copy()\n",
    "    dfMessages = dfMessages[dfMessages.procEvalContainsEmojiItem == True]\n",
    "\n",
    "    li = dfMessages.procTDEmojisDesc.values.tolist()\n",
    "\n",
    "    retLi = list()\n",
    "\n",
    "    for l in li:\n",
    "        aString = \"\"\n",
    "        for e in l:\n",
    "            aString = aString + \":\" + e \n",
    "        retLi.append(aString)\n",
    "\n",
    "    return retLi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param flagResolveNewUrls  Flag (see config above)\n",
    "\n",
    "def resolveUrl(completeUrl, flagResolveNewUrls):\n",
    "    \n",
    "    if \"bit.ly\" in completeUrl:\n",
    "\n",
    "        if(gloCheckIsAlreadyCached(\"resolved-urls.csv\", completeUrl)):\n",
    "            return gloGetCached(\"resolved-urls.csv\", completeUrl)\n",
    "        else:\n",
    "\n",
    "            if(flagResolveNewUrls == False):\n",
    "                return completeUrl\n",
    "\n",
    "            print(\"(Resolve now >>\" + completeUrl + \"<<)\")\n",
    "            try:\n",
    "                r = requests.get(completeUrl, timeout = 5)\n",
    "                u = r.url\n",
    "                gloAddToCache(\"resolved-urls.csv\", completeUrl, u)\n",
    "                return u\n",
    "            except:\n",
    "                print(\"(- Warn: Can not resolve (return completeUrl))\")\n",
    "                return completeUrl\n",
    "\n",
    "    else:\n",
    "        return completeUrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return\n",
    "# a = urlList,\n",
    "# b = refList\n",
    "# c = hostList\n",
    "def extractImportantUrls(df):\n",
    "    dfMessages = df.copy()\n",
    "    dfMessages = dfMessages[dfMessages.procEvalContainsUrl == True]\n",
    "\n",
    "    hostList        = list()\n",
    "    urList          = list()\n",
    "    refList         = list()\n",
    "\n",
    "    counterSucHostname = 0\n",
    "    counterErrHostname = 0\n",
    "\n",
    "    for index, row in dfMessages.iterrows():\n",
    "        for urlItem in row[\"procTDURLs\"]:\n",
    "            \n",
    "            urlData = urlparse(str(urlItem))\n",
    "\n",
    "            completeUrl      = urlData.geturl()\n",
    "\n",
    "            rUrl     = resolveUrl(completeUrl, flagResolveNewUrls=C_RESOLVE_NEW_URLS)\n",
    "            rUrlData = urlparse(rUrl)\n",
    "            rCompleteUrl = rUrlData.geturl()\n",
    "            rCompleteHostname = rUrlData.hostname\n",
    "\n",
    "            if(str(rCompleteHostname) != \"None\"):\n",
    "                counterSucHostname = counterSucHostname + 1\n",
    "\n",
    "                hostList.append(str(rCompleteHostname))\n",
    "\n",
    "                urList.append(str(rCompleteUrl))\n",
    "\n",
    "                if \"t.me\" in str(rCompleteHostname):\n",
    "                    refList.append(str(rCompleteUrl))\n",
    "            else:\n",
    "                counterErrHostname = counterErrHostname + 1\n",
    "\n",
    "    print(\"Got Hostnames (suc=\" + str(counterSucHostname) + \"/err=\" + str(counterErrHostname) + \")\")\n",
    "\n",
    "    return (urList, refList, hostList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param flagResolveNewUrls  Flag (see config above)\n",
    "def resolveImportantYoutubeVideos(urlList, flagResolveNewUrls):\n",
    "\n",
    "    # Thanks https://gist.github.com/rodrigoborgesdeoliveira/987683cfbfcc8d800192da1e73adc486\n",
    "\n",
    "    ytList = list()\n",
    "\n",
    "    for url in urlList:\n",
    "\n",
    "        url = str(url)\n",
    "\n",
    "        if(\"youtube.com\" in url or \"youtu.be\" in url or \"youtube-nocookie.com\" in url):\n",
    "            if(gloCheckIsAlreadyCached(\"resolved-youtube.csv\", url)):\n",
    "                ytList.append(gloGetCached(\"resolved-youtube.csv\", url)) \n",
    "            else:\n",
    "\n",
    "                if(flagResolveNewUrls == False):\n",
    "                    print(\"(Disable resolve new youtube urls (return completeUrl) >>\" + url + \"<<)\")\n",
    "                    ytList.append(url)\n",
    "                else:\n",
    "                    print(\"Resolve now youtube >>\" + url + \"<<\")\n",
    "                    try:\n",
    "                        r = requests.get(url, timeout = 5)\n",
    "                        t = fromstring(r.content)\n",
    "                        a = str(t.findtext('.//title'))\n",
    "                        ytList.append(a)\n",
    "                        gloAddToCache(\"resolved-youtube.csv\", url, a)\n",
    "                    except:\n",
    "                        print(\"(- Warn: Can not resolve youtube url (return completeUrl))\")\n",
    "                        ytList.append(url)\n",
    "\n",
    "    return ytList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Bug: No Hostname detected if string startsWith ! \"http\" in urlparse\n",
    "# TODO: Check: Refs ins both directions\n",
    "\n",
    "# Returns\n",
    "# a = Counter forwardedFromList\n",
    "# b = Counter refList\n",
    "# c = Counter hashtagList\n",
    "# d = Counter hostList\n",
    "# e = Counter emojiList\n",
    "# f = Counter fromList\n",
    "def extractSocialGraph(filePath, debugPrint, debugPrintCount):\n",
    "\n",
    "    dfMessages = dictMessages[filePath].copy()\n",
    "\n",
    "    hashtagList = extractImportantHashtags(dfMessages)\n",
    "    emojiList = extractImportantEmojis(dfMessages)\n",
    "\n",
    "    urlList, refList, hostList = extractImportantUrls(dfMessages)\n",
    "\n",
    "    ytList = resolveImportantYoutubeVideos(urlList, flagResolveNewUrls = C_RESOLVE_NEW_URLS)\n",
    "            \n",
    "    forwardedFromList = list()\n",
    "    if(\"forwarded_from\" in dfMessages.columns):\n",
    "        df = dfMessages.copy()\n",
    "        df = df[df.procEvalIsForwarded == True]\n",
    "    \n",
    "        for index, row in df.iterrows():        \n",
    "            forwardedFromList.append(str(row[\"forwarded_from\"]))\n",
    "            \n",
    "    actorList = list()\n",
    "    if(\"actor\" in dfMessages.columns):\n",
    "        for index, row in dfMessages.iterrows():\n",
    "            actorList.append(str(row[\"actor\"]))\n",
    "    \n",
    "    memberList = list()\n",
    "    if(\"members\" in dfMessages.columns):\n",
    "        for index, row in dfMessages.iterrows():\n",
    "            if(str(row[\"members\"]) != \"nan\"):\n",
    "                for memberItem in row[\"members\"]:\n",
    "                    memberList.append(str(memberItem))\n",
    "                    \n",
    "    fromList = list()\n",
    "    if(\"from\" in dfMessages.columns):\n",
    "        for index, row in dfMessages.iterrows():\n",
    "            s = str(row[\"from\"])\n",
    "            s = gloConvertToSafeString(s)\n",
    "            if(s != \"None\"):\n",
    "                fromList.append(s)\n",
    "            \n",
    "    savedFromList = list()\n",
    "    if(\"saved_from\" in dfMessages.columns):\n",
    "        for index, row in dfMessages.iterrows():\n",
    "            savedFromList.append(str(row[\"saved_from\"]))\n",
    "\n",
    "    configTopN = debugPrintCount\n",
    "\n",
    "    if(debugPrint):\n",
    "\n",
    "        print()\n",
    "        print(\"Set top n to \" + str(debugPrintCount))\n",
    "        print()\n",
    "\n",
    "        print(\"- Top Hosts (resovled) -\")\n",
    "        print (\"\\n\".join(map(str, Counter(hostList).most_common(configTopN))))\n",
    "        print()\n",
    "        print(\"- Top URLs (resolved) -\")\n",
    "        print (\"\\n\".join(map(str, Counter(urlList).most_common(configTopN))))\n",
    "        print()\n",
    "        print(\"- Top Refs from text (resolved) -\")\n",
    "        print (\"\\n\".join(map(str, Counter(refList).most_common(configTopN))))\n",
    "        print()\n",
    "        print(\"- Top Refs (forwarded_from) -\")\n",
    "        print (\"\\n\".join(map(str, Counter(forwardedFromList).most_common(configTopN))))\n",
    "        print()\n",
    "        print(\"- Top Refs (actor) -\")\n",
    "        print (\"\\n\".join(map(str, Counter(actorList).most_common(configTopN))))\n",
    "        print()\n",
    "        print(\"- Top Refs (members) -\")\n",
    "        print (\"\\n\".join(map(str, Counter(memberList).most_common(configTopN))))\n",
    "        print()\n",
    "        print(\"- Top Refs (from) -\")\n",
    "        print (\"\\n\".join(map(str, Counter(fromList).most_common(configTopN))))\n",
    "        print()\n",
    "        print(\"- Top Refs (saved_from) -\")\n",
    "        print (\"\\n\".join(map(str, Counter(savedFromList).most_common(configTopN))))\n",
    "        print()\n",
    "        print(\"- Top hashtags -\")\n",
    "        print (\"\\n\".join(map(str, Counter(hashtagList).most_common(configTopN))))\n",
    "        print()\n",
    "        print(\"- Top emojis -\")\n",
    "        print (\"\\n\".join(map(str, Counter(emojiList).most_common(configTopN))))\n",
    "        print()\n",
    "        print(\"- Top yt (resolved) -\")\n",
    "        print (\"\\n\".join(map(str, Counter(ytList).most_common(configTopN))))\n",
    "        print()\n",
    "    \n",
    "    return (Counter(forwardedFromList), Counter(refList), Counter(hashtagList),  Counter(hostList), Counter(emojiList), Counter(fromList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printSocialGraphDebug(filePathList):\n",
    "    for fP in filePathList:\n",
    "        print(\"Analyse now >>\" + fP + \"<<\")\n",
    "        _ = extractSocialGraph(fP, debugPrint=True, debugPrintCount=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    printSocialGraphDebug(dfInputFiles[dfInputFiles.inputDesc == \"dataSet0\"].inputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False and False): # TODO: Enable - Disable (read)\n",
    "    printSocialGraphDebug(dfInputFiles[dfInputFiles.inputDesc == \"dataSet1a\"].inputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False) and False: # TODO: Enable - Disable (read)\n",
    "    printSocialGraphDebug(dfInputFiles[dfInputFiles.inputDesc == \"dataSet2\"].inputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dictSGD_ForwardedFrom = {}\n",
    "dictSGD_Ref           = {}\n",
    "dictSGD_Hashtag       = {}\n",
    "dictSGD_Host          = {}\n",
    "dictSGD_Emoji         = {}\n",
    "dictSGD_From          = {}\n",
    "\n",
    "gloStartStopwatch(\"Extract Social Graph Data\")\n",
    "\n",
    "for fP in dfInputFiles.inputPath:\n",
    "\n",
    "    gloStartStopwatch(\"Extract Social Graph Data >>\" + fP + \"<<\")\n",
    "\n",
    "    a, b, c, d, e, f = extractSocialGraph(fP, debugPrint=False, debugPrintCount = 0)\n",
    "\n",
    "    dictSGD_ForwardedFrom[fP]   = a\n",
    "    dictSGD_Ref[fP]             = b\n",
    "    dictSGD_Hashtag[fP]         = c\n",
    "    dictSGD_Host[fP]            = d\n",
    "    dictSGD_Emoji[fP]           = e\n",
    "    dictSGD_From[fP]            = f\n",
    "\n",
    "    gloStopStopwatch(\"Extract Social Graph Data >>\" + fP + \"<<\")\n",
    "\n",
    "gloStopStopwatch(\"Extract Social Graph Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Top Influencer (Downloaded?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Top Influencer\n",
    "# param fPList      filePath List\n",
    "# param configTopN  Get Top n influencer e.g. 10\n",
    "def getTopInfluencer(fPList, configTopN):\n",
    "\n",
    "    for fP in fPList:\n",
    "\n",
    "        chatName = queryChatName(fP)\n",
    "\n",
    "        print()\n",
    "        print(\"Analyse Chat (Forwarded From) >>\" + chatName + \"<<\")\n",
    "        \n",
    "        socialGraphData = dictSGD_ForwardedFrom[fP]\n",
    "        socialGraphData = socialGraphData.most_common(configTopN)\n",
    "\n",
    "        counter = 1\n",
    "\n",
    "        # Iterate over data\n",
    "        for oChatName, oChatRefs in socialGraphData:\n",
    "            \n",
    "            # Query other params\n",
    "            oChatName    = gloConvertToSafeChatName(str(oChatName))\n",
    "            oChatRefs    = oChatRefs\n",
    "\n",
    "            # Already downloaded?\n",
    "            flagDownloaded = False\n",
    "            if oChatName in dfQueryMeta.qryChatName.values:\n",
    "                flagDownloaded = True\n",
    "\n",
    "            if(oChatName != \"nan\"):\n",
    "\n",
    "                print(str(counter) + \": (downloaded=\" + str(flagDownloaded) + \") (refs=\" + str(oChatRefs) + \")\\t\\t>>\" + str(oChatName) + \"<<\")\n",
    "                counter = counter + 1\n",
    "\n",
    "\n",
    "        print()\n",
    "        print(\"Analyse Chat (Refs) >>\" + chatName + \"<<\")\n",
    "        \n",
    "        socialGraphData = dictSGD_Ref[fP]\n",
    "        socialGraphData = socialGraphData.most_common(configTopN)\n",
    "\n",
    "        counter = 1\n",
    "\n",
    "        # Iterate over data\n",
    "        for oChatName, oChatRefs in socialGraphData:\n",
    "            \n",
    "            # Query other params\n",
    "            oChatName    = str(oChatName)\n",
    "            oChatRefs    = oChatRefs\n",
    "\n",
    "            if(oChatName != \"nan\"):\n",
    "\n",
    "                print(str(counter) + \" (refs=\" + str(oChatRefs) + \")\\t\\t>>\" + str(oChatName) + \"<<\")\n",
    "                counter = counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Can not get all items in dataSet1\n",
    "\n",
    "\"\"\"\n",
    "# Attila Hildmann #\n",
    "- Anonymous Germany - not found\n",
    "- https://t.me/DEMOKRATENCHAT - no entries\n",
    "- https://t.me/ChatDerFreiheit - no entries\n",
    "- https://t.me/FREIHEITSCHAT2020 - not found\n",
    "\n",
    "# Oliver Janich #\n",
    "- Oliver Janich Premium - not found\n",
    "\n",
    "# Xavier Naidoo #\n",
    "- Xavier(Der VereiNiger)Naidoo😎 - not found\n",
    "- https://t.me/PostAppender_bot - bot chat\n",
    "\"\"\"\n",
    "getTopInfluencer(list(dfInputFiles[dfInputFiles.inputDesc == \"dataSet0\"].inputPath), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Social Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Social Graph Layout Selector\n",
    "\n",
    "param G Graph\n",
    "param layoutSelector:\n",
    "\n",
    "1 = Kamda Kawai Layout\n",
    "2 = Spring Layout\n",
    "3 = Graphviz Layout\n",
    "\"\"\"\n",
    "def getSocialGraphLayout(layoutSelector, G):\n",
    "    if(layoutSelector == 1):\n",
    "        return nx.kamada_kawai_layout(G.to_undirected())\n",
    "    elif(layoutSelector == 2):\n",
    "        return nx.spring_layout(G.to_undirected(), k = 0.15, iterations=200)\n",
    "    elif(layoutSelector == 3):\n",
    "        return nx.nx_pydot.graphviz_layout(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try different arrows (see below): https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.patches.ArrowStyle.html\n",
    "# TODO: Check distances between nodes\n",
    "\n",
    "\"\"\"\n",
    "Draw social grah\n",
    "\n",
    "param   G                           graph\n",
    "param   layoutSelector              see above\n",
    "param   configFactorEdge            e.g. 100 => weight / 100\n",
    "param   configFactorNode            e.g. 10  => weight / 10\n",
    "param   configArrowSize             e.g. 5\n",
    "param   configPlotWidth             e.g. 16\n",
    "param   configPlotHeight            e.g. 9\n",
    "param   outputFilename              e.g. test.png (set \"\" == no output file)\n",
    "param   outputTitle                 e.g. Graph (required)\n",
    "\"\"\"\n",
    "def drawSocialGraph(G, layoutSelector, configFactorEdge, configFactorNode, configArrowSize, configPlotWidth, configPlotHeight, outputFilename, outputTitle):\n",
    "    \n",
    "    gloStartStopwatch(\"Social Graph Plot\")\n",
    "    \n",
    "    plt.figure(figsize=(configPlotWidth,configPlotHeight))\n",
    "        \n",
    "    pos = getSocialGraphLayout(layoutSelector = layoutSelector, G = G)\n",
    "    \n",
    "    # Clean edges\n",
    "    edges       = nx.get_edge_attributes(G, \"weight\")\n",
    "    edgesTLabel = nx.get_edge_attributes(G, \"tLabel\")\n",
    "\n",
    "    clean_edges         = dict()\n",
    "    clean_edges_labels  = dict()\n",
    "    \n",
    "    for key in edges:\n",
    "        \n",
    "        #Set edge weight\n",
    "        clean_edges[key]        = (100 - edges[key]) / configFactorEdge\n",
    "\n",
    "        #set edge layout\n",
    "        clean_edges_labels[key] = edgesTLabel[key]\n",
    "    \n",
    "    # Clean nodes\n",
    "    nodes       = nx.get_node_attributes(G,'weight')\n",
    "    nodesTLabel = nx.get_node_attributes(G,'tLabel')\n",
    "    nodesTColor = nx.get_node_attributes(G,'tColor')\n",
    "\n",
    "    clean_nodes         = dict()\n",
    "    clean_nodes_labels  = dict()\n",
    "    clean_nodes_color   = dict()\n",
    "    \n",
    "    for key in nodes:\n",
    "        \n",
    "        #Set node weight        \n",
    "        clean_nodes[key]        = nodes[key] / configFactorNode\n",
    "\n",
    "        #Set node layout\n",
    "        clean_nodes_labels[key] = nodesTLabel[key]\n",
    "        clean_nodes_color[key]  = nodesTColor[key]\n",
    "    \n",
    "    # Revert DiGraph (arrows direction)\n",
    "    G_rev = nx.DiGraph.reverse(G)    \n",
    "\n",
    "    # Draw\n",
    "    nx.draw(G_rev,\n",
    "        pos,\n",
    "        with_labels=True,\n",
    "        width=list(clean_edges.values()),\n",
    "        node_size=list(clean_nodes.values()),\n",
    "        labels=clean_nodes_labels,\n",
    "        node_color=list(clean_nodes_color.values()),\n",
    "        arrowsize=configArrowSize,\n",
    "        arrowstyle=\"wedge\"\n",
    "        #connectionstyle=\"arc3, rad = 0.1\"\n",
    "    )\n",
    "    \n",
    "    # Set labels\n",
    "    _ = nx.draw_networkx_edge_labels(G_rev, pos, edge_labels=clean_edges_labels)\n",
    "\n",
    "    plt.title(outputTitle)\n",
    "\n",
    "    # Save and show fig\n",
    "    if(outputFilename != \"\"):\n",
    "        plt.savefig(dir_var_output + outputFilename)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    gloStopStopwatch(\"Social Graph Plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates Test Graph\n",
    "def generateTestGraph():\n",
    "\n",
    "    G_weighted = nx.DiGraph()\n",
    "\n",
    "    G_weighted.add_edge(\"N1\", \"N2\", weight=100-30,  tLabel = \"(≙\" + str(100-30) + \")\")\n",
    "    G_weighted.add_edge(\"N1\", \"N3\", weight=100-10,  tLabel = \"(≙\" + str(100-10) + \")\")\n",
    "    G_weighted.add_edge(\"N1\", \"N4\", weight=100-60,  tLabel = \"(≙\" + str(100-60) + \")\")\n",
    "\n",
    "    G_weighted.add_edge(\"N4\", \"N5\", weight=100-80,  tLabel = \"(≙\" + str(100-80) + \")\")\n",
    "    G_weighted.add_edge(\"N4\", \"N6\", weight=100-10,  tLabel = \"(≙\" + str(100-10) + \")\")\n",
    "\n",
    "    G_weighted.add_edge(\"N4\", \"N7\", weight=100-30,   tLabel = \"(≙\" + str(100-30) + \")\")\n",
    "    G_weighted.add_edge(\"N7\", \"N4\", weight=100-70,   tLabel = \"(≙\" + str(100-70) + \")\")\n",
    "\n",
    "    G_weighted.add_node(\"N1\", weight=500.0, tLabel = \"N1-T\", tColor=\"red\")\n",
    "    G_weighted.add_node(\"N2\", weight=500.0, tLabel = \"N2-T\", tColor=\"blue\")\n",
    "    G_weighted.add_node(\"N3\", weight=500.0, tLabel = \"N3-T\", tColor=\"blue\")\n",
    "    G_weighted.add_node(\"N4\", weight=500.0, tLabel = \"N4-T\", tColor=\"red\")\n",
    "    G_weighted.add_node(\"N5\", weight=500.0, tLabel = \"N5-T\", tColor=\"red\")\n",
    "    G_weighted.add_node(\"N6\", weight=500.0, tLabel = \"N6-T\", tColor=\"red\")\n",
    "    G_weighted.add_node(\"N7\", weight=500.0, tLabel = \"N7-T\", tColor=\"blue\")\n",
    "\n",
    "    return G_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add node weight to dict\n",
    "# Only adds new weight if newWeight > oldWeight\n",
    "def addSocialGraphNodeWeight(chatName, chatWeight, targetDict):\n",
    "    \n",
    "    if(chatName in targetDict):\n",
    "        oldWeight = targetDict[chatName]\n",
    "        if(chatWeight > oldWeight):\n",
    "            targetDict[chatName] = chatWeight\n",
    "    else:\n",
    "        targetDict[chatName] = chatWeight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate social graph\n",
    "\n",
    "param   configTopNInfluencer        e.g. For top 10 = 10\n",
    "param   configMinRefs               e.g. 1 must have > 1 % forwarded messages\n",
    "param   listFilePaths               List process filePaths\n",
    "param   socialGraphTargetDict       e.g. forwarded from dict or hashtag dict\n",
    "param   socialGraphTargetAttribute  e.g. procEvalIsForwarded (for calc percent)\n",
    "param   configFlagDebugLabel        e.g. show debug info on label\n",
    "\"\"\"\n",
    "def generateSocialGraph(configTopNInfluencer, configMinRefs, listFilePaths, socialGraphTargetDict, socialGraphTargetAttribute, configFlagDebugLabel):\n",
    "    \n",
    "    # Save node weights to dict\n",
    "    dictSocialNodeWeights   = dict()\n",
    "\n",
    "    # Flag downloaded nodes (exact node weight)\n",
    "    dictExactNodesLabels    = {}\n",
    "    \n",
    "    gloStartStopwatch(\"Social Graph\")\n",
    "    \n",
    "    # Generate directed graph\n",
    "    G_weighted = nx.DiGraph()\n",
    "    \n",
    "    print(\"- Add edges\")\n",
    "    for fP in listFilePaths:\n",
    "        \n",
    "        # Query own params\n",
    "        chatName                        = queryChatName(fP)\n",
    "        chatNumberOfMessages            = queryNumberOfMessages(fP)\n",
    "        chatNumberOfTargetMessages      = queryNumberOfMessagesByAttEqTrue(fP, socialGraphTargetAttribute)\n",
    "\n",
    "        gloStartStopwatch(\"SG-Extract \" + chatName + \"(\" + str(chatNumberOfTargetMessages) + \"/\" + str(chatNumberOfMessages) + \" messages)\")\n",
    "        \n",
    "        # Add exact node size (chat downloaded) and flag node\n",
    "        addSocialGraphNodeWeight(chatName, chatNumberOfMessages, dictSocialNodeWeights)\n",
    "        dictExactNodesLabels[chatName] = str(chatName) + \"\\n=[\" + str(chatNumberOfTargetMessages) + \"/\" + str(chatNumberOfMessages) + \"]\"\n",
    "\n",
    "        # Extract social graph data and get top influencer\n",
    "        socialGraphData = socialGraphTargetDict[fP]\n",
    "        socialGraphData = socialGraphData.most_common(configTopNInfluencer)\n",
    "        \n",
    "        # Iterate over forwarder\n",
    "        for oChatName, oChatRefs in socialGraphData:\n",
    "            \n",
    "            # Query other params\n",
    "            oChatName    = gloConvertToSafeChatName(str(oChatName))\n",
    "            oChatRefs    = oChatRefs\n",
    "\n",
    "            # If has forwarder\n",
    "            if(oChatName != \"nan\"):\n",
    "        \n",
    "                # Calc percent (forwarded_messages)\n",
    "                per = (oChatRefs/chatNumberOfTargetMessages) * 100\n",
    "\n",
    "                # Filter unimportant forwarders\n",
    "                if(per > configMinRefs):\n",
    "                \n",
    "                    # Add estimanted node size (chat not downloaded)\n",
    "                    addSocialGraphNodeWeight(oChatName, oChatRefs, dictSocialNodeWeights)\n",
    "\n",
    "                    # Invert percent (distance)\n",
    "                    wei = 100 - per\n",
    "\n",
    "                    # Label\n",
    "                    if(configFlagDebugLabel):\n",
    "                        lab = str(round(per, 3)) + \"% (\" + str(oChatRefs) + \"/\" + str(chatNumberOfTargetMessages) + \"≙\" + str(round(wei, 3)) + \")\"\n",
    "                    else:\n",
    "                        lab = str(round(per, 3)) + \"% (\" + str(oChatRefs) + \"/\" + str(chatNumberOfTargetMessages) + \")\"\n",
    "\n",
    "                    # Add edge\n",
    "                    G_weighted.add_edge(\n",
    "                        chatName,\n",
    "                        oChatName,\n",
    "                        weight=wei,\n",
    "                        tLabel = lab\n",
    "                    )\n",
    "\n",
    "        gloStopStopwatch(\"SG-Extract \" + chatName + \"(\" + str(chatNumberOfTargetMessages) + \"/\" + str(chatNumberOfMessages) + \" messages)\")\n",
    "        \n",
    "    print(\"- Add different nodes\")\n",
    "    for aNode in dictSocialNodeWeights:\n",
    "        \n",
    "        # Query node params\n",
    "        nodeName   = str(aNode)\n",
    "        nodeWeight = dictSocialNodeWeights[aNode]\n",
    "\n",
    "        # Set defaults\n",
    "        tValueColor = \"#ff8000\"\n",
    "        tLabel = str(nodeName) + \"\\n≈[\" + str(nodeWeight) + \"]\"\n",
    "\n",
    "        # Overwrite (if chat downloaded = exact weight)\n",
    "        if(nodeName in dictExactNodesLabels):\n",
    "            tValueColor = \"#0080ff\"\n",
    "            tLabel = dictExactNodesLabels[nodeName]\n",
    "        \n",
    "        G_weighted.add_node(\n",
    "            nodeName,\n",
    "            weight=nodeWeight,\n",
    "            tLabel = tLabel,\n",
    "            tColor=tValueColor\n",
    "        )\n",
    "        \n",
    "    gloStopStopwatch(\"Social Graph\")\n",
    "        \n",
    "    return G_weighted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generatedTestGraph = generateTestGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawSocialGraph(\n",
    "    G = generatedTestGraph,\n",
    "    layoutSelector=1,\n",
    "    configFactorEdge = 10,\n",
    "    configFactorNode = 1,\n",
    "    configArrowSize = 15,\n",
    "    configPlotWidth = 8,\n",
    "    configPlotHeight = 4.5,\n",
    "    outputFilename = \"\",\n",
    "    outputTitle = \"Test Graph Kamda Kawai Layout\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawSocialGraph(\n",
    "    G = generatedTestGraph,\n",
    "    layoutSelector=2,\n",
    "    configFactorEdge = 10,\n",
    "    configFactorNode = 1,\n",
    "    configArrowSize = 15,\n",
    "    configPlotWidth = 8,\n",
    "    configPlotHeight = 4.5,\n",
    "    outputFilename = \"\",\n",
    "    outputTitle = \"Test Graph Spring Layout\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawSocialGraph(\n",
    "    G = generatedTestGraph,\n",
    "    layoutSelector=3,\n",
    "    configFactorEdge = 10,\n",
    "    configFactorNode = 1,\n",
    "    configArrowSize = 15,\n",
    "    configPlotWidth = 8,\n",
    "    configPlotHeight = 4.5,\n",
    "    outputFilename = \"\",\n",
    "    outputTitle = \"Test Graph Graphviz Layout\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real Graph (ForwardedFrom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "drawSocialGraph(\n",
    "    generateSocialGraph(\n",
    "        configTopNInfluencer = 100,  \n",
    "        configMinRefs = 0.5,       \n",
    "        listFilePaths = list(dfInputFiles[dfInputFiles.inputDesc == \"dataSet0\"].inputPath),\n",
    "        socialGraphTargetDict = dictSGD_ForwardedFrom,\n",
    "        socialGraphTargetAttribute = \"procEvalIsForwarded\",\n",
    "        configFlagDebugLabel = False\n",
    "    ),\n",
    "    layoutSelector = 1,\n",
    "    configFactorEdge = 100,\n",
    "    configFactorNode = 10,\n",
    "    configArrowSize = 15,\n",
    "    configPlotWidth = 32,\n",
    "    configPlotHeight = 32,\n",
    "    outputFilename = \"social-graph-dataSet0-forwarded-from.svg\",\n",
    "    outputTitle = \"Forwarded From (Top 100 by 0.5 percent) (dataSet0)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(\"dataSet2\" in C_LOAD_DATASETS):\n",
    "    drawSocialGraph(\n",
    "        generateSocialGraph(\n",
    "            configTopNInfluencer = 100,  \n",
    "            configMinRefs = 1,       \n",
    "            listFilePaths = list(dfInputFiles[dfInputFiles.inputDesc == \"dataSet2\"].inputPath),\n",
    "            socialGraphTargetDict = dictSGD_ForwardedFrom,\n",
    "            socialGraphTargetAttribute = \"procEvalIsForwarded\",\n",
    "            configFlagDebugLabel = False\n",
    "        ),\n",
    "        layoutSelector = 1,\n",
    "        configFactorEdge = 100,\n",
    "        configFactorNode = 10,\n",
    "        configArrowSize = 15,\n",
    "        configPlotWidth = 32,\n",
    "        configPlotHeight = 32,\n",
    "        outputFilename = \"social-graph-dataSet2-forwarded-from.svg\",\n",
    "        outputTitle = \"Forwarded From (Top 100 by 1 percent) (dataSet2)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enable\n",
    "\"\"\"\n",
    "drawSocialGraph(\n",
    "    generateSocialGraph(\n",
    "        configTopNInfluencer = 25,  \n",
    "        configMinRefs = 0,       \n",
    "        listFilePaths = list(dfInputFiles.inputPath),\n",
    "        socialGraphTargetDict = dictSGD_ForwardedFrom,\n",
    "        socialGraphTargetAttribute = \"procEvalIsForwarded\",\n",
    "        configFlagDebugLabel = False\n",
    "    ),\n",
    "    layoutSelector = 3,\n",
    "    configFactorEdge = 100,\n",
    "    configFactorNode = 10,\n",
    "    configArrowSize = 15,\n",
    "    configPlotWidth = 32,\n",
    "    configPlotHeight = 32,\n",
    "    outputFilename = \"social-graph-global-graphviz-forwarded-from.svg\",\n",
    "    outputTitle = \"Forwarded From (Top 25) (global - graphviz)\"\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real Graph (Hashtag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawSocialGraph(\n",
    "    generateSocialGraph(\n",
    "        configTopNInfluencer = 100,  \n",
    "        configMinRefs = 0.5,        \n",
    "        listFilePaths = list(dfInputFiles[dfInputFiles.inputDesc == \"dataSet0\"].inputPath),\n",
    "        socialGraphTargetDict = dictSGD_Hashtag,\n",
    "        socialGraphTargetAttribute = \"procEvalContainsHashtag\",\n",
    "        configFlagDebugLabel = False\n",
    "    ),\n",
    "    layoutSelector = 1,\n",
    "    configFactorEdge = 100,\n",
    "    configFactorNode = 10,\n",
    "    configArrowSize = 15,\n",
    "    configPlotWidth = 32,\n",
    "    configPlotHeight = 32,\n",
    "    outputFilename = \"social-graph-dataSet0-hashtag.svg\",\n",
    "    outputTitle = \"Hashtags (Top 100 by 0.5 percent) (dataSet0)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(\"dataSet2\" in C_LOAD_DATASETS and False): #TODO: Fix Graph\n",
    "    drawSocialGraph(\n",
    "        generateSocialGraph(\n",
    "            configTopNInfluencer = 30,  \n",
    "            configMinRefs = 4,        \n",
    "            listFilePaths = list(dfInputFiles[dfInputFiles.inputDesc == \"dataSet2\"].inputPath),\n",
    "            socialGraphTargetDict = dictSGD_Hashtag,\n",
    "            socialGraphTargetAttribute = \"procEvalContainsHashtag\",\n",
    "            configFlagDebugLabel = False\n",
    "        ),\n",
    "        layoutSelector = 1,\n",
    "        configFactorEdge = 100,\n",
    "        configFactorNode = 10,\n",
    "        configArrowSize = 15,\n",
    "        configPlotWidth = 32,\n",
    "        configPlotHeight = 32,\n",
    "        outputFilename = \"social-graph-dataSet2-hashtag.svg\",\n",
    "        outputTitle = \"Hashtags (Top 30) (dataSet2)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real Graph (Hosts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawSocialGraph(\n",
    "    generateSocialGraph(\n",
    "        configTopNInfluencer = 100,  \n",
    "        configMinRefs = 0.5,        \n",
    "        listFilePaths = list(dfInputFiles[dfInputFiles.inputDesc == \"dataSet0\"].inputPath),\n",
    "        socialGraphTargetDict = dictSGD_Host,\n",
    "        socialGraphTargetAttribute = \"procEvalContainsUrl\",\n",
    "        configFlagDebugLabel = False\n",
    "    ),\n",
    "    layoutSelector = 1,\n",
    "    configFactorEdge = 100,\n",
    "    configFactorNode = 10,\n",
    "    configArrowSize = 15,\n",
    "    configPlotWidth = 32,\n",
    "    configPlotHeight = 32,\n",
    "    outputFilename = \"social-graph-dataSet0-host.svg\",\n",
    "    outputTitle = \"Hosts (Top 100 by 0.5 percent) (dataSet0)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(\"dataSet2\" in C_LOAD_DATASETS):\n",
    "    drawSocialGraph(\n",
    "        generateSocialGraph(\n",
    "            configTopNInfluencer = 100,  \n",
    "            configMinRefs = 1,        \n",
    "            listFilePaths = list(dfInputFiles[dfInputFiles.inputDesc == \"dataSet2\"].inputPath),\n",
    "            socialGraphTargetDict = dictSGD_Host,\n",
    "            socialGraphTargetAttribute = \"procEvalContainsUrl\",\n",
    "            configFlagDebugLabel = False\n",
    "        ),\n",
    "        layoutSelector = 1,\n",
    "        configFactorEdge = 100,\n",
    "        configFactorNode = 10,\n",
    "        configArrowSize = 15,\n",
    "        configPlotWidth = 32,\n",
    "        configPlotHeight = 32,\n",
    "        outputFilename = \"social-graph-dataSet2-host.svg\",\n",
    "        outputTitle = \"Hosts (Top 100 by 1 percent) (dataSet2)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real Graph (Emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawSocialGraph(\n",
    "    generateSocialGraph(\n",
    "        configTopNInfluencer = 100,  \n",
    "        configMinRefs = 0.5,        \n",
    "        listFilePaths = list(dfInputFiles[dfInputFiles.inputDesc == \"dataSet0\"].inputPath),\n",
    "        socialGraphTargetDict = dictSGD_Emoji,\n",
    "        socialGraphTargetAttribute = \"procEvalContainsEmojiItem\",\n",
    "        configFlagDebugLabel = False\n",
    "    ),\n",
    "    layoutSelector = 1,\n",
    "    configFactorEdge = 100,\n",
    "    configFactorNode = 10,\n",
    "    configArrowSize = 15,\n",
    "    configPlotWidth = 32,\n",
    "    configPlotHeight = 32,\n",
    "    outputFilename = \"social-graph-dataSet0-emoji.svg\",\n",
    "    outputTitle = \"Emojis (Top 100 by 0.5 percent) (dataSet0)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(\"dataSet2\" in C_LOAD_DATASETS):\n",
    "    drawSocialGraph(\n",
    "        generateSocialGraph(\n",
    "            configTopNInfluencer = 100,  \n",
    "            configMinRefs = 1,        \n",
    "            listFilePaths = list(dfInputFiles[dfInputFiles.inputDesc == \"dataSet2\"].inputPath),\n",
    "            socialGraphTargetDict = dictSGD_Emoji,\n",
    "            socialGraphTargetAttribute = \"procEvalContainsEmojiItem\",\n",
    "            configFlagDebugLabel = False\n",
    "        ),\n",
    "        layoutSelector = 1,\n",
    "        configFactorEdge = 100,\n",
    "        configFactorNode = 10,\n",
    "        configArrowSize = 15,\n",
    "        configPlotWidth = 32,\n",
    "        configPlotHeight = 32,\n",
    "        outputFilename = \"social-graph-dataSet2-emoji.svg\",\n",
    "        outputTitle = \"Emojis (Top 100 by 1 percent) (dataSet2)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real Graph (From)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(\"dataSet1a\" in C_LOAD_DATASETS):\n",
    "    drawSocialGraph(\n",
    "        generateSocialGraph(\n",
    "            configTopNInfluencer = 50,  \n",
    "            configMinRefs = 0,        \n",
    "            listFilePaths = list(dfInputFiles[dfInputFiles.inputDesc == \"dataSet1a\"].inputPath),\n",
    "            socialGraphTargetDict = dictSGD_From,\n",
    "            socialGraphTargetAttribute = \"procEvalIsValidText\", #TODO: Improve\n",
    "            configFlagDebugLabel = False\n",
    "        ),\n",
    "        layoutSelector = 1,\n",
    "        configFactorEdge = 100,\n",
    "        configFactorNode = 1,\n",
    "        configArrowSize = 15,\n",
    "        configPlotWidth = 32,\n",
    "        configPlotHeight = 32,\n",
    "        outputFilename = \"social-graph-dataSet1a-from.svg\",\n",
    "        outputTitle = \"From (Top 50) (dataSet1a)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(\"dataSet2\" in C_LOAD_DATASETS):\n",
    "    drawSocialGraph(\n",
    "        generateSocialGraph(\n",
    "            configTopNInfluencer = 50,  \n",
    "            configMinRefs = 0,        \n",
    "            listFilePaths = [\n",
    "                \"DS-13-01-2021/ChatExport_2021-01-13-querdenken089\",\n",
    "                \"DS-13-01-2021/ChatExport_2021-01-13-querdenken69\",\n",
    "                \"DS-13-01-2021/ChatExport_2021-01-13-querdenken773\",\n",
    "                \"DS-13-01-2021/ChatExport_2021-01-13-querdenken711\"\n",
    "                ],\n",
    "            socialGraphTargetDict = dictSGD_From,\n",
    "            socialGraphTargetAttribute = \"procEvalIsValidText\", #TODO: Improve\n",
    "            configFlagDebugLabel = False\n",
    "        ),\n",
    "        layoutSelector = 1,\n",
    "        configFactorEdge = 100,\n",
    "        configFactorNode = 1,\n",
    "        configArrowSize = 15,\n",
    "        configPlotWidth = 32,\n",
    "        configPlotHeight = 32,\n",
    "        outputFilename = \"social-graph-groups-dataSet2-from.svg\",\n",
    "        outputTitle = \"From (Top 50) (dataSet2-groups)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "param   targetDate      e.g. 1970-01-01\n",
    "param   fP              filePath\n",
    "param   highlightWord   set \"\" = no filter\n",
    "\"\"\"\n",
    "def queryNumberOfMessagesByDate(targetDate, fP, highlightWord):\n",
    "\n",
    "    df = dictMessages[fP].copy()\n",
    "\n",
    "    df = df[df.procEvalIsValidText == True]\n",
    "\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    \n",
    "    df = df[df.date <= targetDate]\n",
    "\n",
    "    if(highlightWord != \"\"):\n",
    "        df = df[df.procTDSafeLowercaseText.str.contains(highlightWord)]\n",
    "\n",
    "    l = len(df.index)\n",
    "\n",
    "    if(l > 0):\n",
    "        return l\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "param   filePathList\n",
    "param   outputFilename  set \"\" = no output file\n",
    "param   highlightWords  list of highlight words (leave empty if not used)\n",
    "param   configFrequency e.g. 1D or 1M\n",
    "\"\"\"\n",
    "# TODO: Add percent to label\n",
    "def drawTimePlot(filePathList, outputFilename, highlightWords, configFrequency):\n",
    "\n",
    "    gloStartStopwatch(\"Time Plot\")\n",
    "\n",
    "    plt.figure(figsize=(16, 9))\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        index=pd.date_range( #m/d/y\n",
    "            start='9/1/2018',\n",
    "            end='2/1/2021',\n",
    "            freq=configFrequency\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Add date to process\n",
    "    df[\"date\"] = df.index\n",
    "\n",
    "    vLineHeight = -1\n",
    "\n",
    "    for fP in filePathList:\n",
    "        gloStartStopwatch(\"Time Plot >>\" + fP + \"<<\")\n",
    "\n",
    "        # Plot Graph Var 1\n",
    "        if not highlightWords:\n",
    "            # Plot\n",
    "            plt.plot(\n",
    "                df.index, #x\n",
    "                df.apply(lambda x: queryNumberOfMessagesByDate(x.date, fP, highlightWord = \"\"), axis=1), #y\n",
    "                label = queryChatName(fP) #label\n",
    "            )\n",
    "            # Set vline height\n",
    "            currentHeight = queryNumberOfMessagesByAttEqTrue(fP, \"procEvalIsValidText\")\n",
    "            if(currentHeight > vLineHeight):\n",
    "                vLineHeight = currentHeight\n",
    "\n",
    "        # Plot High Light Word Graph / Var 2\n",
    "        for hWord in highlightWords:\n",
    "            y = df.apply(lambda x: queryNumberOfMessagesByDate(x.date, fP, highlightWord = hWord), axis=1)\n",
    "            # Plot\n",
    "            plt.plot(\n",
    "                df.index, #x\n",
    "                y, #y\n",
    "                label = queryChatName(fP) + \" usage of '\" + hWord + \"'\" #label\n",
    "            )\n",
    "            # Set vline height\n",
    "            currentHeight = y.max()\n",
    "            if(currentHeight > vLineHeight):\n",
    "                vLineHeight = currentHeight\n",
    "\n",
    "        gloStopStopwatch(\"Time Plot >>\" + fP + \"<<\")\n",
    "\n",
    "    # yy - mm - dd\n",
    "    # TODO: Double check https://www.bundesgesundheitsministerium.de/coronavirus/chronik-coronavirus.html?stand=20210104\n",
    "    plt.vlines(x = [\"2018-12-10\"], ymin=0, ymax=vLineHeight, color=\"orange\", ls='--', label=\"Global Compact for Migration (2018-12-10)\")\n",
    "    plt.vlines(x = [\"2020-01-27\"], ymin=0, ymax=vLineHeight, color=\"grey\", ls='--', label=\"Corona Patient Zero Germany\")\n",
    "    plt.vlines(x = [\"2020-03-23\"], ymin=0, ymax=vLineHeight, color=\"purple\", ls='--', label=\"1. Lockdown Germany (2020-03-23)\")\n",
    "    plt.vlines(x = [\"2020-11-02\"], ymin=0, ymax=vLineHeight, color=\"purple\", ls='--', label=\"2. Lockdown light Germany (2020-11-02)\")\n",
    "    plt.vlines(x = [\"2020-12-16\"], ymin=0, ymax=vLineHeight, color=\"purple\", ls='--', label=\"3. Lockdown Germany (2020-12-16)\")\n",
    "\n",
    "    plt.gcf().autofmt_xdate()\n",
    "    _ = plt.legend()\n",
    "\n",
    "    if(outputFilename != \"\"):\n",
    "        plt.savefig(dir_var_output + outputFilename)\n",
    "\n",
    "    gloStopStopwatch(\"Time Plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    drawTimePlot(\n",
    "        filePathList = list(dfInputFiles[dfInputFiles.inputDesc == \"dataSet0\"].inputPath),\n",
    "        outputFilename = \"time-plot-dataSet0.svg\",\n",
    "        highlightWords = [],\n",
    "        configFrequency=C_TIME_PLOT_FREQ\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    if(\"dataSet1\" in C_LOAD_DATASETS):\n",
    "        drawTimePlot(\n",
    "            filePathList = list(dfInputFiles[dfInputFiles.inputDesc == \"dataSet1\"].inputPath),\n",
    "            outputFilename = \"time-plot-dataSet1.svg\",\n",
    "            highlightWords = [],\n",
    "            configFrequency=C_TIME_PLOT_FREQ\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    if(\"dataSet1a\" in C_LOAD_DATASETS):\n",
    "        drawTimePlot(\n",
    "            filePathList = list(dfInputFiles[dfInputFiles.inputDesc == \"dataSet1a\"].inputPath),\n",
    "            outputFilename = \"time-plot-dataSet1a.svg\",\n",
    "            highlightWords = [],\n",
    "            configFrequency=C_TIME_PLOT_FREQ\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    if(\"dataSet2\" in C_LOAD_DATASETS):\n",
    "        drawTimePlot(\n",
    "            filePathList = list(dfInputFiles[dfInputFiles.inputDesc == \"dataSet2\"].inputPath),\n",
    "            outputFilename = \"time-plot-dataSet2.svg\",\n",
    "            highlightWords = [],\n",
    "            configFrequency=C_TIME_PLOT_FREQ\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Context?\n",
    "# TODO: Improve stop words\n",
    "\n",
    "\"\"\"\n",
    "WordCloud\n",
    "\n",
    "param   targetDataFrame     DataFrame\n",
    "param   outputFilename      filename in outputdir (set \"\" == no output file)\n",
    "param   filterList          Exclude list\n",
    "param   flagShow            Set true == show wordcloud\n",
    "param   configPlotWidth     e.g. 1920\n",
    "param   configPlotHeight    e.g. 1080\n",
    "\"\"\"\n",
    "def generateWordCloud(targetDataFrame, outputFilename, filterList, flagShow, configPlotWidth, configPlotHeight):\n",
    "    \n",
    "\n",
    "    dfMessages = targetDataFrame.copy()\n",
    "    \n",
    "    textString = gloGenerateTextFromChat(dfMessages, rowID=\"procTDSafeText\")\n",
    "    \n",
    "    stopWordsList = gloGetStopWordsList(filterList)\n",
    "    \n",
    "    # Generate word cloud and save it to file\n",
    "    wordcloud = WordCloud(\n",
    "                background_color=\"black\",\n",
    "                width=configPlotWidth,\n",
    "                height=configPlotHeight,\n",
    "                stopwords=stopWordsList\n",
    "            ).generate(textString)\n",
    "\n",
    "    if(outputFilename != \"\"):\n",
    "        wordcloud.to_file(dir_var_output + outputFilename)\n",
    "    \n",
    "    if(flagShow):\n",
    "        # Show top 20\n",
    "        print()\n",
    "        print(\"Top 20 occ:\\n\" + str(pd.Series(wordcloud.words_).head(20)))\n",
    "        print()\n",
    "        \n",
    "        # Show word cloud\n",
    "        print(\"- Start generate figure\")\n",
    "        plt.figure(figsize=(14, 14))\n",
    "        plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oliver Janich öffentlich (public_channel - dataSet0)\n",
    "generateWordCloud(\n",
    "    dictMessages[\"DS-05-01-2021/ChatExport_2021-01-05-janich\"],\n",
    "    \"wordcloud-oliver-janich.png\",\n",
    "    [],\n",
    "    flagShow = True,\n",
    "    configPlotWidth = 1920,\n",
    "    configPlotHeight = 1080\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATTILA HILDMANN OFFICIAL (public_channel - dataSet0)\n",
    "if(C_SHORT_RUN == False):\n",
    "    generateWordCloud(\n",
    "        dictMessages[\"DS-05-01-2021/ChatExport_2021-01-05-hildmann\"],\n",
    "        \"wordcloud-attila-hildmann.png\",\n",
    "        [],\n",
    "        flagShow = True,\n",
    "        configPlotWidth = 1920,\n",
    "        configPlotHeight = 1080\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eva Herman Offiziell (public_channel - dataSet0)\n",
    "if(C_SHORT_RUN == False):\n",
    "    generateWordCloud(\n",
    "        dictMessages[\"DS-05-01-2021/ChatExport_2021-01-05-evaherman\"],\n",
    "        \"wordcloud-eva-herman.png\",\n",
    "        [],\n",
    "        flagShow = True,\n",
    "        configPlotWidth = 1920,\n",
    "        configPlotHeight = 1080\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Xavier Naidoo (public_channel - dataSet0)\n",
    "if(C_SHORT_RUN == False):\n",
    "    generateWordCloud(\n",
    "        dictMessages[\"DS-05-01-2021/ChatExport_2021-01-05-xavier\"],\n",
    "        \"wordcloud-xavier-naidoo.png\",\n",
    "        [],\n",
    "        flagShow = True,\n",
    "        configPlotWidth = 1920,\n",
    "        configPlotHeight = 1080\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    if(\"dataSet2\" in C_LOAD_DATASETS):\n",
    "        generateWordCloud(\n",
    "            dictMessages[\"DS-13-01-2021/ChatExport_2021-01-13-querdenken089\"],\n",
    "            \"wordcloud-querdenken-089-group.png\",\n",
    "            [],\n",
    "            flagShow = True,\n",
    "            configPlotWidth = 1920,\n",
    "            configPlotHeight = 1080\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    if(\"dataSet2\" in C_LOAD_DATASETS):\n",
    "        generateWordCloud(\n",
    "            dictMessages[\"DS-13-01-2021/ChatExport_2021-01-13-querdenken591Info\"],\n",
    "            \"wordcloud-querdenken-591-info.png\",\n",
    "            [],\n",
    "            flagShow = True,\n",
    "            configPlotWidth = 1920,\n",
    "            configPlotHeight = 1080\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    if(\"dataSet2\" in C_LOAD_DATASETS):\n",
    "        generateWordCloud(\n",
    "            dictMessages[\"DS-13-01-2021/ChatExport_2021-01-13-querdenken773\"],\n",
    "            \"wordcloud-querdenken-773-group.png\",\n",
    "            [],\n",
    "            flagShow = True,\n",
    "            configPlotWidth = 1920,\n",
    "            configPlotHeight = 1080\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    if(\"dataSet2\" in C_LOAD_DATASETS):\n",
    "        generateWordCloud(\n",
    "            dictMessages[\"DS-13-01-2021/ChatExport_2021-01-13-querdenken773Info\"],\n",
    "            \"wordcloud-querdenken-773-info.png\",\n",
    "            [],\n",
    "            flagShow = True,\n",
    "            configPlotWidth = 1920,\n",
    "            configPlotHeight = 1080\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    if(\"dataSet2\" in C_LOAD_DATASETS):\n",
    "        generateWordCloud(\n",
    "            dictMessages[\"DS-13-01-2021/ChatExport_2021-01-13-querdenken711\"],\n",
    "            \"wordcloud-querdenken-711-group.png\",\n",
    "            [],\n",
    "            flagShow = True,\n",
    "            configPlotWidth = 1920,\n",
    "            configPlotHeight = 1080\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    if(\"dataSet2\" in C_LOAD_DATASETS):\n",
    "        generateWordCloud(\n",
    "            dictMessages[\"DS-13-01-2021/ChatExport_2021-01-13-querdenken711Info\"],\n",
    "            \"wordcloud-querdenken-711-info.png\",\n",
    "            [],\n",
    "            flagShow = True,\n",
    "            configPlotWidth = 1920,\n",
    "            configPlotHeight = 1080\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    if(\"dataSet2\" in C_LOAD_DATASETS):\n",
    "        generateWordCloud(\n",
    "            dictMessages[\"DS-13-01-2021/ChatExport_2021-01-13-querdenken69\"],\n",
    "            \"wordcloud-querdenken-69-group.png\",\n",
    "            [],\n",
    "            flagShow = True,\n",
    "            configPlotWidth = 1920,\n",
    "            configPlotHeight = 1080\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    if(\"dataSet2\" in C_LOAD_DATASETS):\n",
    "        generateWordCloud(\n",
    "            dictMessages[\"DS-13-01-2021/ChatExport_2021-01-13-querdenken69Info\"],\n",
    "            \"wordcloud-querdenken-69-info.png\",\n",
    "            [],\n",
    "            flagShow = True,\n",
    "            configPlotWidth = 1920,\n",
    "            configPlotHeight = 1080\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for test purposes\n",
    "if(C_SHORT_RUN == False and False): #TODO: Enable\n",
    "    generateWordCloud(\n",
    "        dfAllDataMessages,\n",
    "        \"\",\n",
    "        [],\n",
    "        flagShow = True,\n",
    "        configPlotWidth = 1920,\n",
    "        configPlotHeight = 1080\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto Mode (Batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractTimePeriodDataFrame(df, timeStart, timeStop):\n",
    "\n",
    "    #print(\"- Got Start \" + str(timeStart) + \" and Stop \" + str(timeStop))\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    \n",
    "    dfNew = df[df.date <= timeStop]\n",
    "    dfNew = dfNew[dfNew.date >= timeStart]\n",
    "\n",
    "    dfNew = dfNew.set_index(\"date\")\n",
    "    dfNew = dfNew.sort_index()\n",
    "\n",
    "    return dfNew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateWCPeriod():\n",
    "    return list(pd.date_range( #m/d/y\n",
    "            start='1/1/2018',\n",
    "            end='2/1/2021',\n",
    "            #freq=\"W-MON\",\n",
    "            freq=\"1M\"\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Wrapper WordCloud\n",
    "\n",
    "param   fP  filePath\n",
    "param   label e.g. chatName\n",
    "param   filterList additional stopWords\n",
    "\"\"\"\n",
    "def generateWordCloudAuto(fP, label, filterList):\n",
    "\n",
    "    gloStartStopwatch(\"Generate World Cloud Auto >>\" + fP + \"<<\")\n",
    "\n",
    "    periods = generateWCPeriod()\n",
    "\n",
    "    dictSaved = {}\n",
    "\n",
    "    prevStart = periods[0]\n",
    "\n",
    "    for period in periods:\n",
    "\n",
    "        stop = period\n",
    "\n",
    "        e = extractTimePeriodDataFrame(dictMessages[fP], timeStart = prevStart, timeStop = stop)\n",
    "\n",
    "        if(prevStart != stop and len(e.index) > 0):\n",
    "            fileName = \"autoWordCloud/\" + queryChatName(fP) + \"-\" + str(prevStart) + \"-\" + str(stop) + \".png\"\n",
    "            generateWordCloud(\n",
    "                e,\n",
    "                fileName,\n",
    "                filterList,\n",
    "                flagShow = False,\n",
    "                configPlotWidth = 1280,\n",
    "                configPlotHeight = 720\n",
    "            )\n",
    "            #print(\"- Save file \" + fileName)\n",
    "            dictSaved[fileName] = str(prevStart) + \" - \" + str(stop)\n",
    "\n",
    "        \"\"\"\n",
    "        else:\n",
    "            print(\"- Start and Stop equal or no message found\")\n",
    "        \"\"\"\n",
    "\n",
    "        prevStart = stop\n",
    "\n",
    "    gloWriteDictToFile(\"auto-wordcloud-\" + label + \".csv\", dictSaved)\n",
    "\n",
    "    gloStopStopwatch(\"Generate World Cloud Auto >>\" + fP + \"<<\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    generateWordCloudAuto(\n",
    "        fP = \"DS-05-01-2021/ChatExport_2021-01-05-janich\",\n",
    "        label = \"oliver-janich\",\n",
    "        filterList = []\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    generateWordCloudAuto(\n",
    "        fP = \"DS-05-01-2021/ChatExport_2021-01-05-hildmann\",\n",
    "        label = \"attila-hildmann\",\n",
    "        filterList = []\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    generateWordCloudAuto(\n",
    "        fP = \"DS-05-01-2021/ChatExport_2021-01-05-evaherman\",\n",
    "        label = \"eva-herman\",\n",
    "        filterList = []\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    generateWordCloudAuto(\n",
    "        fP = \"DS-05-01-2021/ChatExport_2021-01-05-xavier\",\n",
    "        label = \"xavier-naidoo\",\n",
    "        filterList = []\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateNGram(text, n):\n",
    "    # https://albertauyeung.github.io/2018/06/03/generating-ngrams.html\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "    tokens = [token for token in text.split(\" \") if token != \"\"]\n",
    "    \n",
    "    return list(ngrams(tokens, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateNGramChat(fP, n, mostCommon):\n",
    "    return Counter(\n",
    "        generateNGram(\n",
    "            gloGenerateTextFromChat(dictMessages[fP], rowID=\"procTDSafeText\"),\n",
    "            n = n\n",
    "        )\n",
    "    ).most_common(mostCommon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateNGramAuto(filePathList, n, mostCommon):\n",
    "    for fP in filePathList:\n",
    "\n",
    "        print()\n",
    "        print(\"Analyse now >>\" + fP + \"<<\")\n",
    "\n",
    "        c = generateNGramChat(\n",
    "            fP,\n",
    "            n = n,\n",
    "            mostCommon = mostCommon\n",
    "        )\n",
    "\n",
    "        print (\"\\n\".join(map(str, c)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataSet0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generateNGramAuto(\n",
    "    dfInputFiles[dfInputFiles.inputDesc == \"dataSet0\"].inputPath,\n",
    "    n = 2,\n",
    "    mostCommon = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generateNGramAuto(\n",
    "    dfInputFiles[dfInputFiles.inputDesc == \"dataSet0\"].inputPath,\n",
    "    n = 3,\n",
    "    mostCommon = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generateNGramAuto(\n",
    "    dfInputFiles[dfInputFiles.inputDesc == \"dataSet0\"].inputPath,\n",
    "    n = 4,\n",
    "    mostCommon = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generateNGramAuto(\n",
    "    dfInputFiles[dfInputFiles.inputDesc == \"dataSet0\"].inputPath,\n",
    "    n = 5,\n",
    "    mostCommon = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generateNGramAuto(\n",
    "    dfInputFiles[dfInputFiles.inputDesc == \"dataSet0\"].inputPath,\n",
    "    n = 6,\n",
    "    mostCommon = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excerpt DataSet2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(\"dataSet2\" in C_LOAD_DATASETS):\n",
    "    generateNGramAuto(\n",
    "        dfInputFiles[dfInputFiles.inputDesc == \"dataSet2\"].inputPath,\n",
    "        n = 2,\n",
    "        mostCommon = 10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(\"dataSet2\" in C_LOAD_DATASETS):\n",
    "    generateNGramAuto(\n",
    "        dfInputFiles[dfInputFiles.inputDesc == \"dataSet2\"].inputPath,\n",
    "        n = 3,\n",
    "        mostCommon = 10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(\"dataSet2\" in C_LOAD_DATASETS):\n",
    "    generateNGramAuto(\n",
    "        dfInputFiles[dfInputFiles.inputDesc == \"dataSet2\"].inputPath,\n",
    "        n = 5,\n",
    "        mostCommon = 10\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freq Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateFreqNounsPlot(fP, mostCommon, outputFilename):\n",
    "\n",
    "    gloStartStopwatch(\"Generate text\")\n",
    "    df = dictMessages[fP].copy()\n",
    "    inputText = gloGenerateTextFromChat(df, \"procTDCleanText\")\n",
    "    gloStopStopwatch(\"Generate text\")\n",
    "\n",
    "    gloStartStopwatch(\"Process data\")\n",
    "    plotFreqNouns(inputText, outputFilename=outputFilename, mostCommon=mostCommon, flagRemoveStopwords=True)\n",
    "    gloStopStopwatch(\"Process data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    generateFreqNounsPlot(\"DS-05-01-2021/ChatExport_2021-01-05-janich\", mostCommon=25, outputFilename = \"freq-nouns-oliver-janich.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    generateFreqNounsPlot(\"DS-05-01-2021/ChatExport_2021-01-05-hildmann\", mostCommon=25, outputFilename = \"freq-nouns-attila-hildmann.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    generateFreqNounsPlot(\"DS-05-01-2021/ChatExport_2021-01-05-evaherman\", mostCommon=25, outputFilename = \"freq-nouns-eva-herman.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    generateFreqNounsPlot(\"DS-05-01-2021/ChatExport_2021-01-05-xavier\", mostCommon=25, outputFilename = \"freq-nouns-xavier-naidoo.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP with Bert and co."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval Ner Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalNerPipeline(pipelineKey, inputSelector, configTopN):\n",
    "\n",
    "    if(inputSelector in C_PIPELINE_DATASETS):\n",
    "        \n",
    "        filePaths = dfInputFiles[dfInputFiles.inputDesc == inputSelector].inputPath\n",
    "\n",
    "        for fP in filePaths:\n",
    "            \n",
    "            gloStartStopwatch(\"Process now >>\" + str(fP) + \"<<\")\n",
    "\n",
    "            if(pipelineKey == \"ner-xlm-roberta\" or pipelineKey == \"ner-bert\"):\n",
    "                \n",
    "                df = dictMessages[fP].copy()\n",
    "                df = df[df.procEvalIsValidText == True]\n",
    "                \n",
    "                listPer     = list()\n",
    "                listMisc    = list()\n",
    "                listOrg     = list()\n",
    "                listLoc     = list()\n",
    "\n",
    "                for index, row in df.iterrows():\n",
    "\n",
    "                    d = row[\"procPipeline-\" + pipelineKey]\n",
    "                    \n",
    "                    listPer.extend(d[\"per\"])\n",
    "                    listMisc.extend(d[\"misc\"])\n",
    "                    listOrg.extend(d[\"org\"])\n",
    "                    listLoc.extend(d[\"loc\"])\n",
    "\n",
    "                print(\"- Top per -\")\n",
    "                print (\"\\n\".join(map(str, Counter(listPer).most_common(configTopN))))\n",
    "                print()\n",
    "\n",
    "                print(\"- Top misc -\")\n",
    "                print (\"\\n\".join(map(str, Counter(listMisc).most_common(configTopN))))\n",
    "                print()\n",
    "\n",
    "                print(\"- Top org -\")\n",
    "                print (\"\\n\".join(map(str, Counter(listOrg).most_common(configTopN))))\n",
    "                print()\n",
    "\n",
    "                print(\"- Top loc -\")\n",
    "                print (\"\\n\".join(map(str, Counter(listLoc).most_common(configTopN))))\n",
    "                print()\n",
    "\n",
    "            else:\n",
    "                print(\"Error pipeline not found >>\" + str(pipelineKey) + \"<<\")\n",
    "\n",
    "            gloStopStopwatch(\"Process now >>\" + str(fP) + \"<<\")\n",
    "\n",
    "    else:\n",
    "        print(\"Error data not found >>\" + inputSelector + \"<<\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalNerPipeline(\"ner-xlm-roberta\", \"dataSet0\", configTopN = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalNerPipeline(\"ner-bert\", \"dataSet0\", configTopN = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval Sen Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalSenPipeline(pipelineKey, inputSelector, outputFilename, configRolling, configShowScatter):\n",
    "\n",
    "    if(inputSelector in C_PIPELINE_DATASETS):\n",
    "        \n",
    "        filePaths = dfInputFiles[dfInputFiles.inputDesc == inputSelector].inputPath\n",
    "\n",
    "        plt.figure(figsize=(32, 18))\n",
    "\n",
    "        for fP in filePaths:\n",
    "            \n",
    "            gloStartStopwatch(\"Process now >>\" + str(fP) + \"<<\")\n",
    "\n",
    "            if(pipelineKey == \"sen-bert\"):\n",
    "                \n",
    "                df = dictMessages[fP].copy()\n",
    "                df = df[df.procEvalIsValidText == True]\n",
    "\n",
    "                df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "                df = df.set_index(\"date\")\n",
    "                df = df.sort_index()\n",
    "\n",
    "                # key = x = time / value = y = score\n",
    "                dictData = {}\n",
    "\n",
    "                for index, row in df.iterrows():\n",
    "                    \n",
    "                    date = index\n",
    "                    score = row[\"procPipeline-sen-bert\"]\n",
    "\n",
    "                    if(score != -1):\n",
    "                        dictData[date] = score\n",
    "\n",
    "                # Plot\n",
    "                x,y = zip(*sorted(dictData.items()))\n",
    "                \n",
    "                df = pd.DataFrame(list(zip(x, y)), columns =['x', 'y'])\n",
    "\n",
    "                df['rolling'] = df.y.rolling(configRolling).mean()\n",
    "\n",
    "                sns.lineplot(data=df, x=\"x\", y=\"rolling\", label = queryChatName(fP))\n",
    "\n",
    "                if(configShowScatter):\n",
    "                    sns.scatterplot(data=df, x=\"x\", y=\"y\", label = queryChatName(fP), marker=\"+\")\n",
    "\n",
    "                plt.gcf().autofmt_xdate()\n",
    "\n",
    "                # Add vlines\n",
    "                vLineMin = 2\n",
    "                vLineMax = 4\n",
    "\n",
    "            elif(pipelineKey==\"sentiment\"):\n",
    "\n",
    "                df = dictMessages[fP].copy()\n",
    "                df = df[df.procEvalIsValidText == True]\n",
    "\n",
    "                df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "                df = df.set_index(\"date\")\n",
    "                df = df.sort_index()\n",
    "\n",
    "                # key = x = time / value = y = score\n",
    "                dictData = {}\n",
    "\n",
    "                for index, row in df.iterrows():\n",
    "                    \n",
    "                    date = index\n",
    "                    retDict = row[\"procPipeline-sentiment\"]\n",
    "\n",
    "                    if(retDict != None):\n",
    "                        polarity = retDict[\"polarity\"]\n",
    "                        dictData[date] = polarity\n",
    "\n",
    "                # Plot\n",
    "                x,y = zip(*sorted(dictData.items()))\n",
    "\n",
    "                df = pd.DataFrame(list(zip(x, y)), columns =['x', 'y'])\n",
    "\n",
    "                df['rolling'] = df.y.rolling(configRolling).mean()\n",
    "\n",
    "                sns.lineplot(data=df, x=\"x\", y=\"rolling\", label = queryChatName(fP))\n",
    "\n",
    "                if(configShowScatter):\n",
    "                    sns.scatterplot(data=df, x=\"x\", y=\"y\", label = queryChatName(fP), marker=\"+\")\n",
    "\n",
    "                plt.gcf().autofmt_xdate()\n",
    "\n",
    "                # Add vlines\n",
    "                vLineMin = -0.05\n",
    "                vLineMax = 0.175\n",
    "\n",
    "            elif(pipelineKey==\"subjectivity\"):\n",
    "\n",
    "                df = dictMessages[fP].copy()\n",
    "                df = df[df.procEvalIsValidText == True]\n",
    "\n",
    "                df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "                df = df.set_index(\"date\")\n",
    "                df = df.sort_index()\n",
    "\n",
    "                # key = x = time / value = y = score\n",
    "                dictData = {}\n",
    "\n",
    "                for index, row in df.iterrows():\n",
    "                    \n",
    "                    date = index\n",
    "                    retDict = row[\"procPipeline-sentiment\"]\n",
    "\n",
    "                    if(retDict != None):\n",
    "\n",
    "                        subjectivity = retDict[\"subjectivity\"]\n",
    "                        dictData[date] = subjectivity\n",
    "\n",
    "                # Plot\n",
    "                x,y = zip(*sorted(dictData.items()))\n",
    "\n",
    "                df = pd.DataFrame(list(zip(x, y)), columns =['x', 'y'])\n",
    "\n",
    "                df['rolling'] = df.y.rolling(configRolling).mean()\n",
    "\n",
    "                sns.lineplot(data=df, x=\"x\", y=\"rolling\", label = queryChatName(fP))\n",
    "\n",
    "                if(configShowScatter):\n",
    "                    sns.scatterplot(data=df, x=\"x\", y=\"y\", label = queryChatName(fP), marker=\"+\")\n",
    "\n",
    "                plt.gcf().autofmt_xdate()\n",
    "\n",
    "                # Add vlines\n",
    "                vLineMin = 0\n",
    "                vLineMax = 0.10\n",
    "                \n",
    "            else:\n",
    "                print(\"Error pipeline not found >>\" + str(pipelineKey) + \"<<\")\n",
    "\n",
    "            gloStopStopwatch(\"Process now >>\" + str(fP) + \"<<\")\n",
    "\n",
    "        # yy - mm - dd\n",
    "        # TODO: Double check https://www.bundesgesundheitsministerium.de/coronavirus/chronik-coronavirus.html?stand=20210104\n",
    "        plt.vlines(x = [\"2018-12-10\"], ymin=vLineMin, ymax=vLineMax, color=\"orange\", ls='--', label=\"Global Compact for Migration (2018-12-10)\")\n",
    "        plt.vlines(x = [\"2020-01-27\"], ymin=vLineMin, ymax=vLineMax, color=\"grey\", ls='--', label=\"Corona Patient Zero Germany\")\n",
    "        plt.vlines(x = [\"2020-03-23\"], ymin=vLineMin, ymax=vLineMax, color=\"purple\", ls='--', label=\"1. Lockdown Germany (2020-03-23)\")\n",
    "        plt.vlines(x = [\"2020-11-02\"], ymin=vLineMin, ymax=vLineMax, color=\"purple\", ls='--', label=\"2. Lockdown light Germany (2020-11-02)\")\n",
    "        plt.vlines(x = [\"2020-12-16\"], ymin=vLineMin, ymax=vLineMax, color=\"purple\", ls='--', label=\"3. Lockdown Germany (2020-12-16)\")\n",
    "\n",
    "        _ = plt.legend()\n",
    "\n",
    "        if(outputFilename != \"\"):\n",
    "            plt.savefig(dir_var_output + outputFilename)\n",
    "\n",
    "    else:\n",
    "        print(\"Error data not found >>\" + inputSelector + \"<<\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalSenPipeline(\"sen-bert\", \"dataSet0\", outputFilename = \"\", configRolling = 600, configShowScatter = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalSenPipeline(\"sen-bert\", \"dataSet0\", outputFilename = \"eval-pipeline-sen-dataSet0.svg\", configRolling = 600, configShowScatter = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalSenPipeline(\"sentiment\", \"dataSet0\", outputFilename = \"\", configRolling = 600, configShowScatter = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalSenPipeline(\"sentiment\", \"dataSet0\", outputFilename = \"eval-pipeline-sen-textblob-dataSet0.svg\", configRolling = 600, configShowScatter = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalSenPipeline(\"subjectivity\", \"dataSet0\", outputFilename = \"\", configRolling = 600, configShowScatter = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalSenPipeline(\"subjectivity\", \"dataSet0\", outputFilename = \"eval-pipeline-subjectivity-dataSet0.svg\", configRolling = 600, configShowScatter = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(processSentimentAnalysisPython(\"Heute ist ein toller Tag. Ich freue mich hier zu sein!\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(processSentimentAnalysisPython(\"Heute war ein furchtbarer Tag. Ich hasse alles.\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processSenPipeline(\"Das ist toll. Ich würde es mir wieder kaufen!\", \"sen-bert\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processSenPipeline(\"Das ist toll. Ich würde es aber nicht mehr kaufen!\", \"sen-bert\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processSenPipeline(\"Das funktioniert nicht.\", \"sen-bert\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processTextGenPipeline(inputText, pipelineKey, cMaxLength):\n",
    "    if(pipelineKey in pipelineKeys):\n",
    "        return dictPipelines[pipelineKey](inputText, max_length=cMaxLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sText = \"Hallo, mein Name ist Max und ich esse gerne Eis. Ich schreibe gerade an meiner Masterarbeit und teste neue Verfahren. Ich komme aus dem Großraum München und bin Informatiker.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processNerPipeline(sText, \"ner-xlm-roberta\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processNerPipeline(sText, \"ner-bert\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processTextGenPipeline(sText, \"text-gen-gpt2\", cMaxLength = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processTextGenPipeline(sText, \"text-gen-gpt2-faust\", cMaxLength = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling\n",
    "\n",
    "Overview Topic Modeling\n",
    "https://nlpforhackers.io/topic-modeling/\n",
    "\n",
    "- LDA (Probabilistic Graphical Models)\n",
    "- LSA or LSI (Linear Algebra Singular Value Decomposition)\n",
    "- NMF (Linear Algebra Non-Negative Matrix Factorization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation (LDA) with Gensim\n",
    "\n",
    "https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Get clean words from it (arrays)\n",
    "def sendToWords(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc= removes punctuations\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "        \n",
    "def removeStopwords(texts, stop_words):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "return  (lda_model, corpus, id2word)\n",
    "\"\"\"\n",
    "def processLda(df, num_topics, debugPrint, filterList):\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    df = df[df.procEvalIsValidText == True]\n",
    "\n",
    "    df = df[[\"date\", \"procTDSafeLowercaseText\"]]\n",
    "\n",
    "    df = df.set_index(\"date\")\n",
    "    df = df.sort_index()\n",
    "\n",
    "    # ###############################\n",
    "    # ### Transform Data ############\n",
    "    # ###############################\n",
    "    stops_words = gloGetStopWordsList(filterList)\n",
    "\n",
    "    msgList     = df.procTDSafeLowercaseText.values.tolist()\n",
    "\n",
    "    # Create words\n",
    "    msg_words   = list(sendToWords(msgList))\n",
    "    msg_word    = removeStopwords(texts = msg_words, stop_words = stops_words)\n",
    "\n",
    "    # Create Dictionary (id to word)\n",
    "    id2word = corpora.Dictionary(msg_word)\n",
    "\n",
    "    # Create Corpus\n",
    "    texts = msg_word\n",
    "\n",
    "    # Term Document Frequency (dict to bag of words)\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "    # Build LDA model\n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics)\n",
    "\n",
    "    if(debugPrint):\n",
    "        pprint(lda_model.print_topics())\n",
    "        #doc_lda = lda_model[corpus] # TODO: ?\n",
    "\n",
    "    return (lda_model, corpus, id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "param   outputLabel required\n",
    "\"\"\"\n",
    "def ldaToHtml(lda_model, corpus, id2word, outputLabel):\n",
    "\n",
    "    # pyLDAvis.enable_notebook()\n",
    "\n",
    "    LDAvis_data_filepath = os.path.join(dir_var_output + 'pyLDAvis/' + outputLabel + '-data')\n",
    "\n",
    "    # # this is a bit time consuming - make the if statement True\n",
    "    # # if you want to execute visualization prep yourself\n",
    "    \"\"\"\n",
    "    if 1 == 1:\n",
    "        LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "        with open(LDAvis_data_filepath, 'wb') as f:\n",
    "            pickle.dump(LDAvis_prepared, f)\n",
    "\n",
    "    # load the pre-prepared pyLDAvis data from disk\n",
    "    with open(LDAvis_data_filepath, 'rb') as f:\n",
    "        LDAvis_prepared = pickle.load(f)\n",
    "    \"\"\"\n",
    "\n",
    "    LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "\n",
    "    pyLDAvis.save_html(LDAvis_prepared, dir_var_output + 'pyLDAvis/' + outputLabel + '-report.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param outputLabel required\n",
    "def autoLda(df, debugPrint, outputLabel, filterList, listNumberTopics):\n",
    "\n",
    "    for iTopics in listNumberTopics:\n",
    "\n",
    "        iLabel = outputLabel + \"-t-\" + str(iTopics)\n",
    "\n",
    "        gloStartStopwatch(\"Process LDA (\" + str(iTopics) + \" topics) >> \"+ iLabel + \"<<\")\n",
    "\n",
    "        try:\n",
    "            \n",
    "            lda_model, corpus, id2word = processLda(\n",
    "                df = df,\n",
    "                num_topics = iTopics,\n",
    "                debugPrint = debugPrint,\n",
    "                filterList = filterList\n",
    "            )\n",
    "\n",
    "            ldaToHtml(\n",
    "                lda_model = lda_model,\n",
    "                corpus = corpus,\n",
    "                id2word = id2word,\n",
    "                outputLabel = iLabel\n",
    "            )\n",
    "\n",
    "        except:\n",
    "            print(\"Error in process lda\")\n",
    "\n",
    "        gloStopStopwatch(\"Process LDA (\" + str(iTopics) + \" topics) >> \"+ iLabel + \"<<\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    autoLda(\n",
    "        df = dictMessages[\"DS-05-01-2021/ChatExport_2021-01-05-janich\"],\n",
    "        debugPrint = False,\n",
    "        outputLabel = \"oliver-janich\",\n",
    "        filterList = [],\n",
    "        listNumberTopics = [2,4,8,16]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    autoLda(\n",
    "        df = dictMessages[\"DS-05-01-2021/ChatExport_2021-01-05-hildmann\"],\n",
    "        debugPrint = False,\n",
    "        outputLabel = \"attila-hildmann\",\n",
    "        filterList = [],\n",
    "        listNumberTopics = [2,4,8,16]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    autoLda(\n",
    "        df = dictMessages[\"DS-05-01-2021/ChatExport_2021-01-05-evaherman\"],\n",
    "        debugPrint = False,\n",
    "        outputLabel = \"eva-herman\",\n",
    "        filterList = [],\n",
    "        listNumberTopics = [2,4,8,16]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    autoLda(\n",
    "        df = dictMessages[\"DS-05-01-2021/ChatExport_2021-01-05-xavier\"],\n",
    "        debugPrint = False,\n",
    "        outputLabel = \"xavier-naidoo\",\n",
    "        filterList = [],\n",
    "        listNumberTopics = [2,4,8,16]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    if(\"dataSet1a\" in C_LOAD_DATASETS):\n",
    "        autoLda(\n",
    "            df = dictMessages[\"DS-05-01-2021a/ChatExport_2021-01-05-freiheitsChat\"],\n",
    "            debugPrint = False,\n",
    "            outputLabel = \"group-freiheitsChat\",\n",
    "            filterList = [],\n",
    "            listNumberTopics = [2,4,8,16,32]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    if(\"dataSet1a\" in C_LOAD_DATASETS):\n",
    "        autoLda(\n",
    "            df = dictMessages[\"DS-05-01-2021a/ChatExport_2021-01-05-freiheitsChatBlitz\"],\n",
    "            debugPrint = False,\n",
    "            outputLabel = \"group-freiheitsChatBlitz\",\n",
    "            filterList = [],\n",
    "            listNumberTopics = [2,4,8,16,32]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    if(\"dataSet1a\" in C_LOAD_DATASETS):\n",
    "        autoLda(\n",
    "            df = dictMessages[\"DS-05-01-2021a/ChatExport_2021-01-05-liveFuerDeOsSc\"],\n",
    "            debugPrint = False,\n",
    "            outputLabel = \"group-liveFuerDeOsSc\",\n",
    "            filterList = [],\n",
    "            listNumberTopics = [2,4,8,16,32]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tracer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.bundestag.de/parlament/plenum/sitzverteilung_19wp\n",
    "highlightwords = [\"cdu\", \"spd\", \"afd\", \"fdp\", \"linke\", \"gruenen\", \"merkel\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    drawTimePlot(\n",
    "        filePathList = list([\"DS-05-01-2021/ChatExport_2021-01-05-janich\"]),\n",
    "        outputFilename = \"word-tracer-oliver-janich.svg\",\n",
    "        highlightWords = highlightwords,\n",
    "        configFrequency=C_TIME_PLOT_FREQ\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    drawTimePlot(\n",
    "        filePathList = list([\"DS-05-01-2021/ChatExport_2021-01-05-hildmann\"]),\n",
    "        outputFilename = \"word-tracer-attila-hildmann.svg\",\n",
    "        highlightWords = highlightwords,\n",
    "        configFrequency=C_TIME_PLOT_FREQ\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    drawTimePlot(\n",
    "        filePathList = list([\"DS-05-01-2021/ChatExport_2021-01-05-evaherman\"]),\n",
    "        outputFilename = \"word-tracer-eva-herman.svg\",\n",
    "        highlightWords = highlightwords,\n",
    "        configFrequency=C_TIME_PLOT_FREQ\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(C_SHORT_RUN == False):\n",
    "    drawTimePlot(\n",
    "        filePathList = list([\"DS-05-01-2021/ChatExport_2021-01-05-xavier\"]),\n",
    "        outputFilename = \"word-tracer-xavier-naidoo.svg\",\n",
    "        highlightWords = highlightwords,\n",
    "        configFrequency=C_TIME_PLOT_FREQ\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gloStopStopwatch(\"Global notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next Steps\n",
    "\"\"\"\n",
    "- Add freq words to word tracer\n",
    "- Python NatrualLanguage Toolkit tools?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsai.net/p/data-mining/text-mining-in-python-steps-and-examples-78b3f8fd913b\n",
    "\"\"\"\n",
    "- Concordance (and Kookkurrenz and Correl?)\n",
    "- Tokenization\n",
    "- Finding frequency distinct in the text\n",
    "- Stemming\n",
    "- Lemmatization\n",
    "- Stop words?\n",
    "- Part of speech tagging (POS)\n",
    "- Named entity recognition\n",
    "- Chunking\n",
    "\"\"\"\n",
    "\n",
    "# https://towardsdatascience.com/text-mining-for-dummies-text-classification-with-python-98e47c3a9deb\n",
    "\"\"\"\n",
    "- Sentiment Analysis\n",
    "\"\"\"\n",
    "\n",
    "# https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-deal-text-data-predictive-python/\n",
    "\"\"\"\n",
    "- Features: (Number of words, Number of characters, Average word length, Number of stopwords, Number of special characters, Number of nummberics, Number of uppercase words)\n",
    "- Pre-processing (Lower casing, Punctuation removal, Stopwrods removal, Frequent word removal, Rare words removal, Spelling correction, Tokenization, Stemming, Lemmatization)\n",
    "- Adv-processing (N-grams, Term Frequency, Inverse Document Frequency, Term Frequency-Inverse Document Frequency 'TF-IDF', Bag of words, Sentiment Analysis, Word Embedding)\n",
    "\"\"\"\n",
    "\n",
    "# https://realpython.com/python-keras-text-classification/\n",
    "\"\"\"\n",
    "- Text Analysis with Keras\n",
    "\"\"\"\n",
    "\n",
    "# https://www.tidytextmining.com/ngrams.html\n",
    "\"\"\"\n",
    "- Relationships between words: n-grams and correlations\n",
    "\"\"\"\n",
    "\n",
    "# http://seaborn.pydata.org/tutorial/categorical.html?highlight=bar%20plot\n",
    "\"\"\"\n",
    "- Plotting with categorical data (Box Plot)\n",
    "\"\"\"\n",
    "\n",
    "# https://towardsdatascience.com/visualizing-data-with-pair-plots-in-python-f228cf529166\n",
    "\"\"\"\n",
    "- Visualizing Data with Pairs Plots in Python\n",
    "\"\"\"\n",
    "\n",
    "# https://www.kirenz.com/post/2019-08-13-network_analysis/\n",
    "\"\"\"\n",
    "- Social Network Analysis with Python\n",
    "\"\"\"\n",
    "\n",
    "# https://tgstat.com\n",
    "\"\"\"\n",
    "- Compare website with my analyse\n",
    "\"\"\"\n",
    "\n",
    "# https://huggingface.co/bert-base-german-cased\n",
    "\"\"\"\n",
    "- Language Model Bert German\n",
    "\"\"\"\n",
    "\n",
    "# https://github.com/sekhansen/text-mining-tutorial/tree/master\n",
    "\"\"\"\n",
    "- An Introduction to Topic Modelling via Gibbs sampling\n",
    "\"\"\"\n",
    "\n",
    "# https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
    "\"\"\"\n",
    "- Topic Modeling with Gensim (Python)\n",
    "\"\"\"\n",
    "\n",
    "# https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0\n",
    "\"\"\"\n",
    "- Topic Modeling in Python: Latent Dirichlet Allocation (LDA)\n",
    "\"\"\"\n",
    "\n",
    "# https://github.com/sekhansen/text-mining-tutorial/blob/master/tutorial_notebook.ipynb\n",
    "\"\"\"\n",
    "- Text Mining Python Tutorial\n",
    "\"\"\"\n",
    "\n",
    "# https://textmining.wp.hs-hannover.de/Preprocessing.html\n",
    "\"\"\"\n",
    "- Preprocessing samples\n",
    "\"\"\"\n",
    "\n",
    "# https://likegeeks.com/nlp-tutorial-using-python-nltk/\n",
    "\"\"\"\n",
    "- NLTK and NLP Tutorial\n",
    "\"\"\"\n",
    "\n",
    "# https://www.datacamp.com/community/tutorials/text-analytics-beginners-nltk\n",
    "\"\"\"\n",
    "- Another NLTK Tutorial\n",
    "\"\"\"\n",
    "\n",
    "# https://data-flair.training/blogs/nltk-python-tutorial/\n",
    "\"\"\"\n",
    "- NLTK Overview\n",
    "\"\"\"\n",
    "\n",
    "# https://github.com/expectocode/telegram-analysis\n",
    "\"\"\"\n",
    "- Telegram Python Sample\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
